[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Guide pratique des techniques du son",
    "section": "",
    "text": "Avant-propos\nCe livre est né de la nécessité d’un support de cours pour la formation professionnelle “Technique de Prise de Son”, dispensée par l’auteur. Il intègre donc l’ensemble des notions abordées, expliquées en détail, ainsi que des exemples sonores.\nCe livre est écrit dans la philosophie de l’Open Source. L’intégralité de son contenu est donc disponible gratuitement. Son code source est accessible dans un dépôt GitHub. Ainsi, il est possible à tout à chacun de reporter les éventuelles erreurs ou de proposer des modifications. La grande majorité des outils utilisés pour sa rédaction et la création du contenu sont open source : R & Rmarkdown, Python, FAUST et draw.io."
  },
  {
    "objectID": "index.html#a-qui-sadresse-cet-ouvrage",
    "href": "index.html#a-qui-sadresse-cet-ouvrage",
    "title": "Guide pratique des techniques du son",
    "section": "A qui s’adresse cet ouvrage",
    "text": "A qui s’adresse cet ouvrage\nCe livre s’adresse à toutes personnes désireuses d’en apprendre plus sur le son ainsi que sur les métiers de preneur de son et de mixeur. Ainsi, il fait état des principes physiques nécessaires à la bonne appréhension des techniques de travail des métiers susnommés, avec le souci de les rendre accessibles à toutes et tous.\nIl pourra donc servir aux musiciens, aux étudiants, et pourquoi pas, à certains professionnels des métiers du son et du divertissement en général."
  },
  {
    "objectID": "index.html#mise-à-jour",
    "href": "index.html#mise-à-jour",
    "title": "Guide pratique des techniques du son",
    "section": "Mise à jour",
    "text": "Mise à jour\nLa distribution numérique de ce livre permet une mise à jour régulière de son contenu. Cela implique deux choses :\n\nCertaines sections peuvent être incomplètes, et seront complétées plus tard\nC’est une bonne idée de revenir consulter ce site régulièrement\n\n\nPour l’instant, ce livre n’inclut pas encore d’exemples sonores, cela est en cours de création."
  },
  {
    "objectID": "index.html#structures",
    "href": "index.html#structures",
    "title": "Guide pratique des techniques du son",
    "section": "Structures",
    "text": "Structures\nDans un premier temps, le livre aborde des principes généraux, aussi bien sur la physique que sur l’environnement de production de la musique enregistrée. Est ensuite abordé l’ensemble de la chaîne audio, en y explicitant le rôle et le fonctionnement de chacun de ses composants. L’objectif est de fournir une base technique objective au preneur de son.\nDans un second temps, le livre détaille un ensemble de techniques de prise de son et de mixage, insistant particulièrement sur les mécanismes généraux de la prise et sur l’écoute critique.\n\nLa partie dédiée à la pratique du mixage son n’est pas encore disponible en ligne."
  },
  {
    "objectID": "index.html#à-propos-de-lauteur",
    "href": "index.html#à-propos-de-lauteur",
    "title": "Guide pratique des techniques du son",
    "section": "À propos de l’auteur",
    "text": "À propos de l’auteur\nJean-Loup Pecquais est formateur et consultant dans le monde de l’audio professionnel (FLUX:: Immersive, Whiti Audio, Arkalya). Il est plus particulièrement spécialisé dans les techniques de mixage sonore immersives. Il est diplômé de l’ENS Louis-Lumière en 2019."
  },
  {
    "objectID": "generalites/oreille.html#loreille-externe",
    "href": "generalites/oreille.html#loreille-externe",
    "title": "1  Oreille et audition",
    "section": "1.1 L’oreille externe",
    "text": "1.1 L’oreille externe\nComme son nom l’indique, l’oreille externe est la partie visible de notre organe de l’audition. Elle est caractérisée par ce pavillon à la forme si particulière. Le rôle de ce pavillon auriculaire permet de “collecter” le son. Sa forme applique une emprunte fréquentielle sur le son, donnant une information principalement d’ordre spatial. Ce pavillon joue donc un rôle important dans notre capacité à localiser les sons, avec une précision maximale lorsque l’évènement sonore se trouve devant nous.\n\n\n\n\n\n\nNote\n\n\n\nComme nous le verrons plus tard, la forme de notre tête et même de notre torse a également un rôle important dans notre capacité à localiser des évènements sonores dans l’espace.\n\n\nL’oreille externe comporte aussi le canal auditif, avec à son terme le tympan. Ce dernier est une simple membrane, vibrant de façon homologue à l’onde sonore lui provenant. L’ensemble de ce système de “captation” possède un rendement maximal sur les fréquences autour de 3 kHz (fréquences aiguës)."
  },
  {
    "objectID": "generalites/oreille.html#loreille-moyenne",
    "href": "generalites/oreille.html#loreille-moyenne",
    "title": "1  Oreille et audition",
    "section": "1.2 L’oreille moyenne",
    "text": "1.2 L’oreille moyenne\nDerrière le tympan se trouve l’oreille moyenne, et plus particulièrement une collection de trois os, nommés “marteau”, “enclume” et “étrier”. Le marteau est directement relié au tympan et transmet les vibrations du tympan à l’“enclume” puis à l’“étrier”. Ce dernier permet la transmission du son à l’oreille interne par la fenêtre ovale.\nNous y trouvons aussi la trompe d’eustache, reliée à la gorge, réalisant ainsi un équilibrage de la pression entre celle de l’oreille interne et celle nous environnant.\nLe rôle principal de cette oreille moyenne est de réaliser une adaptation d’impédance acoustique entre l’air et le milieu liquide, dans lequel baigne notre oreille interne. En d’autres termes, elle permet un transfert efficace de l’énergie sonore vers l’oreille interne. Sans cette étape, notre sens de l’audition serait grandement amoindri.\nÉvoquons également le muscle stapédien. Il permet, lorsqu’il est contracté, de limiter l’amplitude de mouvement des trois os évoqués précédemment. Il agit alors comme une protection lorsque nous sommes confrontés à de forts niveaux sonores. Si cette rigidification n’avait pas lieu, notre tympan serait beaucoup plus facilement arraché par des stimulus sonores importants.\nCependant, ce muscle stapédien échappe à notre contrôle cognitif et est donc mis en action par réflexe. Son temps de mise en action est d’au moins 40 millisecondes (ms) après l’émission d’un son supérieur à 90-100 décibel (dB). Pire, la protection maximale n’est atteinte que 150 ms plus tard. Cela signifie que, si nous sommes exposés à des déflagrations sonores très importantes (armes à feu, explosions, etc.), notre réflexe stapédien n’aura pas le temps de s’activer, et notre système auditif sera sévèrement endommagé."
  },
  {
    "objectID": "generalites/oreille.html#loreille-interne",
    "href": "generalites/oreille.html#loreille-interne",
    "title": "1  Oreille et audition",
    "section": "1.3 L’oreille interne",
    "text": "1.3 L’oreille interne\nNous avons précédemment évoqué l’attachement de l’étrier à la fenêtre ovale. Cette dernière fait elle-même partie de la cochlée, où commence l’oreille interne. Cette cochlée prend une forme de coquille d’escargot et renferme la membrane basilaire et l’organe de Corti, récepteur de l’audition. Son ensemble est immergé dans différentes lymphes (milieu liquide).\nLa membrane basilaire court le long de la cochlée et près de 30 000 récepteurs ressemblant à de petits cheveux la parcourent. Cette membrane vibre lorsque la lymphe change de pression. Sur l’ensemble de sa longueur est répartie notre sensibilité aux différentes fréquences. La partie proche de l’oreille moyenne est plus sensible aux aiguës, alors que la zone en bout de son enroulement est plus sensible aux graves. On a donc une correspondance entre fréquence et emplacement sur la membrane basilaire. Lorsque les récepteurs vibrent suffisamment fortement, un signal électrique est émis par l’organe de Cortie dans le système nerveux.\nLa diminution de sensibilité, voire la perte de certaines fréquences audible dans l’audition, est associée à la mort de ces récepteurs peuplant la membrane basilaire. Ce phénomène est irréversible et peut aboutir à l’apparition d’acouphènes.\n\n\n\n\n\n\nAvertissement\n\n\n\nL’oreille est un organe fragile, dont l’endommagement est irréversible. Il convient donc d’en prendre soin en limitant :\n\nson exposition à de forts niveaux sonores\nson exposition au bruit\n\nDans les cas où l’exposition est contrainte (voire souhaité en concert par exemple), il est vivement conseillé de porter des protections auditives (bouchons d’oreilles).\nIl est également bon de rappeler que, dans la législation française, les niveaux sonores de diffusions dans des lieux de spectacles sont normés et ne doivent pas dépasser certains seuils. L’ingénieur du son affecté au mixage est donc responsable du respect de ces normes.\nEn studio, il n’existe pas de norme de niveau de diffusions dans les casques ou les écoutes, mais l’ingénieur du son reste tout de même responsable de ce que les musiciens entendent, et donc de toute perte d’audition de l’un d’eux lors d’une session de travail."
  },
  {
    "objectID": "generalites/qualifier_le_son.html#sec-le-son-physique",
    "href": "generalites/qualifier_le_son.html#sec-le-son-physique",
    "title": "2  Quantifier et qualifier le son",
    "section": "2.1 Phénomène physique",
    "text": "2.1 Phénomène physique\n\n2.1.1 Quelques définitions\nLe son est une vibration mécanique d’un fluide. Dans le cadre de ce cours, nous ne considérerons que l’air comme médium de propagation. Cette onde cause une variation de la pression dans l’espace. Nous, les êtres humains, le percevons grâce à notre ouïe. Il s’agit donc, par définition, d’un phénomène ondulatoire et peut être caractérisé par un nombre d’oscillations par seconde, aussi appelé fréquence. On estime que notre espèce est sensible aux fréquences allant de 20 Hz (très grave) jusqu’à 20 000 Hz (très aigu).\nOn parlera d’évènement sonore pour parler généralement de phénomènes physiques produisant une onde sonore.\nLes sons composés d’une seule fréquence se nomment sons purs. Cependant, de tels signaux n’existent pas dans la nature, et sont souvent utilisés afin de réaliser des mesures ou des tests psychoacoustiques.\n\n\n\n\n\nFigure 2.1: Onde sinusoïdale et visualisation de son spectre\n\n\n\n\nDans notre environnement, les sons sont donc composés de plusieurs fréquences. La fréquence la plus grave d’un son est sa fréquence fondamentale. Les autres sont alors appelées partiels. Si ces partielles ont pour fréquence un multiple de la fréquence fondamentale, alors on les nomme harmoniques.\n\nPlus généralement, on admettra que la composition fréquentielle, ou spectrale, de tout son peut être décomposée par une somme de sinusoïde. L’outil permettant de passer de la représentation temporelle d’un signal à sa représentation fréquentiel s’appelle la transformée de Fourrier.\n\n\n\n\n\n\nFigure 2.2: Signal carré et visualisation de son spectre\n\n\n\n\nLa fréquence fondamentale donne la hauteur du son (sa note en musique par exemple). Les partiels enrichissent cette fréquence fondamentale et créés le timbre d’un son. C’est en partie grâce au timbre que l’on peut reconnaître différents instruments de musiques jouant la même note.\nUn son se caractérise également par l’évolution de son amplitude au cours du temps. On parle alors de son enveloppe. Un modèle courant d’enveloppe est l’ADSR : Attack, Decay, Sustain, Release, soit Attaque, Décroissance, Maintient et Relâchement.\n\n\n\nFigure 2.3: Exemple d’enveloppe ADSR\n\n\nLorsque son temps et très bref, l’ensemble attaque et décroissance forme les transitoires. Cette partie du signal est responsable de la sensation percussive du son.\n\n\n2.1.2 Relation entre temps, distance et fréquence\nIl est important de garder à l’esprit que les notions de temps, de fréquence et de distance sont étroitement liées. Nous avons vu ci-dessus que tous les sons peuvent être décrits par une somme de sinusoïde. Leur fréquence la plus grave, dite fondamentale, permet de définir la période. La période est le temps que met un signal à répéter son motif oscillatoire (voir schémas 3.1 et 3.2). Le lien mathématique entre fréquence et période est très simple, car l’un est l’inverse de l’autre :\n\\[ f = \\frac 1 T \\]\nSi nous étudions les fréquences extrêmes, audibles par notre ouïe, nous trouvons que pour \\(f_{min} = 20 \\,Hz\\), sa période \\(T_{f_{min}} = 50 \\,ms\\). Pour \\(f_{max} = 20\\,000 \\,Hz\\), \\(T_{f_{max}} = 0.5 \\,ms\\).\nUne onde sonore est également caractérisée par sa célérité. Celle-ci est constante dans un milieu donné. Dans l’air, à une température de \\(15 \\,°C\\) et au niveau de la mer, sa célérité \\(c\\) est de \\(340\\,m.s^{-1}\\). On admettra cette valeur pour réaliser l’ensemble de nos différents calculs.\nComme son unité l’indique, la célérité du son est homogène à une distance divisée par un temps, soit :\n\\[ c =\\frac d t \\]\nSuivant cette formule, nous pouvons alors calculer la longueur d’onde correspondant à une fréquence. La longueur d’onde se note \\(\\lambda\\).\n\\[ \\lambda = cT \\; \\iff \\; \\lambda = \\frac c f\\]\nSi nous étudions à nouveau les bornes minimale et maximale de notre audition, nous trouvons que \\(\\lambda_{f_{min}} = 17 \\,m\\) et \\(\\lambda_{f_{max}} = 17 \\,mm\\).\nNous pouvons également calculer le temps de propagation du son. En pratique, nous serons souvent intéressés par le temps de propagation séparant deux points dans l’espace (par exemple, le temps séparant deux microphones par rapport à un instrument).\n\n\n\nFigure 2.4: Distance entre deux microphones.\n\n\n\\[ t = \\frac {d_2-d_1}{c}\\]"
  },
  {
    "objectID": "generalites/qualifier_le_son.html#perception-du-son",
    "href": "generalites/qualifier_le_son.html#perception-du-son",
    "title": "2  Quantifier et qualifier le son",
    "section": "2.2 Perception du son",
    "text": "2.2 Perception du son\nNous avons abordé quelques notions de physique permettant de mieux caractériser le phénomène sonore. Comme indiqué au début de ce chapitre, le son peut également être discuté sous l’angle de notre ouïe, et donc, de notre perception. Cette branche de la science se nomme la psychoacoustique et cherche à étudier la façon dont nous percevons le son.\nNotre corps, et a fortiori notre cerveau, sont des machines extrêmement complexes. Nous sommes équipés d’une multitude de capteurs permettant de sentir le contact d’une matière, des odeurs, d’entendre, de goûter, de voir, de positionner nos membres dans l’espace, de ressentir la douleur, etc. Pris indépendamment, chacun de ces sens est déjà un phénomène complexe à décrire, mais il existe en plus une grande interdépendance entre ceux-ci. Par exemple, l’interdépendance entre la vision et l’audition est à l’origine d’un certain nombre de mécanismes biaisant notre écoute.\nNous nous bornerons au fil de ce cours à quelques notions liées à l’ouïe et à son interdépendance à d’autre sens quand cela sera pertinent.\n\n2.2.1 Spectre, timbre et vocabulaire\nD’un point de vue perceptif, le spectre d’un évènement sonore est facilement remarquable. Il est, par contre, beaucoup plus difficile à qualifier. Il n’est pas rare de rencontrer les adjectifs “chaud”, “brillant”, “rond”, “aéré”, “ouvert”, “sombre”, voir d’autres encore plus ésotérique, pour tenter de communiquer la sensation ressentie à l’écoute de tel ou tel son.\nCette difficulté liée à l’absence de vocabulaire commun quant à la qualification le son emmène systématiquement la redéfinition de ce vocable en fonction de son interlocuteur. En effet, le mot “rond” ne signifiera pas forcément la même chose selon à qui on s’adresse. Une stratégie possible consiste à questionner son interlocuteur sur l’utilisation de ses adjectifs tout en cherchant à y associer des exemples sonores.\nNous pouvons tout de même nous essayer à cet exercice pour nous permettre d’avoir un vocabulaire commun au fil de ce cours. Vous aurez sans doute compris qu’il n’y aura, dans les termes employés, aucun critère absolu.\n\n\n\nFigure 2.5: Proposition de découpage du spectre\n\n\nProposition d’association entre bandes de fréquences et sensation.\n\n20 Hz — 80 Hz : Subharmonique, sensation tripale\n80 Hz — 160 Hz : Grave, sensation d’assise\n160 Hz — 380 Hz : bas-médium, sensation de « chaleur », voir « boueux »\n380 Hz — 1400 Hz : Medium, sensation de « boîte » quand trop présent, sonne « creux » quand trop absent\n1400 Hz — 3200 Hz : Haut-medium : zone de sensibilité maximale de l’oreille.\n3200 Hz — 8000 Hz : Aigu, apporte de la précision voir de l’agressivité\n8000 Hz — 20 000 Hz : Air, apporte une sensation d’ouverture voir de finesse\n\nIl est intéressant de former son oreille à reconnaître une plage de fréquence, ainsi que d’y associer son propre vocabulaire et une sensation. Les appellations proposées ci-dessus ne sont à prendre que comme guides et n’ont pas valeur de référence. Cela favorise une écoute critique et analytique.\n\n\n\n\n\n\nImportant\n\n\n\nAussi, les fréquences graves ont un effet masquant sur les fréquences plus aiguës. Ce phénomène est dû au fonctionnement de notre oreille, et plus particulièrement de la cochlée.\n\n\n\n\n2.2.2 Pression acoustique & niveau sonore\nNous l’avons abordé plus haut, lorsqu’une onde sonore se déplace dans l’air, on constate la variation de la pression atmosphérique en ce point. Dès lors, il est facile de corréler l’amplitude de la variation de la pression avec le niveau sonore entendu (ou mesuré).\nL’unité du système international de la pression est le pascal (Pa). Or, il est très rare de parler de la pression acoustique en pascal, car la variation de cette pression exprimée en pascal ne correspond pas à ce que nous percevons. En d’autres termes, si la pression acoustique exprimée en pascal double, nous ne percevons pas un son deux fois plus fort.\nNotre oreille fonctionne de façon logarithmique, et non linéairement, face à une variation de pression acoustique. C’est pour cela que l’on parle généralement de niveau de pression acoustique, où SPL (pour Sound Pressure Level en anglais), qui s’exprimera en décibel. La relation entre la variation de pression et le niveau de pression acoustique se fait grâce à la relation :\n\\[L_p = 20\\,\\log_{10}\\Big(\\frac{p_{eff}}{p_{ref}}\\Big) \\qquad p_{ref} = 20\\mu Pa\\]\n\n\n\n\n\n\nNote\n\n\n\nSi la pression acoustique double, on observe une augmentation du niveau sonore de 6 dB SPL. Lorsqu’on ressent un doublement du niveau sonore, on observe une augmentation de 20 dB.\nLa pression acoustique est divisée par deux à chaque doublement de distance.\n\n\nLa question se complexifie lorsque l’on rajoute la dimension fréquentielle à la question de la perception du niveau sonore. En effet, nous percevons des niveaux sonores différents pour différentes fréquences pourtant émises au même niveau de pression acoustique. Pour inclure cette dépendance fréquentielle, nous avons mis en place une unité de mesure : la sonie ou bruyance (loudness en anglais). Il est donc possible ensuite de définir des courbes d’isosonie, c’est-à-dire des courbes indiquant un niveau sonore de perception égale en fonction de la fréquence et du niveau de pression acoustique.\n\n\n\nFigure 2.6: Courbes d’isosonie, aussi dites de Fletcher-Munson\n\n\nQue conclure de cet abaque ?\n\nNotre oreille ne perçoit pas les fréquences de manière égale.\nNotre zone de sensibilité maximale se situe dans l’aigu (3k-4k Hz).\nNotre perception d’un matériau sonore en fonction du niveau auquel nous l’écoutons !\n\n\n\n2.2.3 Positionnement dans l’espace\nNotre système auditif nous permet de situer l’émission d’un son dans l’espace. Cette capacité de localisation repose sur un ensemble de facteurs étroitement liés entre eux.\nOn qualifie notre écoute de binaurale, littéralement, écouter avec deux oreilles. La présence de deux “capteurs de pression” (oserait-on parler de microphones ?) sur les faces latérales de notre crâne et un premier élément expliquant notre capacité de localisation du son.\nEn effet, l’espacement de nos oreilles (en moyenne 15 cm), créer un décalage temporel entre nos deux canaux d’écoutes. Ce léger retard entendu d’un côté ou de l’autre nous permettra de placer un son plutôt à gauche ou plutôt à notre droite. On appelle cet écart de temps différence de temps interaural, ou ITD (interaural time difference en anglais) et se note \\(\\Delta t\\).\nOn pourrait d’ailleurs, grâce aux formules de ce début de chapitre, calculer le retard maximal moyen entre nos deux oreilles.\n\\[\\Delta t_{max} = \\frac d c = \\frac {0,15}{340} = 0.4 \\&gt; ms\\]\n\n\n\nFigure 2.7: Illustration de l’ITD\n\n\nSi nos oreilles sont espacées de quelques centimètres, notre tête les séparant représente un obstacle acoustique non négligeable. De plus, les pavillons des oreilles imposent également une certaine directivité à notre écoute. En première approximation, on pourra donc considérer que l’ensemble formé par la tête et les pavillons implique une atténuation linéaire des ondes sonores, elle-même fonction de l’angle d’incidence. On appelle cette différence de niveau différence d’intensité interaural, ou ILD (interaural level difference) et se note \\(\\Delta i\\). On considère que si la différence de niveau de pression acoustique entre les deux oreilles est supérieure à 20 dB, on entendra l’évènement sonore complètement latéralisé.\n\n\n\n\n\n\nImportant\n\n\n\nL’ombre acoustique que représentent la tête et le pavillon n’est en réalité pas du tout linéaire en fréquence. La modification du timbre induite par ce système n’est pas perçue par notre cerveau comme une information de couleur, mais bien comme une information de spatialisation. Ainsi, selon l’angle d’incidence de l’évènement sonore, son spectre sera filtré d’une certaine manière qui permettra à notre cerveau de le positionner dans l’espace. La réponse en fréquence d’une tête se nomme HRTF (Head Related Transfer Function).\n\n\nEnfin, nous sommes également capables de déterminer la distance d’un évènement sonore. La plupart des paramètres permettant d’évaluer cette distance sont relatifs. Cela signifie que l’évènement doit être comparé à un autre pour pouvoir le repositionner dans l’espace. On pourra alors comparer :\n\nLeurs niveaux sonores : un évènement sonore plus fort paraît plus proche\nLeurs timbres : l’absorption de l’air aura pour effet de diminuer les fréquences aiguës\nLa sensation de réverbération associée : plus le signal de l’évènement sonore semblera solliciter la réponse acoustique du lieu, plus celui-ci semblera fort.\nLe temps d’arrivée des premières réflexions : le son direct d’un évènement sonore lointain arrivera quasi simultanément avec ses premières réflexions. Le son direct d’un évènement sonore proche arrivera avant ses premières réflexions.\n\nLe chapitre suivant traitera des notions d’acoustique élémentaire ainsi que de la réverbération."
  },
  {
    "objectID": "generalites/l_acoustique_des_salles.html#généralités",
    "href": "generalites/l_acoustique_des_salles.html#généralités",
    "title": "3  Acoustique des salles",
    "section": "3.1 Généralités",
    "text": "3.1 Généralités\n\n3.1.1 La réverbération\nAfin d’étudier l’acoustique d’une salle, on procède à la mesure de sa réponse impulsionnelle. Pour se faire, on émet dans le lieu à mesurer un signal audio impulsionnel (clappement de main, explosion d’un ballon, émission d’une impulsion de Dirac grâce à un haut-parleur), et l’on enregistre le résultat à l’aide d’un microphone de mesure.\nLa réponse impulsionnelle d’une salle est généralement décrite en deux temps : le temps des premières réflexions et le temps du champ diffus.\n\n\n\nFigure 3.1: Schéma d’une réponse impulsionnelle de réverbération.\n\n\nLes premières réflexions sont les premiers rebonds d’une onde sonore sur les parois d’une salle et sont caractéristiques de la signature acoustique du lieu. Ces rebonds reviennent à l’auditeur avec un certain temps. Ce retard se nomme souvent « pré-délai » dans les moteurs de réverbération artificiels. Ce prédélai est fonction de deux paramètres :\n\nla taille de la pièce ; plus la pièce est petite, plus les premières réflexions reviendront à l’auditeur rapidement.\nles positions de la source sonore et de l’auditeur ; plus l’auditeur est proche de la source, plus les premières réflexions arriveront après le son direct, plus l’auditeur est loin de la source, plus les premières réflexions arriveront en même temps que le son direct.\n\nLorsque les premières réflexions elles-mêmes auront rebondi plusieurs fois sur les parois du lieu, le phénomène d’écho des premières réflexions va se muer en champs diffus, par nature plus dense. La longueur du champ diffus se mesure grâce au RT60. Cette méthode de mesure propose de regarder le temps que met la réverbération à perdre 60 dB. Ce temps permettra ensuite de donner une longueur de réverbération.\n\n\n3.1.2 Calcul du temps de réverbération\nL’équation de Sabine permet de calculer le temps de réverbération d’une salle à partir de son volume et du coefficient d’absorption de ses matériaux.\n\\[RT_{60} = 0.1611 \\times \\frac{V}{\\sum_{i=0}^{k} S_i.\\alpha_i +  \\mu V}\\]\n\\(V\\) s’exprime en \\(m^3\\) et \\(S\\) en \\(m^2\\). \\(\\alpha\\) est le coefficient d’absorption du matériau, en sabins. Ce coefficient est compris entre 0 et 1, plus il est important plus le matériau est absorbant. \\(\\mu\\) est le coefficient d’absorption de l’air (\\(m^{-1}\\)).\n\n\n\n\n\n\nAstuce\n\n\n\nPour des fréquences inférieures à 8 kHz, on peut considérer le terme \\(\\mu V\\) comme négligeable\n\n\nEn guise d’exemple sur l’utilisation de la formule ci-dessus, prenons le cas d’une pièce de \\(25\\,m^2\\) (\\(5\\,m\\) par \\(5\\,m\\)) et de \\(2.40\\,m\\) de hauteur. Nous considérons que le sol est en parquet et les murs en plâtre. Nous avons donc \\(25\\,m^2\\) de parquet et \\(4\\times(5\\times2.4)=48\\,m^2\\). On trouve sur les sites de fabricant de matériaux que le plâtre peint a un coefficient d’absorption de 0.05 sabins et le bois un coefficient de 0.15 sabins. Notre calcul final.\n\\[RT_{60} = 0.1611 \\times \\frac{25 \\times 2.4}{25\\times0.15+48\\times0.05} \\approx 1.57\\,s\\]\nOn peut dès lors calculer la distance critique, distance à partir de laquelle on entendra autant un évènement sonore que la réponse acoustique de la salle à son stimulus.\n\\[d_c \\approx 0.057 \\times \\sqrt{\\frac{V}{RT60}}\\]\nDans notre exemple \\(d_c \\approx 0.35\\,m\\).\nIl est souvent considéré que la taille de la pièce joue un rôle déterminant sur la longueur de réverbération. L’équation de Sabine indique bien que le coefficient d’absorption des matériaux y joue un rôle beaucoup plus important. Le modèle de réverbération de l’IRCAM va jusqu’à complètement décorréler la taille de la pièce simulée du temps de réverbération. Au final, la taille de l’espace joue davantage sur la structure temporelle des échos, et donc, principalement sur les premières réflexions.\n\n\n3.1.3 Limite de l’équation de Sabine\nIl convient d’observer plusieurs réserves quant à l’utilisation de l’équation de Sabine. Premièrement, elle ne tient pas compte de l’aspect fréquentiel lié à l’absorption des matériaux. En effet, le temps de réverbération des graves est presque toujours plus long que celui des aigus. Afin de contourner ce problème, on pourra chercher des coefficients d’absorption tenant compte de la fréquence et ainsi résoudre l’équation de Sabine pour certaines plages fréquentielles.\nL’équation de Sabine pose également problème pour de petits espaces (régie d’écoute par exemple) en prédisant un temps de réverbération trop long. Dans ce cas, l’équation d’Eyring est plus adaptée.\n\\[RT_{60} = -0.1611 \\times \\frac{V}{\\sum_{i=0}^{k} S_i.\\ln(1-\\alpha_i) +  \\mu V}\\]\n\n\n\n\n\n\nAvertissement\n\n\n\nL’équation d’Eyring n’améliore pas non plus la problématique fréquentielle.\n\n\n\n\n3.1.4 L’indice de “Speech Clarity” C50\nL’indice d’intelligibilité (noté \\(C_{50}\\)), ou “Speech Clarity” en anglais, indique la faculté d’une pièce à permettre une bonne compréhension d’une voix parlée. Son principe repose sur la mesure de l’énergie de la réponse impulsionnelle de la pièce avant 50 ms et après 50 ms. On en fait ensuite un rapport logarithmique pour obtenir une valeur en décibel.\n\\[ C_{50} = 10 \\times \\log \\frac{Energie(&lt;50ms)}{Energie(&gt;50ms)} \\, dB \\]\nPlus la valeur du \\(C_{50}\\) est grande, plus la salle concentre la majorité de son énergie avant les 50 ms de propagation de la réverbération. À l’inverse, plus le \\(C_{50}\\) est faible, plus la salle a une énergie prédominante après 50 ms de temps de propagation. Dans ce cas une voix parlée paraîtra moins intelligible, car la réponse acoustique de la pièce engendrera un effet de fusion et de masquage.\n\n\n3.1.5 Le phénomène d’onde stationnaire\nLa plupart des pièces de vie sont des salles rectangulaires. Dans ce cas, les surfaces sont toutes parallèles. Ce type de salle est particulièrement propice à l’apparition d’ondes stationnaires. Une onde stationnaire est un phénomène acoustique provoquant l’augmentation de volume de certaines fréquences (ventre) et la disparition d’autres (nœuds).\nNous aborderons ici ce phénomène sous l’angle de l’acoustique des salles, mais il est applicable dans d’autres situations, comme la vibration d’une corde par exemple.\n\n\n\nFigure 3.2: Les points rouges représente les noeuds, les amplitudes maximales sont les ventres. Infographie par Lucas Vieira\n\n\nIl est possible de calculer les fréquences d’un mode grâce aux formules vues au chapitre précédent :\n\\[f(n) = \\frac{c}{2L}.n\\] où \\(c=340\\,m.s^{-1}\\), \\(L\\) est la longueur considérée de la pièce. Pour \\(n=1\\) on trouve le mode propre. Pour \\(n\\) strictemment supérieur à un on trouvera tous les modes harmoniques.\nÉtudions la fréquence du mode propre pour deux cas théoriques : une salle de 16 m² (4x4) et une autre de 49 m² (7x7). On trouvera donc :\n\\[f(1)_{L=4m} = 42.5 \\,Hz \\&gt;\\&gt;\\&gt;\\&gt; f(1)_{L=7m} = 24 \\,Hz\\]\nOn en déduit donc que, plus la pièce est grande, plus la fréquence des modes propres sera grave. Il convient également de considérer la distance de chaque surface parallèle, car les pièces sont rarement cubiques. Cela implique donc la présence de trois modes propres, plus leurs modes harmoniques, pour une seule et même salle.\nAu-delà d’une certaine fréquence, dite de Schroeder, le recoupement entre les différents modes et leurs harmoniques est tel qu’ils ne sont plus distinguables. Le phénomène de réverbération s’apparente donc à un processus aléatoire gaussien.\n\\[F_{Schroeder} \\approx 2000 \\sqrt{{T_{60}}\\over{V}} (Hz)\\]"
  },
  {
    "objectID": "generalites/l_acoustique_des_salles.html#premières-réflexions-et-filtres-en-peignes",
    "href": "generalites/l_acoustique_des_salles.html#premières-réflexions-et-filtres-en-peignes",
    "title": "3  Acoustique des salles",
    "section": "3.2 Premières réflexions et filtres en peignes",
    "text": "3.2 Premières réflexions et filtres en peignes\nNous avons vu que la réponse acoustique, ou réverbération, d’une salle se décompose généralement en deux parties, la première étant les premières réflexions. Ces premières réflexions sont donc, comme leur nom l’indique, les premiers rebonds que nous entendons suite à un évènement sonore.\nDans de petites pièces, les premières réflexions peuvent être entendues si proche du son direct que cela génère un type de filtrage bien particulier appelé filtre en peigne.\n\n\n\n\n\nFigure 3.3: Filtre en peigne correspondant à un retard d’une milliseconde\n\n\n\n\nToujours en utilisant les formules définies au premier chapitre, on établit la relation suivante :\n\\[ fc = \\frac 1{2t} = \\frac c{2d} \\]\nOù \\(fc\\) correspond à la fréquence d’annulation la plus grave du filtre en peigne. Les autres fréquences se calculent grâce à la relation \\(f(n) = fc \\times n\\). Le phénomène de filtre en peigne est donc également harmonique.\nAinsi, on peut calculer les filtres en peignes présents au point d’écoute d’une régie de mixage ou de prise de son grâce à la mesure du chemin des premières réflexions.\n\n\n\nFigure 3.4: Ensemble des premières reflexions entendues par une oreille pour une enceinte (hors plafond et plancher/bureau).\n\n\n\nLa réflexion du son sur une paroi est tout à fait comparable à de l’optique géométrique. Une onde sonore arrivant avec un angle d’incidence \\(\\alpha\\) sur une surface sera réfléchie avec le même angle. Ainsi, il est souvent conseillé d’utiliser un miroir lorsque l’on positionne des traitements acoustiques. Lorsque la personne assise au point d’écoute voit une enceinte dans un miroir placé sur un mur, on sait alors qu’il faudra placer le panneau à la place du miroir.\n\nOn se rend donc compte que l’influence des filtres en peigne générés par les premières réflexions est très importante. Ce phénomène à lui seul explique l’intérêt d’une grande régie d’écoute. En effet, plus une pièce est grande, plus l’écart de temps entre le son direct et les premières réflexions est important. Cela implique deux choses :\n\nNotre cerveau favorisera le son direct plus facilement (effet de précédence)\nÀ partir d’une certaine taille, l’effet du filtre en peigne se mue en information d’acoustique pour notre cerveau. Au-delà de 40 ms (trajet d’une première réflexion d’environ 14 m), l’écart entre le son direct et les premières réflexions est tel que nous entendons un écho (effet Haas).\n\nAfin de réduire au maximum les effets des filtres en peignes, il est recommandé de placer des traitements aux points de réflexion critique par rapport à la position d’écoute (voir schéma ci-dessus).\n\n\n\n\n\nFigure 3.5: Même filtre en peigne, avec une atténuation de 20 dB sur la reflexion"
  },
  {
    "objectID": "generalites/l_acoustique_des_salles.html#traitement-acoustique",
    "href": "generalites/l_acoustique_des_salles.html#traitement-acoustique",
    "title": "3  Acoustique des salles",
    "section": "3.3 Traitement acoustique",
    "text": "3.3 Traitement acoustique\nGrâce aux différents points abordés ci-dessus, nous avons maintenant bien l’idée que l’acoustique d’un lieu est un des facteurs les plus déterminants sur le rendu sonore. Mais c’est aussi celui sur lequel il est plus difficile et technique d’intervenir.\nOn favorisera au maximum une architecture optimisée pour l’acoustique. Dans ce but, il convient de n’avoir aucune surface parallèle, cela permettant de grandement limiter l’apparition d’ondes stationnaires. On choisira également des matériaux avec des propriétés acoustiques intéressantes (plâtre et carrelage sont à proscrire, au profit du bois par exemple).\nOn se posera ensuite la question des endroits de la pièce les plus propices pour y positionner un évènement sonore (enceinte, musicien, etc.). On cherchera donc un point où la contribution des différents modes semble équilibrée. Pour cela, il suffit de se munir d’une enceinte et d’y diffuser une musique ou un signal test qui nous est familier. En déplaçant l’enceinte, on pourra évaluer la contribution acoustique de la pièce en différents points.\nUne fois ces considérations prises en compte, on pourra alors aborder le traitement de l’acoustique.\n\n\n\n\n\n\nAvertissement\n\n\n\nIl ne faut pas confondre isolation acoustique et traitement acoustique. Dans le premier cas, on chercher a limiter la contribution sonore d’un lieu sur son environnement, dans l’autre on cherche à améliorer la propagation du son dans un espace donné. Une isolation acoustique satisfaisante nécessite de lourds travaux, voire l’aménagement d’une “boîte dans une boîte”. Ces notions d’acoustiques dépassent le cadre de ce cours.\n\n\n\n3.3.1 Les types de traitements\nOn trouve, en général, deux types de traitements :\n\nLes absorbeurs, qui réduisent l’énergie d’une onde sonore à son impact.\nLes diffuseurs, qui répartissent l’énergie d’une onde sonore dans l’espace.\n\nDans un lieu où la quantité de réverbération est jugée trop importante, on utilisera des absorbeurs. À l’inverse, dans un lieu où l’on souhaite préserver la quantité de réverbération, mais en évitant les phénomènes de modes ou de filtre en peignes, on utilisera des diffuseurs.\nDans de petits lieux, l’usage de diffuseur semble contre-productif, la priorité étant d’absorber au maximum les premières réflexions, celle-ci arrivant très rapidement après l’émission du son direct.\n\n\n3.3.2 Considération d’acoustique pour le travail de son\nIl est vivement recommandé d’installer un studio, de prise de son ou de monitoring, dans un lieu plutôt grand. En effet, plus le lieu est grand, plus il sera facile de positionner un point de prise de son ou d’écoute suffisamment éloigné des parois afin de minimiser l’influence des premières réflexions. Aussi, plus le lieu est grand, plus l’espace y sera suffisant pour installer des traitements acoustiques. Certains types de traitements, comme les basstraps, peuvent prendre une place bien trop importante pour être installée dans des pièces de dimension habituelle (chambres, bureau, etc.). On se rappellera aussi de choisir une pièce de travail avec le minimum de surface parallèle, afin de limiter les ondes stationnaires.\nEn ce qui concerne les traitements en eux-mêmes, il est vivement recommandé de traiter en priorité le bas du spectre. L’ajout de basstrap est donc prioritaire sur le reste des traitements. Plus la longueur d’onde à traiter est grande (donc la fréquence grave), plus la taille des matériaux devra être importante. On retrouve donc le point abordé précédemment : traiter une pièce correctement, demande un certain espace. Par ailleurs, il est important que les traitements appliqués à un lieu soient linéaires en fréquence, c’est-à-dire qu’il ne se concentre pas sur une seule zone du spectre. Cela arrive souvent avec les kits de mousses peu onéreux, mais n’ayant une réelle efficacité que dans les médiums et hautes fréquences.\nPour une régie d’écoute, on sera tenté de privilégier des traitements d’absorption. En effet, une réverbération trop longue dans une régie de monitoring risque fort de fausser certaines prises de décisions (distance des microphones à la source, quantité de réverbération, etc.). À l’inverse, une pièce avec un temps de réverbération trop court pourra créer un sentiment d’inconfort, voire de malaise.\nPour une salle de prise de son, l’idéal est de disposer d’un grand espace avec un traitement acoustique principalement basé sur de la diffusion, pour ensuite disposer de traitements absorbants amovibles permettant de sculpter le rendu acoustique en fonction de la prise de son à réaliser. Pour des petits lieux (- de 25 m²), on cherchera à absorber au maximum afin de limiter les effets de filtre en peigne."
  },
  {
    "objectID": "generalites/notions_electronique.html#les-grandeurs-physiques",
    "href": "generalites/notions_electronique.html#les-grandeurs-physiques",
    "title": "4  Notions élémentaires d’électronique",
    "section": "4.1 Les grandeurs physiques",
    "text": "4.1 Les grandeurs physiques\nCommençons par aborder les grandeurs physiques liées à l’électricité.\n\n4.1.1 L’intensité\nL’intensité électrique, notée I et exprimée en Ampère (A), est une grandeur permettant de mesurer le débit du courant électrique. Ceci est parfaitement analogue à un débit d’eau. Si un robinet est faiblement ouvert, l’écoulement de l’eau sera faible, s’il est complètement ouvert, le débit sera fort.\n\n\n4.1.2 La tension\nLa tension, généralement notée U et exprimée en Volt (V), désigne une différence de potentiel entre deux points d’un circuit. Imaginons deux réservoirs d’eau, remplis d’un volume différent et connectés par une valve. Dans ce cas, la différence de potentiel serait la différence du volume d’eau entre les deux réservoirs. En d’autres termes, s’il n’y a pas de tension, il n’y a pas de débit.\n\nOn choisit, en général, la masse, valant zéro volt, comme point de référence.\n\nDans le cas de l’audio, la tension électrique du signal sonore est homologue à la variation de pression.\nTout comme la pression acoustique, il est possible de rendre compte d’une variation de tension électrique à un niveau sonore en décibel. Le relation liant la tension et le niveau est :\n\\[ L_{dB} = 20 \\, log (\\frac{U}{U_{ref}}) \\]\nIl existe plusieurs valeurs pour U_{ref}, donnant lieu à différentes unités de mesures :\n\ndBm, définie à l’apparition du téléphone, propose \\(U_{ref} = 0.775 V\\) pour une impédance de \\(600 \\omega\\). Cette impédance correspond à celle des lignes téléphoniques.\ndBu / dBv, qui ne tient plus compte de la charge d’impédance, \\(U_{ref} = 0.775 V\\).\ndBV, où \\(U_{ref} = 1 V\\)\n\n\nLorsque la tension double, le niveau augmente de six décibels. Lorsque la tension est multiplié par dix, le niveau augmente de vingt décibels.\n\nOn peut également définir l’augmentation du niveau sonore par rapport à la puissance du signal. On admet que :\n\\[ P = \\frac{U^2}{Z} \\, U = \\sqrt{P \\times Z} \\]\nOù \\(P\\) est la puissance. En remplaçant dans l’équation précédente, on trouve :\n\\[ L_{dB} = 20 \\, log (\\frac{\\sqrt{P}}{\\sqrt{P_{ref}}}) \\&gt; = 10 \\, log (\\frac{P}{P_{ref}}) \\]\n\nLorsque la puissance double, le niveau augmente de trois décibels. Lorsque la puissance est multiplié par dix, le niveau augmente de dix décibels.\n\nOn utilisant la loi d’ohm (voir ci-dessous) et la relation entre la puissance, la tension et l’impédance, on trouve également que :\n\\[ P = U \\times I \\]\n\n\n4.1.3 L’impédance\nNous connaissons, en général, la loi d’Ohm. Celle-ci permet de donner une relation entre l’intensité du courant et sa tension, aux bornes d’un composant d’un circuit (aussi appelé dipôle).\n\\[ U = R \\times I \\]\nOù \\(R\\) est la résistance du dipôle. Elle traduit la facilité d’un courant à se déplacer dans le dipôle. Pour reprendre les analogies ci-dessus, la résistance correspondrait à une valve. À tension constante, si la résistance tend vers zéro, le débit est très important. Si la résistance tend vers l’infini, le débit sera très faible. Si elle est nulle, alors nous sommes dans le cas d’un court-circuit (interrupteur fermé). Si elle est infinie, cela traduit une absence de connexion entre deux points d’un circuit (interrupteur ouvert). L’unité de cette résistance est l’ohm.\nL’impédance traduit elle aussi l’opposition d’un circuit au passage d’un courant électrique, mais dans le cas d’une tension oscillante. Dès lors, l’impédance englobe les effets de résistance, de capacitance et d’inductance (voir ci-dessus)."
  },
  {
    "objectID": "generalites/notions_electronique.html#les-composants-électroniques",
    "href": "generalites/notions_electronique.html#les-composants-électroniques",
    "title": "4  Notions élémentaires d’électronique",
    "section": "4.2 Les composants électroniques",
    "text": "4.2 Les composants électroniques\n\n4.2.1 Les composants passifs\n\n\n\n\n \n\n\n\n\n\n\n\n\n \n\n\nFigure 4.1: Représentation d’une résistance et de son symbole\n\n\nÉtudions maintenant les composants électroniques les plus communs. Nous avons en premier les résistances. Ce sont des dipôles purement résistifs. Leur valeur s’exprime en ohm. Une résistance s’oppose donc au passage du courant. Pour rappel, la tension a ses bornes est \\(U = R \\times i\\).\n\n\n\n\n \n\n\n\n\n\n\n\n\n \n\n\nFigure 4.2: Représentation d’un condensateur et de son symbole\n\n\nViennent ensuite les condensateurs. Ils sont constitués de matériaux conducteurs séparés par une couche isolante. La relation entre tension et intensité à ses bornes en régime oscillant est :\n\\[ U = Z_c \\times I \\]\nOù \\(Z_c\\) est l’impédance d’un condensateur idéal. Nous pouvons ici la même analyse que plus haut, quand \\(Z_c\\) tend vers l’infini le courant ne passe plus, quand \\(Z_c\\) tend vers 0, le débit est important. L’impédance d’un condensateur est fonction de sa capacité (noté C, et s’exprime en farads).\n\\[ Z_c = \\frac{1}{jC\\omega} \\&gt; = 2 \\pi \\, f\\]\nSi la fréquence \\(f\\) tend vers l’infini, \\(Z_c\\) tend vers zéro, si la fréquence tend vers zéro, \\(Z_c\\) tend vers l’infini. On constate donc que l’impédance d’un condensateur varie en fonction de sa fréquence. On peut assimiler un condensateur à un interrupteur ouvert en basse fréquence et à un interrupteur fermé en haute fréquence.\n\n\n\nSymbole d’une bobine\n\n\nTerminons sur les bobines. Ces composants sont constitués d’un enroulement de câble en cuivre et possède une inductance notée L et s’exprimant en henrys. Étudions à nouveau la relation entre tension et intensité, aux bornes d’une bobine :\n\\[ U = Z_L \\times I \\]\nOù \\(Z_L\\) est l’impédance d’une bobine idéale. Cette impédance se calcule grâce à la relation suivante :\n\\[ Z_L = j\\omega L = 2 \\pi \\, f \\]\nSi la fréquence \\(f\\) tend vers l’infini, \\(Z_L\\) tend vers l’infini. Si \\(f\\) tend vers zéro, alors \\(Z_L\\) tend vers zéro. On observe donc le comportement inverse du condensateur. Une bobine se comporte comme un court-circuit en basse fréquence et comme un interrupteur ouvert en haut fréquence.\n\nOn admet j comme un outil mathématique permettant de simplifier certaines écritures et certains calculs. On l’appelle le nombre complexe, tel que \\(j^2 = -1\\). Dans nos applications, sa présence dans les relations des impédances de condensateur et de bobine implique un déphasage de \\(-\\frac{\\pi}{2}\\) pour un condensateur, et, de \\(\\frac{\\pi}{2}\\) pour une bobine.\n\nL’association de résistances, de condensateurs et de bobines donne des circuits RL, RC où RLC, permettant de réaliser des opérations de filtrage sur le signal.\n\n\n4.2.2 Tubes & semi-conducteurs\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n \n\n\nFigure 4.3: Tubes, transistor et circuits intégrés\n\n\nLes tubes, tubes à vide, ou parfois, lampes, sont historiquement les premiers composants permettant d’amplifier le signal, contre une certaine tension d’alimentation. On les retrouve donc dans les préamplificateurs, égaliseurs, compresseurs, et autres amplificateurs jusque dans les années soixante. Ils sont alors progressivement remplacés par les transistors, composants appelés semi-conducteurs. Ces transistors permettent de réaliser la même amplification du signal qu’une lampe, mais sont beaucoup plus petits et demandent aussi moins de puissance électrique pour réaliser le même facteur d’amplification (aussi appelé gain). Peu de temps après la mise au point des transistors, les circuits intégrés sont inventés. Ces petites boîtes renferment plusieurs transistors, et peuvent également servir à l’amplification de signaux.\nIl est très important de savoir que l’invention du transistor et des circuits intégrés est sans doute l’avancée technologique la plus importante du siècle dernier. Elle a permis le développement exponentiel de l’industrie informatique grâce à la miniaturisation des composants.\nL’utilisation de tubes, de transistors ou de circuits intégrés au sein des machines audio, est souvent associée à une certaine « couleur ». Il y aurait donc un son des tubes, un son des transistors et un son des circuits intégrés. Les différences entre ces dipôles apparaissent principalement dans les zones de non-linéarité des composants, typiquement dans leur zone de saturation. La saturation apparaît lorsque la tension du signal amplifiée dépasse la tension d’alimentation du composant responsable de cette amplification. On observe alors l’apparition de certaines harmoniques. La distribution des harmoniques générés est différente en fonction du dipôle utilisé.\nIl est compliqué d’attribuer une couleur sonore particulière à un composant. En effet, le comportement d’un composant est fondamentalement conditionné par la topologie du circuit dans lequel il est utilisé ainsi que par les autres composants qui l’entourent. Il convient donc, à l’humble avis de l’auteur, d’être relativement prudent sur des expressions telles que « son des tubes » ou « son des transistors », particulièrement quand il s’agit de dire que l’une des technologies « sonnerait mieux » que l’autre. L’histoire de l’électronique musicale regorge d’exemples et de contre-exemples pour chacune de ces affirmations."
  },
  {
    "objectID": "generalites/notions_electronique.html#linfluence-de-limpédance-entre-différents-appareils.",
    "href": "generalites/notions_electronique.html#linfluence-de-limpédance-entre-différents-appareils.",
    "title": "4  Notions élémentaires d’électronique",
    "section": "4.3 L’influence de l’impédance entre différents appareils.",
    "text": "4.3 L’influence de l’impédance entre différents appareils.\nSur la fiche technique des appareils, on trouve des valeurs pour son impédance d’entrée et son impédance de sortie. Imaginons que nous connections un appareil A dans un appareil B. En pratique, nous faisons en sorte que l’impédance de sortie de l’appareil A soit dix fois inférieure à l’impédance d’entrée de l’appareil B. À partir du moment où ces impédances sont proches, voire que l’impédance de sortie de A soit plus grande que celle d’entrer de B, nous allons atténuer le signal transitant entre les deux appareils. Étudions de plus près ce phénomène.\nSoit le schéma électronique ci-dessous. On appelle \\(U_{out}\\) la tension de sortie de l’appareil A et \\(Z_{out}\\) son impédance de sortie. De façon similaire, on appelle \\(U_{in}\\) la tension d’entré de l’appareil B et \\(Z_{in}\\) son impédance d’entré.\n\n\n\n\n\nDans ce circuit, \\(Z_{eq} = Z_{in} + Z_{out}\\) et \\(U_{out} = Z_{eq} \\times i\\). Alors, \\(i = \\frac{U_{out}}{Z_{eq}} = \\frac{U_{out}}{Z_{in} + Z_{out}}\\). Toujours grâce au circuit ci-dessus, on peut dire que \\(U_{in} = Z_{in} \\times I\\). En remplaçant dans l’expression précédente on trouve : \\(\\frac{U_{in}}{Z_{in}} = \\frac{U_{out}}{Z_{in} + Z_{out}}\\)\n\\[ \\frac{U_{in}}{U_{out}} = \\frac{Z_{in}}{Z_{in} + Z_{out}} = \\frac{1}{1+\\frac{Z_{out}}{Z_{in}}} \\]\nDès lors, si \\(Z_{in}\\) est très grand devant \\(Z_{out}\\), alors \\(U_{in}\\) tend vers \\(U_{out}\\). Si \\(Z_{out}\\) est très grand devant \\(Z_{in}\\), alors \\(U_{in}\\) tend vers \\(0\\).\nCela nous amène à démontrer l’affirmation ci-dessus. Maintenant, nous savons que dans un circuit, l’impédance varie en fonction de la fréquence. Dès lors, une mauvaise adaptation d’impédance ne fera pas que diminuer l’amplitude du signal, mais filtrera aussi une partie de spectre, généralement les hautes fréquences.\n\nOn considère aussi l’adaptation d’impédance en tension. Lorsque que nous considérons la puissance les relations sont différentes (cf section sur les hautparleurs)."
  },
  {
    "objectID": "generalites/production_musicale.html#les-acteurs-de-la-réalisation-dune-œuvre-enregistrée",
    "href": "generalites/production_musicale.html#les-acteurs-de-la-réalisation-dune-œuvre-enregistrée",
    "title": "5  Description d’une production musicale type",
    "section": "5.1 les acteurs de la réalisation d’une œuvre enregistrée",
    "text": "5.1 les acteurs de la réalisation d’une œuvre enregistrée\nNous allons ici rapidement discuter des différents rôles apparaissant dans la production d’une œuvre musicale enregistrée. Ceux-ci sont volontairement très séparés, bien que dans les cas pratiques, une personne puisse en incarner plusieurs.\nLe compositeur est la personne qui a composé la mélodie et l’harmonie de l’œuvre.\nL’arrangeur est chargé de l’orchestration (choix des instruments) et l’écriture des différentes partitions.\nL’interprète a la responsabilité de retranscrire une partition le plus justement possible, à la fois dans sa dimension technique et sensible.\nLe directeur artistique (ou DA) supervise l’ensemble de l’enregistrement. Il aura, par exemple, à choisir le preneur de son, le mixeur ou dans quel studio enregistrer. Lors de la session d’enregistrement, il aura à diriger les musiciens (comme un réalisateur dirige ses acteurs au cinéma) afin de leur faire jouer la meilleure interprétation possible pour l’œuvre. Lors du mixage, il sera le principal interlocuteur du mixeur. Pour faire court : il est le garant de l’orientation esthétique du projet.\nLe producteur finance l’ensemble de projets. C’est donc un investisseur qui attend un retour sur investissement.\n\nL’appellation abusive de « producteur » pour parler du directeur artistique vient d’un anglicisme du mot « producer ». Le producteur est donc bien l’équivalent du directeur artistique dans les pays anglo-saxons. Si le DA a besoin d’un certain talent, le producteur a surtout besoin d’argent.\n\nLe preneur de son est chargé d’enregistrer les musiciens et musiciennes. Il a donc un rôle premier très technique : il doit inscrire sur un support les ondes sonores produites par ces musiciens. Il a également un rôle esthétique très important, d’un point de vue sonore, car le choix du dispositif de prise de son aura un fort effet sur la suite de la vie de l’œuvre.\nLe mixeur intervient après la prise de son et doit réaliser une sommation de l’ensemble des points de captations (microphone) vers un format écoutable par le grand public (mono, stéréo, 5.1, Ambisonique, Dolby Atmos, etc.). Son rôle esthétique est fortement contraint par le travail de prise de son. Si celle-ci est réussie, il pourra amplifier et bonifier les choix de production. Dans le cas contraire, il devra lutter pour essayer de faire sortir le meilleur d’une matière imparfaite.\nLe technicien de mastering est le dernier maillon de la chaîne. Son rôle premier sera de préparer le travail de mixage à aux supports de diffusion. Il se devra également d’offrir une oreille nouvelle sur le travail réalisé au mixage."
  },
  {
    "objectID": "generalites/production_musicale.html#la-préproduction",
    "href": "generalites/production_musicale.html#la-préproduction",
    "title": "5  Description d’une production musicale type",
    "section": "5.2 La préproduction",
    "text": "5.2 La préproduction\nLa préproduction concerne toutes les étapes d’une œuvre enregistrée qui ont lieu avant ledit enregistrement. On parlera donc en premier lieu de la composition et particulièrement de l’arrangement.\nLa qualité d’un arrangement aura une influence énorme sur la facilité à mixer une œuvre. Si les instruments sont astucieusement répartis sur l’ensemble du spectre sonore, cela sera une difficulté de moins à gérer au mixage par exemple.\nIl est aussi courant pour des artistes de réaliser des « démos ». Celles-ci sont souvent des enregistrements réalisés en home studio afin de définir un cap esthétique pour la suite de la production sonore. C’est un atout extrêmement précieux pour un preneur de son, cela permet de rapidement identifier quel est le projet esthétique de l’œuvre."
  },
  {
    "objectID": "generalites/production_musicale.html#la-production",
    "href": "generalites/production_musicale.html#la-production",
    "title": "5  Description d’une production musicale type",
    "section": "5.3 La production",
    "text": "5.3 La production\nC’est ici que le travail du preneur de son commence. L’étape de production consiste à fixer les interprétations définitives. Le premier objectif est donc de s’assurer du bon enregistrement de tous les canaux prévus. Bien sûr, l’enjeu n’est pas seulement technique, mais aussi esthétique. Et il n’est pas moindre, les choix pris lors de la prise de son seront des carcans impossibles à outrepasser lors de la phase de mixage. Enfin, l’élément le plus déterminant de cette étape est d’obtenir des musiciens leurs meilleures interprétations. La présence d’un directeur artistique est d’une aide précieuse afin de diriger et d’orienter les musiciens. Il permet aussi de faire le lien entre les artistes et l’équipe technique, en exprimant les besoins des uns aux autres.\nSur les projets les plus modestes, le poste de directeur artistique est souvent sacrifié. Il en va donc à l’ingénieur du son de, parfois, remplir ce rôle."
  },
  {
    "objectID": "generalites/production_musicale.html#la-postproduction",
    "href": "generalites/production_musicale.html#la-postproduction",
    "title": "5  Description d’une production musicale type",
    "section": "5.4 La postproduction",
    "text": "5.4 La postproduction\nArrivé à ce stade, la majorité du travail est déjà accompli, il ne reste que le mixage et le mastering. Classiquement, chacune de ces tâches incombe à un technicien différent. Le travail du mixeur consistera à réaliser la sommation, généralement en stéréo, de l’ensemble des canaux enregistrés lors de la prise de son. Afin de faire cohabiter tous ces signaux, il est commun d’utiliser des traitements pour les répartir sur l’ensemble du spectre et de gérer leur dynamique. Parfois, ces traitements remplissent un rôle esthétique, en déformant le signal d’origine pour aboutir à une nouvelle matière.\nUne fois le travail du mixeur terminé, le mastering commence. Le but et d’homogénéiser l’ensemble des titres d’un disque, en volume, en dynamique et en couleur. Ensuite, il convient aussi de définir le niveau de sortie général du disque. La dernière étape consistera à monter l’ordre des morceaux pour le disque, d’y inscrire les métadonnées (nom de l’artiste, des titres, genre musical, etc.) et de générer le fichier final, dédier à l’exploitation."
  },
  {
    "objectID": "outils_equipements/le_chemin_du_signal.html",
    "href": "outils_equipements/le_chemin_du_signal.html",
    "title": "6  Le chemin du signal",
    "section": "",
    "text": "La première mission d’un preneur de son est d’assurer l’arrivée à bon port des signaux dans l’enregistreur. En effet, toute notion de mise en scène sonore et d’esthétique devient très secondaire si le contenu n’a pas été enregistré.\nLe diagramme ci-dessous reprend les principaux étages rencontrés par un signal audio dans un contexte de production numérique. Il est essentiel d’être le plus familier possible avec ces différents composants.\n\n\n\nFigure 6.1: Le chemin du signal. Elle peut-être agrandie en ouvrant l’image dans un nouvel onglet.\n\n\nNous pourrions catégoriser à partir de ce schéma différents « milieux ». Tout d’abord, nous avons le milieu acoustique, où nous trouverons toutes sortes d’instruments de musique, les différents lieux dans lesquels ils pourront s’y trouver. C’est donc le domaine de l’onde sonore mécanique.\nOn trouve ensuite le milieu analogique, où l’onde sonore est représentée, de façon analogue, par des grandeurs électriques. Celles d’un signal sonore dans un circuit analogique sont fonction, par exemple, de la variation de la pression atmosphérique provoquée en un point par une onde sonore. Les éléments clefs du milieu analogique sont les préamplificateurs et les amplificateurs, mais on trouve aussi certains traitements, comme les égaliseurs et les compresseurs. On définira « analogique » comme une représentation dans laquelle les grandeurs (tension, courant, etc.) qui entrent dans les calculs sont représentées par des grandeurs analogues et qui varient de manière identique (définition du CNRTL).\nPour passer du milieu acoustique au milieu analogique, et vice-versa, on utilise des microphones et des haut-parleurs. Tous deux sont des transducteurs, permettant de transformer une énergie en une autre. Le microphone transforme une énergie mécanique en énergie électrique. Le haut-parleur réalise l’opération inverse.\nOn en vient ensuite au milieu numérique. Fondamentalement, le signal est toujours de nature électrique, mais il a subi une opération très importante nommée échantillonnage. On a donc mesuré à intervalle régulier la tension électrique générée par l’onde sonore. Le passage par le numérique permet une myriade de traitements sur le signal, beaucoup plus complexes que ceux permis par l’électronique analogique. L’audio numérique permet aussi un stockage de l’information à moindre coût et l’acheminement d’un grand nombre de voies (canaux) grâce à un faible nombre de modulations (câble).\nL’appareil permettant de passer du milieu analogique au milieu numérique est le convertisseur analogique/numérique. Il ne s’agit pas d’un transducteur, car les signaux d’entrées et de sorties sont de même nature électrique. Pour opérer l’opération inverse, on utilise un convertisseur numérique/analogique.\nLe milieu informatique nous permet d’utiliser des applications relatives aux traitements du son par le biais d’ordinateurs. Il s’agit aujourd’hui indubitablement de notre outil de travail principal. Nous y réalisons la grande majorité des traitements audio, ainsi que l’enregistrement et le routage des sources.\nLe lien entre un ordinateur et un convertisseur A/N/A se fait grâce à un bus de sérialisation associé à un pilote (ou driver). L’ensemble des deux permet de mettre en forme la donnée numérique et de la rendre compréhensible à l’ordinateur.\nChaque élément évoqué ci-dessus sera abordé dans des sections dédiées dans la suite de ce livre."
  },
  {
    "objectID": "outils_equipements/les_microphones.html#petit-historique-des-microphones",
    "href": "outils_equipements/les_microphones.html#petit-historique-des-microphones",
    "title": "7  Les microphones",
    "section": "7.1 Petit historique des microphones",
    "text": "7.1 Petit historique des microphones\nSans vouloir rentrer dans un récit exhaustif sur l’invention et l’évolution des microphones, relater les moments clefs de cette technologie permet d’avoir une vision globale du marché d’aujourd’hui.\nLa nécessité de capter un évènement sonore grâce à un microphone provient de trois besoins :\n\nle transmettre (télécommunication)\nl’amplifier (concert, spectacle vivant)\nl’enregistrer (industrie du disque)\n\nEn 1876, Alexandre Graham Bell propose un système à base liquide, permettant de transformer une onde sonore en tension électrique. Le système ne fut jamais réellement exploité, car le rendu sonore était jugé trop peu satisfaisant.\nLe premier type de microphone utilisé industriellement est le microphone à charbon (au UK, par David Edward Hugues, aux US par Emile Berliner et Thomas Edison. Le brevet sera d’ailleurs disputé, avec un gain de cause pour Edison malgré des démonstrations publiques de Hugues antérieur aux publications d’Edison). En raison de sa faible bande passante et de son niveau de bruit élevé, il se révèle de piètres qualités pour l’enregistrement et la transmission de la musique. Il aura, par contre, une place de choix dans les téléphones durant de longues décennies.\nUn premier brevet, peut-être même le premier, pour un microphone dynamique à bobine mobile est attribué à l’ingénieur et industriel allemand Ernst Werner von Siemens en 1877. La technologie de la bobine mobile a été mise en pratique dans les années 1920 lorsque la Marconi-Sykes Company a créé le magnétophone pour la radio britannique. Ils s’imposent d’abord dans le monde du concert, dès les années 40, pour leur grande résistance mécanique.\nViennent ensuite les microphones à condensateur, dont les premiers modèles remontent à 1916, par le chercheur Edward Wente. Ces microphones sont tout d’abord réputés assez capricieux, leurs réponses en fréquences pouvant varier significativement en fonction de l’humidité de l’air et de la température.\nÀ cause de ces variations sonores présentes dans les premiers microphones à condensateur, on leur préférera un temps les microphones à ruban. Ils sont inventés en 1923 par Harry Olson. Ils sont par contre d’une grande fragilité mécanique.\nGeorge Neumann est un des noms à connaître dans cette histoire des microphones. On lui doit, entre autres, la stabilisation des microphones statiques. Il sera aussi le premier à produire un microphone (U87) utilisant un transistor en lieu et place des traditionnels tubes.\nÀ partir des années 1970, les microphones dynamiques sont adoptés en studio d’enregistrement, notamment portés par la marque Shure. Ces microphones ont la grande qualité d’être très robustes, et remplaceront leurs homologues à ruban dans bien des cas.\nDepuis, les principales améliorations ont concerné la robustesse d’une part, et la miniaturisation des dispositifs d’autre part, menant ainsi au développement des capsules MEMS."
  },
  {
    "objectID": "outils_equipements/les_microphones.html#les-types-et-technologies-de-microphones",
    "href": "outils_equipements/les_microphones.html#les-types-et-technologies-de-microphones",
    "title": "7  Les microphones",
    "section": "7.2 Les types et technologies de microphones",
    "text": "7.2 Les types et technologies de microphones\nAvant d’aborder en détail certaines constructions de microphones, il convient de faire attention à certains raccourcis associant des méthodes de fabrications à un niveau présumé de qualité. Par exemple, il est commun d’associer les microphones à électret à une construction « bas de gamme ». Or, c’est oublier que la série 4000 de chez DPA, considérée par beaucoup comme une référence indétrônable de la prise de son, ne contient que des microphones à électret. Les MEMS souffrent du même biais, ceux-ci se retrouvent pourtant de plus en plus souvent sur des microphones ambisoniques, comme le Zyla ou le SPC mic.\nNous allons maintenant aborder les types de microphones suivants :\n\nLes microphones électrostatiques/à condensateur\nLes microphones à ruban\nLes microphones dynamiques\n\n\n7.2.1 Les microphones électrostatiques/à condensateur\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.1: Neumann U87, Schoeps CMC64, Line Audio CM4\n\n\nCe sont, historiquement, les premiers microphones à permettre une captation du spectre audible satisfaisante. Ils sont cependant très sensibles aux conditions de température et d’humidité et il fallut attendre les années trente pour que ce problème cesse. Ils nécessitent une alimentation externe, appelée alimentation fantôme, normalisée à +48V. Il existe deux familles de microphones électrostatiques, les condensateurs à hautes fréquences et condensateur polarisés en courant continu.\nLes microphones à condensateur polarisés en courant continu ont le fonctionnement le plus commun. Un courant continu vient polariser la capsule/condensateur. Lorsqu’une onde sonore rencontre la capsule, une de ses armatures se déforme et génère une variation de tension analogue à la variation de pression.\nLes microphones à condensateur à haute fréquence proposent une approche différente. Un oscillateur est intégré dans le microphone et la variation de pression enregistrée par le condensateur vient moduler la fréquence de cet oscillateur. Le signal est ensuite démodulé dans la plage audible. Cette méthode de construction offre une impédance de sortie plus faible et une plus grande résistance aux variations de conditions climatiques.\nConcernant leurs caractéristiques, ces microphones possèdent des réponses en fréquence souvent très linéaire et une excellente réponse en transitoire. Leur niveau de sortie (sensibilité) est élevé. Leur impédance de sortie est basse.\nExemples : Neumann U87/AKG C414/Shoeps CMC4/Série 4000 DPA/Série MKH Sennheiser\n\n\n7.2.2 Les microphones à ruban\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.2: Royer R121, Cascade Vinjet, Coles 4038\n\n\nLes microphones à ruban souvent préférés à leurs homologues statiques dans les débuts de la musique enregistrée. Leur fonctionnement repose sur l’utilisation d’une feuille métallique placée entre deux aimants. Lorsqu’une onde sonore rencontre cette feuille (le ruban), celle-ci vibre et perturbe le champ électromagnétique créé par les aimants et génère une tension analogue à la variation de pression.\nD’un point de vue sonore, les microphones à ruban ont souvent un bas du spectre assez généreux et une réponse plutôt douce pour les hautes fréquences. Ils sont aussi connus pour avoir une impédance de sortie assez élevée et un niveau de sortie faible. Attention à l’alimentation fantôme (+48V), elle peut endommager le microphone.\nExemples : Royer R121/Cohles/Beyerdynamic M160\n\n\n7.2.3 Les microphones dynamiques\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.3: Shure SM57, Electro-Voice RE20, Sennheiser MD441\n\n\nLes microphones dynamiques sont conçus pour des conditions d’utilisation rudes, où les niveaux sonores sont élevés et où le risque de chute est important. Ils sont donc monnaie courante en sonorisation. Leur membrane est attachée à une bobine entourant un aimant. Lorsqu’une onde sonore la met en vibration, la bobine se déplace autour de l’aimant, et, par perturbation du champ électromagnétique, génère une tension de sortie analogue à la variation de pression.\nLeur réponse en fréquence est souvent accidentée, particulièrement dans le haut du spectre. Cela peut être vu comme un inconvénient ou comme un outil de « coloration » du son. Comme leurs homologues à ruban, ils possèdent un niveau de sortie faible et une impédance de sortie élevée.\nExemples : Shure SM57/Electrovoice RE20/Sennheiser MD441\n\n\n7.2.4 La taille des membranes\nLa taille des membranes influe sur la captation du son. Plus la capsule est grande, plus les fréquences aiguës seront diffractées et donc atténuées dans la prise de son. Un microphone à petite membrane est donc techniquement un microphone plus « juste ». Cependant, l’emploi de large membrane permet aussi d’adoucir un surplus d’énergie dans le haut du spectre.\n\n\n7.2.5 Microphones à tubes ou transistors?\nHistoriquement, les tubes ont été les premiers composants électroniques à permettre l’amplification du signal. Le transistor est apparu à la fin des années 40 et a permis de remplir les mêmes fonctions qu’un tube, par une consommation moindre et avec un encombrement beaucoup plus faible.\nCertains microphones continuent à être fabriqués avec des tubes, préférant leur comportement vis-à-vis du son. Une écrasante majorité est cependant fabriquée avec des transistors.\nLe choix entre un microphone à tube et un microphone à transistor semble cependant anecdotique par rapport à son type, à son placement et à sa directivité."
  },
  {
    "objectID": "outils_equipements/les_microphones.html#timbre-et-directivité",
    "href": "outils_equipements/les_microphones.html#timbre-et-directivité",
    "title": "7  Les microphones",
    "section": "7.3 Timbre et directivité",
    "text": "7.3 Timbre et directivité\nLa directivité d’un microphone permet de décrire sa capacité à réaliser une « écoute » sélective de son environnement. On rencontre les directivités suivantes :\n\n\n\n\n\nFigure 7.4: Représentation des six grandes directivités de microphones\n\n\n\n\n\nOmnidirectionnel : capte l’ensemble du champ sonore de façon indifférenciée.\nHypercardioïde : compromis entre Omnidirectionnel et cardioïde.\nCardioïde : capte à l’avant, mais rejette à l’arrière du microphone.\nSupercardioïde : ressers la zone d’écoute avant au prix de l’apparition d’une résurgence arrière.\nHypercardioïde : ressers davantage la zone d’écoute et augmente d’autant plus la résurgence arrière.\nBidirectionnel : capte à l’avant et à l’arrière, mais selon un lobe plus resserré qu’en cardioïde.\n\n\n\n\n                                                \n\n\nPlus la directivité d’un microphone est large, plus la contribution de l’acoustique est apparente. Le timbre est également aussi linéaire que possible. À l’inverse, plus la directivité tant à être étroite, plus le microphone aura une capacité à échantillonner seulement une zone de l’espace. Le timbre est, par contre, amoindri dans le bas du spectre. Les microphones omnidirectionnels sont donc les plus larges et les plus « neutres », tandis que les microphones bidirectionnels sont les plus focalisés et ont la plus importante perte dans le bas du spectre.\nLe preneur de son choisit donc une directivité en fonction de la tâche à accomplir. Les microphones directifs ont l’avantage de limiter la contribution d’évènements sonores que l’on ne souhaite pas capter. Les microphones omnidirectionnels ont la faculté d’être un dispositif de prise de son plus transparent, mais seront beaucoup plus sensibles à une acoustique moins optimale, ainsi qu’au bruit environnant.\nNous allons par la suite nous intéresser au cœur du microphone : sa capsule. Il existe deux familles de capsules, celles dites « à pression », et celles dites à « gradient de pression ».\n\n7.3.1 Capsules à pression\nUne capsule sensible à la pression est omnidirectionnelle : elle capte les fluctuations de pressions en un point. Mathématiquement, cette relation s’exprime, en coordonnées polaires, par :\n\\[ \\theta = 1 \\]\nL’angle d’incidence de l’onde sonore par rapport au microphone importe donc peu.\nPour réaliser une capsule à pression, on enferme une partie de la membrane dans un milieu acoustique à pression constante.\n\n\n7.3.2 Capsules à gradient de pression\nUne capsule à gradient de pression est sensible à la variation du champ de pression. Ces capsules ne sont plus omnidirectionnelles, mais bidirectionnelles : elles captent devant et derrière elles. Mathématiquement, une telle directivité s’exprime par la relation (en coordonnées polaires) :\n\\[ \\theta = cos(\\alpha) \\]\nOù \\(\\alpha\\) est l’angle d’incidence d’un son par rapport à la capsule.\nPour réaliser une capsule à gradient de pression, il suffit de laisser exposer les deux faces de la membrane aux variations de pressions.\n\n\n7.3.3 Et les autres directivités ?\nIl est possible, à partir des deux équations ci-dessus, de retrouver toutes les autres directivités. Par exemple, un microphone cardioïde a une équation de directivité polaire tel que :\n\\[ \\theta(\\alpha) = \\frac{1}{2}(1 + cos[\\alpha]) \\]\nElles découlent donc des deux directivités primaires : omnidirectionnelle et bidirectionnelle. Pour obtenir une directivité particulière, il suffit de « doser » l’influence de ces deux directivités. Par exemple, un microphone cardioïde possède une contribution égale de chacune d’elles. Plus on augmente la proportion de la directive bidirectionnelle, plus on tend vers un microphone supercardioïde, voire hypercardioïde. À l’inverse, augmenter la proportion de la directivité omnidirectionnelle fait tendre le microphone vers une directivité hypocardioïde.\nIl existe deux solutions pour agir sur la contribution des directivités primaires. La première consiste à utiliser un labyrinthe acoustique pour changer le milieu acoustique d’une des faces de la membrane. L’autre consiste à avoir une capsule omnidirectionnelle et une seconde bidirectionnelle et de sommer leur tension de sortie.\nLes microphones à multidirectivité permettent à l’utilisateur d’influer, soit sur le labyrinthe acoustique, soit sur la sommation des deux capsules. Il n’est pas rare que ces microphones soient moins performants qu’un microphone spécifiquement dédié à une seule directivité.\nOn retiendra donc :\n\\[ \\theta(\\alpha) = A + B \\cos (\\alpha) \\&gt; où \\&gt; A + B = 1\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.5: Directivités réelles du microphone (U 87)\n\n\n\n\n7.3.4 Directivités réelles & détimbrage\nNous avons jusque là considéré que la directivité d’un microphone était un phénomène indépendant de la fréquence. Or, cela n’est pas vrai. En d’autres termes, la directivité d’un microphone n’est pas la même en fonction de la fréquence de l’onde sonore lui arrivant. Typiquement, un microphone tendra vers une directivité plus resserrée dans le haut du spectre, et vers une directivité plus large dans le bas du spectre.\n\n\n\nFigure 7.6: Réponse en fréquence du microphone Neumann U87 (omnidirectionnel)\n\n\n\n\n\nFigure 7.7: Réponse en fréquence du microphone Neumann U87 (cardioïde)\n\n\n\n\n\nFigure 7.8: Réponse en fréquence du microphone Neumann U87 (figure en huit)\n\n\nCela signifie donc que positionner un microphone hors axe face à un évènement sonore n’aura pas seulement un effet sur le niveau du signal en sortie du microphone, mais également sur le timbre. On appelle alors « timbré », un évènement sonore capté plein axe par un microphone, et détimbré un évènement sonore capté hors axe.\nCe phénomène est un outil précieux pour les preneurs de son. Par exemple, lorsqu’on enregistre une voix, certains sons sont exagérés par le microphone, particulièrement les « s ». En tournant légèrement le microphone pour placer la voix hors axe, on peut déjà grandement améliorer les problèmes de sifflante avant même de penser à un éventuel traitement ultérieur."
  },
  {
    "objectID": "outils_equipements/transport_signaux_analogiques.html#anatomie-dun-câble",
    "href": "outils_equipements/transport_signaux_analogiques.html#anatomie-dun-câble",
    "title": "8  Transport de signaux analogiques",
    "section": "8.1 Anatomie d’un câble",
    "text": "8.1 Anatomie d’un câble\nLes câbles véhiculant un seul signal sont généralement constitués de quatre à cinq composants :\n\nd’un cœur composé d’un filament (souvent multibrin) en un métal conducteur, véhiculant le signal électrique.\nd’une gaine isolante (plastique) protégeant le cœur\nd’une tresse en cuivre connectée à la masse\nParfois, d’une feuille de cuivre, enrobant la tresse, permettant de réaliser une cage de faraday et de protéger le cœur des ondes électromagnétiques.\nEnfin, d’une dernière gaine isolante, permettant de protéger l’ensemble du câble.\n\n\n\n\nFigure 8.1: Coupe d’un câble\n\n\nCeux permettant de transporter plus de signaux rajouteront des cœurs multibrins entourés de leur gaine isolante. La plupart du temps, les câbles véhiculent un ou deux signaux à la fois, mais certains permettent d’en acheminer beaucoup plus (multipaire, Sub-D). On appelle un câble en fonction de ses connecteurs (ou fiches).\n\n8.1.1 Longueur du câble, son et impédance\nÉtudions la section d’un câble à modulation unique. Nous pouvons faire plusieurs observations. Le fil conducteur du signal, généralement en cuivre, est d’une longueur non négligeable. Plus cette longueur est importante, plus ce fil aura une résistance importante. De plus, ce fil est séparé de la tresse de masse (élément également conducteur) par un isolant. Il existe donc un effet de capacitance entre le point chaud et la masse. Enfin, la tresse de masse peut être comparée à une bobine, et possède donc une inductance. Nous pouvons donc assimiler un câble à un circuit RLC filtrant le haut du spectre audio.\nPhysiquement, notre description précédente est valide, mais elle est à corréler à l’impédance de sortie de la source. Typiquement, lors de l’utilisation d’un microphone statique, son impédance est suffisamment faible pour que la longueur du câble soit totalement transparente. Certains microphones dynamiques ou à ruban, possédant une impédance plus élevée, peuvent très légèrement souffrir de la longueur du câble.\nCe phénomène d’altération du timbre à cause de la longueur d’un câble a surtout lieu avec les instruments électriques passifs (guitares et basses). L’impédance de sorte de ces instruments est tellement grande que l’on peut aisément entendre la différence de son entre deux câbles.\n\nIl est amusant de constater que certains musiciens utilisent ce phénomène, et jouent avec des câbles volontairement trop longs, pour atténuer le haut du spectre. Brian May et Eric Johnson en sont deux exemples.\n\n\n\n8.1.2 Les connexions asymétriques\nLes connexions asymétriques permettent de transporter un signal entre une source et un récepteur. Il s’agit de la façon la plus simple de connecter deux appareils devant échanger des signaux. Cependant, sur de longue distance, le câble peut se comporter comme une antenne et induire sur le signal certaines ondes électromagnétiques (comme la radio). Ces connexions ne nécessitent qu’un fil conducteur par modulation.\n\n\n8.1.3 Les connexions symétriques\nLe but de ces connexions est de palier au problème des connexions asymétrique. Dans l’appareil émetteur, le signal à transporter est dupliqué, et ce duplicata est inversé en phase. Cette étape s’appelle la symétrisation.\n\nC’est deux signaux qui sont appelés point chaud (signal d’origine) et point froid (signal opposé en phase). Sur le trajet du câble, lorsqu’une perturbation électromagnétique est induite sur le signal, celle-ci s’inscrit en phase sur les deux conducteurs (point chaud et point froid). À l’arrivée, l’appareil récepteur inverse la phase du point froid et le somme avec le point chaud. Cette étape se nomme la dé-symétrisation. Ainsi le signal d’origine est sommé en phase, tandis que les interférences sont sommées hors phase et s’annulent.\nLes connexions symétriques nécessitent deux fils conducteurs pour chaque modulation."
  },
  {
    "objectID": "outils_equipements/transport_signaux_analogiques.html#les-fiches-connecteurs",
    "href": "outils_equipements/transport_signaux_analogiques.html#les-fiches-connecteurs",
    "title": "8  Transport de signaux analogiques",
    "section": "8.2 Les fiches & connecteurs",
    "text": "8.2 Les fiches & connecteurs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.2: Jack TS, Jack TRS, Jack Bantam, XLR, Sub-D 25\n\n\nOn appelle « fiche » les éléments mécaniques situés aux extrémités d’un câble et permettant sa connexion à un équipement. Celle-ci nous permet de facilement identifier le type de câble que nous avons entre les mains.\n\nContrairement aux dires de certains mythes, majoritairement reliée à l’audiophilie, la différence de matériau utilisé pour le contact de la fiche n’a pas d’incidence sur le son.\n\nOn rencontre principalement les connectiques jack TS, jack TRS, jack bantam, XLR et Sub-D. Les fiches jack TS ont deux points de connexion. Les fiches jack TRS, XLR et bantam en ont deux. Les Sub-D 25 en possèdent vingt-cinq.\n\nLes fiches jack TS sont souvent utilisées sur les instruments électriques et électroniques (guitare, basse, synthétiseurs, etc.). Ces connexions sont nécessairement symétriques. Les jack TRS sont un peu plus rare et se trouvent généralement sur des instruments avec des sorties stéréophoniques, ou sur des équipements audio possédant des entrés/sorties symétriques. La connectique XLR remplit fondamentalement le même usage qu’un jack TRS, mais offre un verrouillage mécanique, permettant de sécuriser la connexion. On la trouve principalement sur les microphones et sur préamplificateurs. L’avantage du jack TRS est son plus faible encombrement mécanique. On le préfère donc sur les appareils possédant un grand nombre d’entrées/sorties. Le jack bantam se trouve sur les boîtiers de patch. Leur petite taille permet une grande densité de point de connexion. Les patchbay prennent ainsi moins de place. On trouve aussi les connectiques Sub-D 25, principalement pour remplacer des connexions XLR. En effet, une seule connectique Sub-D 25 permet de remplacer huit câbles XLR."
  },
  {
    "objectID": "outils_equipements/transport_signaux_analogiques.html#exemples-pratiques",
    "href": "outils_equipements/transport_signaux_analogiques.html#exemples-pratiques",
    "title": "8  Transport de signaux analogiques",
    "section": "8.3 Exemples pratiques",
    "text": "8.3 Exemples pratiques\n\n8.3.1 Câble jack « mono »\nCe type de câble est souvent utilisé sur les instruments électriques. On l’appelle « mono » car il ne véhicule qu’un seul signal.\n\n\n8.3.2 Câble jack « stéréo »\nL’appellation de ce câble est ambiguë. Le terme stéréo fait référence à ses deux voies de connexion, cependant, le nom « stéréo » impliquerait qu’une des voies est destinée à alimenter l’enceinte gauche, et l’autre, le canal droit. Or, ce type de câble peut également convenir pour des connexions symétriques.\n\nOn notera qu’il est possible de brancher un câble jack TS dans une fiche TRS. Un seul des canaux sera alors acheminé.\n\n\n\n8.3.3 Câble « Y »\nCes câbles possèdent trois connecteurs, et sont le plus souvent équipés de jack TS. Ils permettent de récupérer un signal pour le transmettre sur deux appareils. Attention, la duplication du signal étant passive, on risque un problème d’impédance si l’impédance d’entrée des deux appareils est trop différente. Le cas d’école est souvent rencontré lorsqu’on branche deux casques sur le même amplificateur. Si l’impédance des deux casques est trop différente, un des deux aura presque l’intégralité du niveau du signal alors que l’autre sous-modulera.\n\n\n8.3.4 Les connexions d’insert\nCes câbles ont la particularité d’avoir une fiche jack TRS et deux fiches jack TS. En studio, on les rencontre très souvent pour insérer un périphérique de traitement dans la chaîne audio. Le « tip » du jack TRS est connecté au « tip » d’un des jack TS qui est connectés sur l’entré du périphérique. Le deuxième jack TS est raccordé à la sortie de l’appareil et son « tip » est connecté au « ring » du jack TRS.\nCes câbles sont aussi utilisés pour séparer une sortie dite « stéréo » en deux voies « mono »."
  },
  {
    "objectID": "outils_equipements/transport_signaux_analogiques.html#routage-des-signaux",
    "href": "outils_equipements/transport_signaux_analogiques.html#routage-des-signaux",
    "title": "8  Transport de signaux analogiques",
    "section": "8.4 Routage des signaux",
    "text": "8.4 Routage des signaux\nEn studio d’enregistrement, il n’est pas rare de rencontrer plusieurs cabines de prise de son, chacune équipée de boîtier de patch. Ces boîtiers sont constitués d’un certain nombre d’entrées XLR. On achemine ensuite les sorties de ces boîtiers via des multipaires jusqu’à une « patchbay ». Le rôle de la « patchbay » est de permettre de connecter n’importe quelle entrée (signal provenant d’un microphone) vers n’importe quel préamplificateur.\nOn trouve évidemment beaucoup d’usage à ces « patchbay ». Elles sont à considérer comme la matrice de routage d’un studio d’enregistrement."
  },
  {
    "objectID": "outils_equipements/les_preamplificateurs.html#informations-techniques-des-préamplificateurs",
    "href": "outils_equipements/les_preamplificateurs.html#informations-techniques-des-préamplificateurs",
    "title": "9  Les préamplificateurs",
    "section": "9.1 Informations techniques des préamplificateurs",
    "text": "9.1 Informations techniques des préamplificateurs\nLe gain (quantité d’amplification) : Les préamplificateurs pour microphones fournissent généralement un gain compris entre 20 et 60 dB, qui est nécessaire pour amplifier les signaux de faible niveau provenant d’un microphone pour atteindre un niveau utilisable. Le gain d’un préampli est généralement réglable, ce qui permet à l’utilisateur de définir le niveau optimal pour un microphone et une application donnés.\nL’impédance d’entrée d’un préamplificateur de microphone est un paramètre critique, car elle détermine la charge imposée par le préamplificateur au microphone. Une mauvaise adaptation d’impédance peut provoquer des interactions indésirables entre le microphone et le préampli, entraînant, entre autre, des modifications de la réponse en fréquence.\nLa réponse en fréquence d’un préamplificateur est une considération importante, car elle détermine la manière dont le préamplificateur va affecter le son du microphone. Une réponse en fréquence plate est généralement souhaitable.\nLe plancher de bruit d’un préamplificateur est une mesure du bruit résiduel ajouté au signal par le préamplificateur lui-même. Un plancher de bruit faible est nécessaire pour maintenir un bon rapport signal/bruit dans le système audio. Cette donnée est évidemment d’autant plus critique que le niveau du signal d’entré est faible. Par extension, on désigne la plage dynamique d’un préamplificateur de microphone comme la gamme comprise entre le niveau maximal du signal que le préamplificateur peut accepter et le plancher de bruit.\nLa distorsion est une mesure des modifications indésirables du signal audio qui sont introduites par le préampli. De faibles niveaux de distorsion sont souhaitables, car ils garantissent que le son du microphone est capturé et amplifié avec précision. On évoquera alors la distortion harmonique, qui, comme son nom l’indique, augmente le signal d’entré d’harmoniques supplémentaires. On rencontre aussi la distortion d’intermodulation. Admettons que nous envoyons en entrée d’un équipement audio deux sons purs de fréquences F1 et F2. La distortion d’intermodulation engendre deux nouvelles fréquences F1+F2 et F1-F2. On cherche donc a maintenir la contribution de ces deux résurgences le plus bas possible.\nCi-dessous, en guise d’exemple, on trouve le tableau des spécifications du préampli Rupert Neve Design 511 :\nMeasured at Main Output, un-weighted, 22 Hz - 22 kHz, source impedance 150 Ohm balanced. Noise performance can vary depending on the 500 series and / or interference from stray magnetic fields. NOISE Unity Gain: Better than -103 dBV Gain @ +66 dB: Better than -60 dBV Equivalent Input Noise: -125 dB\nFREQUENCY RESPONSE Main output, no load. +/- 0.1 dBu from 10 Hz to 31.5 kHz -2.6 dB @ 120 kHz\nMAXIMUM OUTPUT LEVEL +23 dBu\nTOTAL HARMONIC DISTORTION AND NOISE, NO SILK @ 1 kHz, +20 dBu output level, no load: Better than 0.0025% @ 20 Hz, +20 dBu output level, no load: 0.025% Typical (2nd and 3rd harmonic)\nTOTAL HARMONIC DISTORTION AND NOISE WITH SILK ENGAGED @ 100 Hz, +20 dBu input level, no load. TEXTURE @ min: 0.015%, mostly 3rd harmonic typical TEXTURE @ max: 2%, mostly 2rd harmonic typical\nGAIN Unity up to +66 dB in 6 dB steps. Trim continuously adjustable from -6 dB to +6 dB.\nPHANTOM POWER Supplied by the 500 series rack power supply. Switch selectable on faceplate.\nHIGH PASS FILTER Continuously variable swept frequency from 20 Hz to 250 Hz. Slope: 12 dB/Octave\nPOWER REQUIREMENTS @ +/-16VDC, 100mA"
  },
  {
    "objectID": "outils_equipements/les_preamplificateurs.html#critères-de-choix-dun-préamplificateur",
    "href": "outils_equipements/les_preamplificateurs.html#critères-de-choix-dun-préamplificateur",
    "title": "9  Les préamplificateurs",
    "section": "9.2 Critères de choix d’un préamplificateur",
    "text": "9.2 Critères de choix d’un préamplificateur\nLe critère de première importance dans le choix d’un préamplificateur est son gain maximal. Plus l’amplification disponible est grande, plus le préampli sera capable de répondre à des situations exigeantes, telles que l’enregistrement d’un évènement sonore à faible niveau, ou l’emploi d’un microphone à faible sensibilité.\nLe second critère important dans le choix d’un préampli est sa réponse en fréquence. Théoriquement, celle-ci doit être la plus neutre possible. Une certaine coloration peut être acceptée (voire souhaitée), mais celle-ci doit rester raisonnable pour répondre à des critères d’utilisations professionnelles.\nLa réponse en transitoire est un autre élément important. Certains préamplis auront tendance à adoucir la sensation d’attaque des sources. Cet effet n’est pas souhaitable.\nEnfin le rapport signal sur bruit doit être le plus grand possible. Nous cherchons toujours à rajouter le moins de bruit possible sur le chemin de notre signal."
  },
  {
    "objectID": "outils_equipements/les_preamplificateurs.html#les-technologies-de-préampli",
    "href": "outils_equipements/les_preamplificateurs.html#les-technologies-de-préampli",
    "title": "9  Les préamplificateurs",
    "section": "9.3 Les technologies de préampli",
    "text": "9.3 Les technologies de préampli\nNous avons vu dans le chapitre trois qu’il existe trois familles de composants électroniques permettant d’amplifier le signal : les tubes, les transistors et les circuits intégrés. On retrouve donc des topologies de circuit de préamplificateurs utilisant chacun de ces composants.\nChacune de ces topologies offre de très légère variation de son lorsque les préamplis sont poussés dans leur retranchement (seuil de saturation). Comme pour les microphones, il est délicat de parler de son « à tube » ou « à transistor ». De plus, l’influence sur le son d’un préamplificateur apparaît en pratique comme très marginale par rapport au positionnement du microphone."
  },
  {
    "objectID": "outils_equipements/les_preamplificateurs.html#les-réglages-dun-préampli",
    "href": "outils_equipements/les_preamplificateurs.html#les-réglages-dun-préampli",
    "title": "9  Les préamplificateurs",
    "section": "9.4 Les réglages d’un préampli",
    "text": "9.4 Les réglages d’un préampli\nUn préampli propose souvent les réglages suivants :\n\nUn potentiomètre de gain (qui est souvent remplacé par un sélecteur cranté, plus précis, pour les modèles haut de gamme).\nUn bouton activant l’alimentation fantôme. En effet, c’est bien le préampli qui génère cette tension d’alimentation pour les microphones statiques.\nUn bouton d’inversion de phase.\nUn coupe-bas."
  },
  {
    "objectID": "outils_equipements/les_convertisseurs.html#la-nécessité-de-la-conversion-analogique-numérique",
    "href": "outils_equipements/les_convertisseurs.html#la-nécessité-de-la-conversion-analogique-numérique",
    "title": "10  La conversion analogique numérique",
    "section": "10.1 La nécessité de la conversion analogique numérique",
    "text": "10.1 La nécessité de la conversion analogique numérique\nDurant toute la période de l’audio analogique, le support de stockage de prédilection fut la bande magnétique. Cependant, celle-ci n’offre pas un rapport signal sur bruit très satisfaisant, limitant alors la dynamique musicale enregistrable. De plus, la bande a également un coût non négligeable. On a donc cherché à remplacer ce support afin de résoudre ces deux problèmes. Le stockage numérique offre, sous certaines conditions, une dynamique bien supérieure à celle des supports analogiques.\nUne représentation numérique de l’audio permet aussi la réalisation de traitement délicat, voire impossible, en analogique. On pense, par exemple, aux algorithmes de réverbération, d’écho et de « pitch shifting » (modification de la hauteur d’un son).\nEnfin, nos principaux outils de manipulation du son sont aujourd’hui informatiques. Dès lors, une représentation numérique des signaux est toute indiquée pour les manipuler grâce à nos ordinateurs. Il en découle donc une nécessité de bien maîtriser les principes entourant la numérisation des signaux.\n\nEn français, le « digital » est un anglicisme. Le mot correct est donc bien « numérique », et non « digital », qui qualifie ce qui a rapport au doigt."
  },
  {
    "objectID": "outils_equipements/les_convertisseurs.html#théorie-de-léchantillonnage",
    "href": "outils_equipements/les_convertisseurs.html#théorie-de-léchantillonnage",
    "title": "10  La conversion analogique numérique",
    "section": "10.2 Théorie de l’échantillonnage",
    "text": "10.2 Théorie de l’échantillonnage\n\n10.2.1 D’un signal continu vers un signal échantillonné\nUne des caractéristiques principales d’un signal analogique est qu’il est continu. Une fonction, en mathématique, est dite continue si elle est définie en n’importe quel instant. Afin d’être numérisé, un signal doit donc être dénombré. En effet, la notion d’infini imposé par la continuité du signal n’a pas d’existence en numérique.\nLa numérisation du signal est comparable à l’utilisation d’un multimètre pour mesurer une tension. Un convertisseur va prélever la valeur du signal, de façon régulière, au cours du temps.\nAfin de correctement numériser un signal, il convient de définir deux paramètres :\n\nla vitesse de prélèvement, ou fréquence d’échantillonnage\nla plage de valeur permise pour le signal, ou résolution de quantification\n\n\n\n10.2.2 La fréquence d’échantillonnage\nCette fréquence définit le nombre de prélèvements par seconde. Par exemple, un morceau édité sur un CD audio a une fréquence d’échantillonnage de 44 100 Hz (44,1 kHz), cela signifie que le signal est mesuré 44 100 fois par seconde.\nLa fréquence de travail la plus courante est 48 kHz, mais l’on rencontre parfois des valeurs supérieures, multiple de celle-ci : 96 kHz, 192 kHz, etc. Cette augmentation proportionnelle de la fréquence d’échantillonnage s’appelle suréchantillonnage. Certains techniciens espèrent ainsi améliorer la qualité de l’enregistrement. Ce suréchantillonnage à un coût en ressource CPU et en espace de stockage. Un flux audio échantillonné à 96 kHz demande deux fois plus de ressource et d’espace qu’un flux échantillonné à 48 kHz. Cette valeur initiale de 44 100 Hz (ou 48 kHz) n’a pas été choisie au hasard. Pour la comprendre, il faut revenir au phénomène physique que nous cherchons à numériser.\nRappelons que le son est une onde mécanique, et nous l’entendons lorsqu’elle oscille dans une plage de fréquence comprise entre 20 Hz (très grave) et 20 000 Hz (très aigu). Il faut donc que notre système de numérisation soit capable de reproduire une fréquence maximale allant jusqu’à 20 000 Hz. Pour cela, nous utilisons les résultats des travaux des chercheurs Harry Nyquist et Claude Shannon (tous deux ayant travaillé aux laboratoires Bell).\nLe théorème de Shannon-Nyquist stipule que, pour être capable d’échantillonner un signal de fréquence \\(f\\), la fréquence d’échantillonnage doit au moins être de \\(2f\\). Ainsi, un ensemble de points généré par une fréquence inférieure à \\(f\\) ne peut correspondre qu’à cette seule et unique fréquence. Notre plage d’écoute étant limitée à 20 kHz, la fréquence d’échantillonnage minimale dont nous avons besoin est de 40 kHz.\nQue se passe-t-il si la fréquence du signal dépasse la moitié de la fréquence d’échantillonnage ? Dans ce cas, la vitesse de prélèvement n’est plus suffisante et nous observons l’apparition de nouvelles fréquences ne provenant pas du signal original. Ce phénomène se nomme repliement spectral.\n\n\n10.2.3 La résolution de quantification\nLa résolution de quantification permet de définir la plage de valeur dynamique permise dans le système numérique. Celle-ci s’exprime en bit. Par exemple, si nous prenons un convertisseur travaillant en 8 bit. Le nombre de valeurs que peut prendre un signal numérisé par un tel convertisseur est de \\(2^8-1 = 255\\) en base 10. Admettons que ce convertisseur accepte des signaux ayant une tension en entrée variant entre +15V et -15 V, celles-ci seront échelonnées sur 255 valeurs. Si maintenant, ce convertisseur travaille en 16 bit, il y aura 65 535 échelons. La précision de mesure de la dynamique du signal n’est donc pas du tout la même.\nEn pratique, augmenter la résolution de quantification permet principalement de définir le niveau de bruit du convertisseur. Plus la résolution est élevée, plus le bruit se retrouvera faible. En 8 bit, l’écart entre le niveau maximal d’un signal et le bruit est de 48 dB, en 16 bit cet écart est de 96 dB, en 24 bit, 144 dB. On peut approximativement calculer cette dynamique par la relation suivante :\n\\[ \\Delta_L \\approx 6 \\times N_{bits} \\]\nLa résolution de quantification standard en enregistrement est 24 bit. La plage dynamique est telle qu’elle rend le travail d’enregistrement beaucoup plus souple sur les niveaux d’acquisition des différentes sources."
  },
  {
    "objectID": "outils_equipements/les_convertisseurs.html#quelle-influence-sur-le-signal",
    "href": "outils_equipements/les_convertisseurs.html#quelle-influence-sur-le-signal",
    "title": "10  La conversion analogique numérique",
    "section": "10.3 Quelle influence sur le signal ?",
    "text": "10.3 Quelle influence sur le signal ?\nLe son numérique a longtemps eu la réputation d’être « dur », particulièrement dans le haut du spectre. Cela s’explique assez facilement par le fonctionnement des premiers convertisseurs.\nEn effet, toute la difficulté de fabrication d’un convertisseur réside dans la réalisation d’un filtre anti-repliement, pour prévenir le repliement spectral. Ce filtre doit enlever toutes les fréquences au-dessus de la moitié de la fréquence d’échantillonnage, sans pour autant affecter le spectre audible. Ce type de filtre est extrêmement délicat à réaliser en analogique. Cependant, ce problème est résolu grâce à une méthode d’échantillonnage appelée « sigma-delta » (voir ci-dessous).\nLe repliement spectral n’apparaît pas seulement lors de la conversion. Il peut également survenir lors de l’utilisation de certains traitements (saturation, simulation analogique, compresseurs). Lorsqu’il devient audible, le repliement spectral se caractérise par l’apparition de fréquences non harmoniques souvent qualifiées de « dures » et désagréables. Il est cependant bon de rappeler que ce phénomène, certes bien réel, apparaît dans des conditions de saturation du signal importante et sur des sources sonores riches en hautes fréquences.\nMalgré la dure vie que mène parfois la réputation du son numérique, il est important de rappeler qu’il a apporté un grand nombre d’avantages sur le son analogique, y compris sur des questions de rendus sonores. Par exemple, la dynamique est bien plus importante, la distorsion involontaire du signal infime et l’ajout de bruit inexistant."
  },
  {
    "objectID": "outils_equipements/les_convertisseurs.html#la-conversion-sigma-delta",
    "href": "outils_equipements/les_convertisseurs.html#la-conversion-sigma-delta",
    "title": "10  La conversion analogique numérique",
    "section": "10.4 La conversion sigma-delta",
    "text": "10.4 La conversion sigma-delta\nAujourd’hui, les convertisseurs ne travaillent pas directement à 44,1 kHz/16 bit ou 48 kHz/24 bit. Ils utilisent à la place un procédé appelé échantillonnage sigma-delta. Le principe est d’utiliser une fréquence d’échantillonnage très rapide (384 kHz) et de coder la dynamique du signal, en relatif, sur un seul bit (ce bit prend une valeur de 1 si le nouvel échantillon est plus fort que l’ancien, 0 pour le cas inverse). Les formats de travail que nous utilisons sont générés après cette première étape.\nL’intérêt de cette méthode est double :\n\nLe signal est suréchantillonné dès l’enregistrement\nLes filtres permettant d’éviter le repliement spectral sont donc très simples à réaliser"
  },
  {
    "objectID": "outils_equipements/transport_signaux_numeriques.html#les-liaisons-point-à-point",
    "href": "outils_equipements/transport_signaux_numeriques.html#les-liaisons-point-à-point",
    "title": "11  Transports de signaux numériques",
    "section": "11.1 Les liaisons point à point",
    "text": "11.1 Les liaisons point à point\nLes protocoles ci-dessous permettent de transmettre un ou plusieurs signaux numériques entre deux appareils.\n\n11.1.1 AES3 ; AES/EBU\nL’AES3 est un protocole défini par l’Audio Engineering Society et par l’European Union Broadcast. Il est principalement destiné aux appareils audio dits « professionnels ». Il permet de véhiculer deux canaux audio, à une fréquence d’échantillonnage maximal de 48 kHz, via une fiche XLR ou BNC (coaxial).\nLa S/PDIF est relativement proche de l’AES3, plutôt utilisé dans les équipements grand public, utilisant des câbles coaxiaux (sur fiches RCA) ou optiques (fiche toslink).\n\n\n11.1.2 ADAT\nL’ADAT lightpipe, souvent abrégé ADAT, est un autre protocole de transmission de signaux numériques. Il a été développé par Alesis pour fonctionner avec les magnétophones à bandes numériques de la même marque. ADAT signifie enfaîte « Alesis Digital Audio Tape ». On retrouve ce protocole sur un grand nombre d’appareils, notamment les interfaces audio, afin d’augmenter le nombre d’entrées/sorties accessibles.\nL’ADAT peut transporter jusqu’à huit canaux à 44.1/48 kHz, quatre canaux à 88.2/96 kHz et deux canaux à 176.4/192 kHz. Le débit d’information est donc constant, doubler la fréquence d’échantillonnage divise par deux le nombre de canaux.\nLa connectique la plus courante pour l’ADAT est la fibre optique avec fiches toslink.\n\n\n\nFigure 11.2: Câble Toslink\n\n\n\n\n11.1.3 MADI\nLe MADI, ou AES10, est un protocole permettant d’acheminer un grand nombre de canaux. On peut donc récupérer soixante-quatre canaux audio à une fréquence de 44.1/48 kHz. Comme pour l’ADAT, le nombre de canaux est divisé par deux à chaque doublement de la fréquence d’échantillonnage.\nCe protocole se retrouve fréquemment dans le monde de l’audio professionnel. Les connexions entre appareils supportant le MADI peuvent se faire soit avec des fibres optiques, soit sur câble coaxial (fiches BNC). Certains constructeurs, comme DIGICO, ont choisi les câbles RJ45 comme support d’acheminement.\n\n\n\n\n\n\nFigure 11.3: ?(caption)\n\n\n\n\n\n\n\nFigure 11.4: ?(caption)\n\n\n\n\n\n\nFibre optique et câble BNC"
  },
  {
    "objectID": "outils_equipements/transport_signaux_numeriques.html#les-réseaux-audio-numériques",
    "href": "outils_equipements/transport_signaux_numeriques.html#les-réseaux-audio-numériques",
    "title": "11  Transports de signaux numériques",
    "section": "11.2 Les réseaux audio numériques",
    "text": "11.2 Les réseaux audio numériques\nAujourd’hui, dans le monde du spectacle vivant, la plupart des salles de spectacle sont équipées avec des solutions de transmission des signaux audio sur réseau. Ces solutions se retrouvent aussi de plus en plus dans les studios d’enregistrement et de production audiovisuelle.\nIl existe plusieurs protocoles permettant le déploiement de tels dispositifs, mais leur logique fondamentale reste identique. Chaque appareil capable de se connecter au réseau audio peut recevoir et envoyer un flux audio a n’importe quels autres appareils appartenant au même réseau.\nLes réseaux audio sont régis par les mêmes règles que les réseaux informatiques. Chaque appareil pouvant être connecté à un réseau est identifiable par une adresse matérielle unique, appelée adresse MAC. Lorsqu’un appareil est connecté sur un réseau, il faut lui attribuer une adresse logique appelée adresse IP. Il y a ici deux façons de faire. Soit l’utilisateur attribue manuellement une adresse différente à chaque machine (solution préférée en audio, mais fastidieuse lorsque le réseau comprend un grand nombre d’appareils), soit le réseau possède un serveur DHCP qui se chargera d’attribuer une adresse IP unique à chacun des appareils connectés. Cet outil est généralement intégré dans un appareil nommé routeur, permettant d’interconnecter plusieurs appareils ainsi que de gérer le routage des flux d’information. Une fois les appareils interconnectés, chaque constructeur de solutions audio sur IP fournit un logiciel de routage de l’audio entre les appareils.\nLes principaux acteurs industriels des réseaux audionumériques sont Audinet avec DANTE, ALC NetworX (appartenant à Lawo) avec Ravenna, et les protocoles open source AES67 et AVB."
  },
  {
    "objectID": "outils_equipements/introduction_informatique_musicale.html#fonctionnement-dun-ordinateur",
    "href": "outils_equipements/introduction_informatique_musicale.html#fonctionnement-dun-ordinateur",
    "title": "12  Introduction à l’informatique musicale",
    "section": "12.1 Fonctionnement d’un ordinateur",
    "text": "12.1 Fonctionnement d’un ordinateur"
  },
  {
    "objectID": "outils_equipements/introduction_informatique_musicale.html#les-systèmes-dexploitation",
    "href": "outils_equipements/introduction_informatique_musicale.html#les-systèmes-dexploitation",
    "title": "12  Introduction à l’informatique musicale",
    "section": "12.2 Les systèmes d’exploitation",
    "text": "12.2 Les systèmes d’exploitation\n\n\n12.2.1 Linux\n\n\n\n\n12.2.2 Apple MacOS\n\n\n\n\n12.2.3 Microsoft Windows"
  },
  {
    "objectID": "outils_equipements/introduction_informatique_musicale.html#les-pilotes-audio",
    "href": "outils_equipements/introduction_informatique_musicale.html#les-pilotes-audio",
    "title": "12  Introduction à l’informatique musicale",
    "section": "12.3 Les pilotes audio",
    "text": "12.3 Les pilotes audio\n\n\n12.3.1 ASIO\n\n\n\n12.3.2 CoreAudio\n\n\n\n12.3.3 ALSA\n\n\n\n12.3.4 Jack Audio"
  },
  {
    "objectID": "outils_equipements/introduction_informatique_musicale.html#les-stations-de-travail-audio-numérique-daw",
    "href": "outils_equipements/introduction_informatique_musicale.html#les-stations-de-travail-audio-numérique-daw",
    "title": "12  Introduction à l’informatique musicale",
    "section": "12.4 Les Stations de Travail Audio-Numérique (DAW)",
    "text": "12.4 Les Stations de Travail Audio-Numérique (DAW)\n\n\n12.4.1 Le moteur audio\n\n\n12.4.2 Les fonctionnalités"
  },
  {
    "objectID": "outils_equipements/introduction_informatique_musicale.html#les-protocoles-de-transmission-dinformations",
    "href": "outils_equipements/introduction_informatique_musicale.html#les-protocoles-de-transmission-dinformations",
    "title": "12  Introduction à l’informatique musicale",
    "section": "12.5 Les protocoles de transmission d’informations",
    "text": "12.5 Les protocoles de transmission d’informations\n\n12.5.1 MIDI\n\n\n12.5.2 OSC"
  },
  {
    "objectID": "outils_equipements/les_enceintes.html#anatomie-dun-haut-parleur",
    "href": "outils_equipements/les_enceintes.html#anatomie-dun-haut-parleur",
    "title": "13  Enceintes et amplificateurs",
    "section": "13.1 Anatomie d’un haut-parleur",
    "text": "13.1 Anatomie d’un haut-parleur\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 13.1: Coupe de hautparleur (dans l’ordre, woofer, mid-range, tweeter). Infographie par Svjo, CC BY-SA 3.0\n\n\nSur les schémas ci-dessus, nous trouvons les éléments suivants :\n\nL’aimant\nLa bobine\nLa suspension\nLa membrane\n\nLa plupart des haut-parleurs sont qualifiés de dynamiques. La membrane du haut-parleur est reliée à une bobine, elle-même entourée par un système d’aimants. Lorsqu’un courant est appliqué aux bornes de cette bobine, sa position change dût à la modification du champ électromagnétique. Si le courant oscille, la bobine oscille de façon analogue, entraînant la membrane et permet donc la reproduction du son.\nOn appelle généralement « subwoofer » les haut-parleurs conçus pour retranscrire les fréquences très graves (20-200 Hz), « woofer » les haut-parleurs dédiés aux fréquences graves (50 Hz à 1000 kHz), « mid-range » les haut-parleurs du médium (1 kHz à 6 kHz), et « tweeter », ceux de l’aigu (au-delà de 5 kHz).\nLe haut-parleur est sans doute l’appareil audio le plus imparfait qui soit. Il est sujet à de nombreuses sources de distorsion du signal.\nNous ne savons pas fabriquer des haut-parleurs capables de reproduire uniformément toutes les fréquences. Ces derniers sont souvent spécialisés dans une certaine plage de fréquence. La plupart des enceintes de monitoring utilisent 3 voies : deux actives (utilisant des haut-parleurs) pour l’aigu et le médium, et une passive (évent avant ou arrière) pour le grave. L’utilisation du plusieurs voies imposent donc l’utilisation de filtres induisant un déphasage de certaines fréquences.\nÉgalement, un haut-parleur peut être approché par un modèle « masse-ressort ». Cela signifie qu’il y a une certaine inertie à sa mise en action et une certaine inertie à sa mise en arrêt. L’enceinte idéale devrait posséder une inertie nulle. Cette inertie est potentiellement responsable d’un adoucissement des transitoires et d’une sensation de flou."
  },
  {
    "objectID": "outils_equipements/les_enceintes.html#amplification-et-impédance",
    "href": "outils_equipements/les_enceintes.html#amplification-et-impédance",
    "title": "13  Enceintes et amplificateurs",
    "section": "13.2 Amplification et impédance",
    "text": "13.2 Amplification et impédance\nNous avons précédemment abordé le préamplificateur, qui permet d’amplifier la tension d’un signal audio analogique et d’en baisser son impédance. On appelle alors « amplificateur » un amplificateur de puissance. On rappel que la puissance d’un signal s’exprime par la relation ci-dessous. On cherche donc à augmenter la tension et l’intensité du signal.\n\\[ P = U \\times I \\]\nNous pouvons rapidement aborder la notion de classe d’amplification. En audio, nous n’utilisons que les classes A, AB et D. La classe A utilise un transistor (ou tube) pour amplifier l’ensemble du signal. Elle possède un très mauvais rendement. Cela signifie qu’il faut fournir beaucoup d’énergie au transistor pour un faible gain sur la puissance du signal. La classe AB utilise deux transistors, un pour les alternances négatives et un pour les alternances positives. Le rendement est meilleur que pour la classe A. Cependant, le point de raccordement entre les deux transistors est assez sensible et peu généré de la distorsion sur le signal (distorsion de croisement). La classe D, aussi appelée à tort « numérique », utilise un transistor afin d’indiquer l’état du signal. On retrouve donc la même idée que dans l’échantillonnage du signal. Ces amplificateurs offrent un excellent rendement.\nNous avons également abordé précédemment la notion d’adaptation d’impédance en tension, sans aborder l’adaptation d’impédance en puissance. Pour rappel, afin de préserver la tension entre deux appareils A et B, nous faisons en sorte d’avoir une faible impédance à la sortie de l’appareil A et une grande impédance à l’entré de l’appareil B. Pour préserver la puissance du signal, ce paradigme ne fonctionne plus. On cherche alors à avoir la même impédance entre la sortie d’un appareil et l’entrée d’un autre. En pratique, on raccordera, sur la sortie « 8 ohms » d’un amplificateur, un haut-parleur ayant une impédance de « 8 ohms ».\n\nOn remarque qu’ici, les impédances sont extrêmement faibles. Les impédances typiques des haut-parleurs (et donc des sorties d’amplificateurs en puissance) sont 4, 8 et 16 ohms.\n\nLa plupart des enceintes de monitoring ont aujourd’hui une amplification de classe D directement intégré."
  },
  {
    "objectID": "outils_equipements/les_enceintes.html#puissance-et-sensibilité",
    "href": "outils_equipements/les_enceintes.html#puissance-et-sensibilité",
    "title": "13  Enceintes et amplificateurs",
    "section": "13.3 Puissance et sensibilité",
    "text": "13.3 Puissance et sensibilité\nOn trouve généralement deux mesures de la puissance pour les enceintes. La puissance crête à crête (peak) et la puissance moyenne (RMS, ou Root Mean Square). La puissance se calcule grâce à la formule suivante :\n\\[ P =\\frac {U^2}{Z} \\]\n\\(P\\) est la puissance, \\(U\\) la tension et \\(Z\\) l’impédance. Pour réaliser la mesure de puissance d’un haut-parleur, on le soumet à un signal test, en général un bruit rose, pendant plusieurs heures. La puissance crête à crête se calcul grâce à la tension crête à crête (aussi dite maximale). Par exemple, pour une tension maximale de 100 V, sous une impédance de 8 ohms, on trouve une puissance crête à crête d’environ 1200 watts (abbrégé W). On considère généralement que la tension moyenne d’un signal est six décibels plus petite que sa tension maximale. En reprenant notre exemple, pour une tension maximale de 100 V, on a une tension moyenne de 50 V, sous une impédance de 8 ohms, on trouve une puissance moyenne d’environ 300 W.\nAlors, il faut donc faire très attention sur les spécifications données par les constructeurs, parfois volontairement floues. À défaut, si on lit que la puissance admissible d’un haut-parleur est de 1000 W, on supposera par défaut qu’il s’agit d’une puissance crête à crête. On ne dépassera donc pas une puissance moyenne d’amplification de 250 W.\nLa sensibilité est une mesure du niveau sonore à un mètre de l’enceinte pour une puissance RMS d’entrée d’un watt. Grâce à cette valeur, on peut calculer le niveau sonore produit par le haut-parleur à diverses distances et en fonction de différentes puissances d’entrées. On comprend aussi que la puissance électrique admissible dans l’enceinte ne donne que peu d’information sur son niveau sonore de sortie."
  },
  {
    "objectID": "outils_equipements/les_enceintes.html#conseils-pratiques",
    "href": "outils_equipements/les_enceintes.html#conseils-pratiques",
    "title": "13  Enceintes et amplificateurs",
    "section": "13.4 Conseils pratiques",
    "text": "13.4 Conseils pratiques\n\n13.4.1 Choisir une paire d’écoutes\nChoisir une paire d’enceintes peut sembler être un exercice difficile. Il existe énormément de modèles, coûtant de quelques dizaines d’euros à plusieurs dizaines de milliers.\nIl y a cependant plusieurs critères assez objectifs pour évaluer la qualité d’une enceinte :\n\nLa réponse en fréquence : l’enceinte flatte-t-elle particulièrement une zone du spectre ? En délaisse-t-elle une autre ?\nLa réponse en transitoires : les attaques sont-elles respectées ? Retrouve-t-on l’énergie initiale du signal ?\nLa linéarité en fonction du volume : a-t-on une sensation de compression du signal lorsque l’on augmente le niveau envoyé dans l’enceinte ?\nLe centre fantôme : le centre du système stéréophonique paraît-il stable ? Paraît-il précis ?\nLa couleur sonore de l’enceinte : a-t-on plaisir à écouter du son et de la musique sur ce système ?\n\n\n\n13.4.2 Placer correctement son écoute\nAfin de satisfaire les critères de la stéréophonie, il convient de respecter les règles suivantes :\n\nLes deux enceintes doivent être de même marque, de même modèle et appairée. Si une des membranes a dû être changée sur l’une d’elle, l’autre aurait dû recevoir la même opération.\nLes deux enceintes doivent être séparées par un angle de 60°\nL’auditeur doit être placé à équidistance des deux haut-parleurs, et regarder vers le milieu du segment formé par les deux enceintes.\n\nUne fois ces critères respectés, voici quelques conseils sur le placement des enceintes dans une pièce :\nOn préférera des pièces de grandes tailles, afin de repousser au maximum le temps d’arrivée des premières réflexions. Le système stéréophonique devrait être positionné dans un souci de symétrie : l’enceinte de gauche ne devrait pas être plus proche d’un mur que l’enceinte de droite, par exemple. Dans le cas de petit espace, on préféra coller les enceintes contre un mur. Cela permettra de supprimer l’influence d’une des premières réflexions au prix de l’augmentation du niveau de grave. Il est vivement recommandé de procédé au traitement, même minimal, acoustique de la pièce de travail, à commencer par les zones de réflexions premières et par les angles (ou le grave va s’accumuler). Si le traitement acoustique n’est pas envisageable, il convient de privilégier une écoute à faible niveau et une proximité maximale avec les enceintes."
  },
  {
    "objectID": "outils_equipements/les_enceintes.html#lécoute-au-casque",
    "href": "outils_equipements/les_enceintes.html#lécoute-au-casque",
    "title": "13  Enceintes et amplificateurs",
    "section": "13.5 L’écoute au casque",
    "text": "13.5 L’écoute au casque\nLe casque est un outil permettant d’écouter un signal tout en s’extrayant de son environnement (acoustique et/ou bruit). Cependant, de par son mode de fonctionnement, à savoir deux haut-parleurs placés dans une enceinte en contact direct avec les oreilles, il génère un certain nombre de déformations.\nPremièrement, la stéréophonie écoutée au casque est hypertrophiée. En effet, dans ces conditions d’écoutes, l’oreille gauche n’entend que le haut-parleur gauche et l’oreille droite n’entend que le haut-parleur droit.\nDeuxièmement, il est très difficile de trouver des casques avec une réponse en transitoire satisfaisante. Il convient donc d’être excessivement prudent lorsque l’on mix du contenu percussif sur un casque.\nTroisièmement, les casques sont encore moins linéaires en fréquence que les haut-parleurs, il convient là aussi d’être très prudent lors de la réalisation d’un mixage.\nCes défauts peuvent être compensés par l’habitude et la connaissance du système d’écoute, mais la transportabilité d’un mixage (à savoir, sa compatibilité avec d’autres systèmes d’écoute) réalisé au casque est souvent discutable.\n\n13.5.1 Casque fermé ou casque ouvert ?\nLe casque fermé, comme son nom l’indique, propose une fabrication enfermant le haut-parleur dans une enceinte close. Cette méthode de fabrication offre l’avantage d’isoler celui qui écoute de l’environnement, mais aussi d’isoler l’environnement de ce qui est diffusé dans le casque. Par contre, ces casques ont souvent une réponse en fréquence très accidentée, et ne sont pas recommandés pour le mixage. Il est par contre vivement recommandé pour les musiciens en session de prise de son.\nLe casque ouvert, à l’inverse de son homologue fermé, n’offre aucune isolation acoustique, au prix d’une meilleure réponse en fréquence du casque. Ces casques sont tout indiqués pour le mixage, mais beaucoup moins pour des situations de prise de son."
  },
  {
    "objectID": "methode-pds/mono_et_multimicrophonie.html",
    "href": "methode-pds/mono_et_multimicrophonie.html",
    "title": "15  Mono et multi-microphonie",
    "section": "",
    "text": "(En cours d’écriture)"
  },
  {
    "objectID": "methode-pds/le_couple_de_pds.html#généralités-sur-les-mécanismes-de-la-localisation-du-son-par-loreille-humaine",
    "href": "methode-pds/le_couple_de_pds.html#généralités-sur-les-mécanismes-de-la-localisation-du-son-par-loreille-humaine",
    "title": "16  La prise de son au couple",
    "section": "16.1 Généralités sur les mécanismes de la localisation du son par l’oreille humaine",
    "text": "16.1 Généralités sur les mécanismes de la localisation du son par l’oreille humaine\nAfin de mieux comprendre comment fonctionne un couple de prise de son, il convient d’étudier rapidement les principes fondamentaux de notre écoute.\nNotre capacité à localiser les sons dans l’espace repose principalement sur deux mécanismes : + La différence de temps + La différence de niveau\n\n16.1.1 La localisation par différence de temps\nNos oreilles sont espacées, d’environ 15 à 25 cm. Cette distance implique qu’un son émis plus proche de l’oreille droite arrivera également plus tôt qu’à l’oreille gauche. Cet écart de temps, de quelques millisecondes, est suffisant pour donner à notre cerveau un indice sur la localisation du son.\nAfin de sentir l’ordre de grandeur en jeu, calculons la différence de temps (\\(\\Delta t\\)) maximale pour un individu possédant un écart d’oreille de 20 cm.\nOn sait que la célérité du son dans l’air vaut \\(c = 340 m.s^{-1}\\), et est invariant en fonction de la fréquence. On sait également que \\(c = \\frac{d}{t}\\).\nDès lors, si on pose \\(d = 20 cm\\) soit \\(d = 0.2 m\\), on peut en déduire que :\n\\(t = \\frac{d}{c} \\iff t= \\frac{0.2}{340} \\approx 0.0006 s \\approx 0.6 ms\\)\nAfin de mettre en relief ce résultat, il est communément admis que l’oreille humaine commence à faire la différence entre deux répétitions d’un même son à partir de \\(20 ms\\).\n\n\n16.1.2 La localisation par différence d’intensité\nA priori, l’espace entre nos deux oreilles n’est pas creux. La densité de notre crâne et de son contenu va réfléchir et absorber une partie des fréquences rencontrées.\nÉgalement, la partie externe de nos oreilles, appelées pavillon, permet, grâce à sa forme, de donner une directivité à notre écoute.\nEn d’autres termes, notre tête et le pavillon de nos oreilles se comportent comme un filtre, variant en fonction de l’angle d’incidence de la source. Cette altération du timbre n’est pas perçue comme une coloration, mais bien comme une information de localisation. La modélisation mathématique de ces filtres se retrouve dans la littérature scientifique sous le nom HRTF.\nCette atténuation séquentiellement dépendante est décisive dans notre capacité à localiser les sons. On la retrouve communément sous le nom \\(\\Delta i\\).\n\n\n16.1.3 Prévalence fréquentielle de ces deux phénomènes\nIl est communément admis que le \\(\\Delta t\\) aura une efficacité maximale dans les basses fréquences, et le \\(\\Delta i\\) dans les hautes fréquences."
  },
  {
    "objectID": "methode-pds/le_couple_de_pds.html#principes-de-la-prise-de-son-au-couple",
    "href": "methode-pds/le_couple_de_pds.html#principes-de-la-prise-de-son-au-couple",
    "title": "16  La prise de son au couple",
    "section": "16.2 Principes de la prise de son au couple",
    "text": "16.2 Principes de la prise de son au couple\nPour créer son effet stéréophonique, les couples de prise de son utilisent les mêmes mécanismes que notre écoute naturelle :\n\nLa différence de temps\nLa différence d’intensité\n\nIl va de soi que, pour fonctionner de façon optimale, les microphones utilisés pour réaliser une prise de son stéréophonique doivent être de même marque, de même modèle et appairée.\n\nL’appairage garantit que les microphones aient des caractéristiques techniques suffisamment proches pour être considérés comme identiques.\n\nAfin de manipuler ces mécanismes, le preneur de son peut jouer sur les paramètres suivant :\n\nLa directivité des microphones\nL’angle entre les capsules\nLa distance entre les capsules\n\nModifier chacun de ses paramètres influe sur l’angle de prise de son. Plus l’ange de prise de son est faible, plus l’impression de stéréophonie sera grande. Plus l’angle de prise de son est grand, plus l’impression de stéréophonie sera faible, jusqu’à tendre vers la monophonie.\n\n\n\n\n\n\nMise en garde\n\n\n\nAttention de ne pas confondre l’angle de prise de son avec l’angle entre les capsules.\n\n\n\n16.2.1 Comment choisir un angle de prise de son.\nL’angle de prise de son est étroitement lié à la distance du couple par rapport à l’évènement sonore à enregistrer. En règle générale, plus le couple est loin des objets sonores à enregistrer, plus son angle de prise de son sera faible. À l’inverse, plus le couple sera proche, plus son angle de prise de son sera grand.\nEnsuite, lors de la réalisation d’un couple de prise de son, il est commun d’enregistrer un ensemble d’éléments : plusieurs instruments (batterie), voire plusieurs musiciens (quatuor à corde, orchestre). L’objectif est bien souvent de retrouver une sensation de disposition des éléments dans l’espace proche de la situation réelle. On cherche donc un angle de prise de son suffisamment petit pour que les sources occupent l’intégralité de l’espace stéréophonique, mais également suffisamment grand pour ne pas créer une sensation de trou au centre.\n\n\n16.2.2 Comment réaliser un angle de prise de son.\nPlusieurs outils existent pour aider le preneur de son à choisir son angle de prise de son correctement.\nIl est important de commencer par les abaques de Michael Williams, ayant cherché à étudier l’angle de prise de son et ses qualités en fonction des paramètres vues précédemment. Les résultats de ses travaux se trouvent sur le site mmad.info.\nOn trouve également beaucoup d’application mobile, comme celle du constructeur du microphone Neumann, s’appuyant sur les travaux de Michael Williams pour aider leurs utilisateurs à correctement positionner leurs microphones. Évidemment, et heureusement, rien n’est spécifique à un fabricant de microphones en particulier, l’application d’un constructeur A peut servir pour placer des microphones d’un constructeur B.\nPlus récemment, des chercheurs britanniques ont développé une application, nommée MARRS, permettant de positionner son couple de prise de son par rapport aux sources via une interface graphique très simple à utiliser. Cette application est disponible sur mobile et sur navigateur internet.\n\n\n16.2.3 Privilégier le \\(\\Delta i\\) ou le \\(\\Delta t\\) ?\nLa différence de perception du champ stéréophonique est très différente entre celui produit par le \\(\\Delta i\\) ou par le \\(\\Delta t\\).\n\nUn couple reposant sur le \\(\\Delta i\\) aura une sensation de localisation des sources précise. De plus si un tel couple enregistre une source ce déplaçant a vitesse constante, la sensation de déplacement retranscrite par le couple sera, elle aussi, linéaire. Il est également possible de sommer les deux microphones ensemble afin d’obtenir un signal monophonique. Un tel couple est appelé compatible mono.\nUn couple reposant sur le \\(\\Delta t\\) aura une sensation de localisation plus floue, mais apportera un sens de l’espace plus grand et une dimension spacieuse. À l’inverse d’un couple \\(\\Delta i\\), la sensation d’un déplacement linéaire d’une source n’est pas linéaire. Il n’est pas possible de sommer les deux capsules pour en obtenir une réduction mono sans générer des altérations de timbre sévères.\n\nChaque couple possède ses avantages et ses inconvénients. Heureusement, nous ne sommes pas limités à l’un où l’autre et nous pouvons à loisir réaliser une combinaison des deux mécanismes."
  },
  {
    "objectID": "methode-pds/le_couple_de_pds.html#les-topologies-classiques-de-prise-de-son-au-couple",
    "href": "methode-pds/le_couple_de_pds.html#les-topologies-classiques-de-prise-de-son-au-couple",
    "title": "16  La prise de son au couple",
    "section": "16.3 Les topologies classiques de prise de son au couple",
    "text": "16.3 Les topologies classiques de prise de son au couple\nLe premier ingénieur à se poser la question du son stéréophonique est l’anglais Alan Blumlein en 1929. Il imagine l’entièreté de la chaîne d’enregistrement et de diffusion nécessaire à la stéréophonie. Cependant, la BBC lui impose comme contrainte que toutes ses propositions soient compatibles avec des systèmes monophoniques. Il inventera donc le couple XY et MS.\nPlus tard, la plupart des radios européennes développeront des couples de prises de son mêlant \\(\\Delta i\\) et \\(\\Delta t\\), tel que l’ORTF.\n\n16.3.1 Le couple Blumlein / XY\nLes deux microphones sont ici directifs, placés au même point de l’espace et ongulé d’une certaine valeur entre eux.\nDe par les contraintes technologiques de son époque, Blumlein a décrit ce couple pour une utilisation de deux microphones bidirectionnels. Il est aujourd’hui plus commun de le rencontrer avec deux cardioïdes.\nDans sa version originale, le couple Blumlein comprend donc deux microphones bidirectionnels avec un angle de 90°.\nLa formulation du couple XY comprend deux microphones cardioïdes avec un angle compris entre 90° et 135°.\n\n\n16.3.2 Le couple MS\nLe couple MS, également inventé par Alan Blumlein, permet de doser la quantité de stéréophonie après l’enregistrement.\nPour se faire, ce couple utilise deux microphones :\n\nUn omnidirectionnel, historiquement, mais aujourd’hui fréquemment remplacé par un microphone cardioïde.\nUn bidirectionnel\n\nLe microphone omnidirectionnel, ou cardioïde, va rendre compte du centre de la stéréophonie, tandis que le microphone bidirectionnel rendra compte de la latéralité.\nUne fois enregistrés, ces deux canaux ont besoin d’être convertis, plus exactement dématricés, vers une paire de canaux stéréophonique. L’opération est très simple :\n\\[L = M+S\\] \\[R = M-S\\]\nCette opération peut être réalisée sur une console de mixage, telle que décrite ci-dessous.\n\n\n\nDématriçage MS\n\n\n\n\n16.3.3 Le couple ORTF\nLe couple ORTF, inventé par la radio française du même nom, combine l’effet du \\(\\Delta i\\) et du \\(\\Delta t\\) afin de s’approcher de l’écoute humaine.\nSa topologie est précisément définie. Elle propose l’utilisation d’une paire de microphones cardioïde, ongulé du 110° et avec un écart de 17 cm.\n\n\n16.3.4 Les couples AB\nLes couples AB peuvent avoir une définition ambiguë. Une partie de la littérature scientifique considère comme couple AB tout couple non coïncident. À cet égard l’ORTF est considéré comme un couple AB. Pour d’autre, les couples AB ne concernent que des couples constitués de microphones omnidirectionnels.\nCes derniers ont la particularité de n’utiliser que le \\(\\Delta t\\) afin de placer les sources dans l’espace. Le rendu est donc souvent spacieux, au prix d’une certaine instabilité et d’un certain manque de précision de l’image stéréophonique."
  },
  {
    "objectID": "methode-pds/le_couple_de_pds.html#compléter-une-prise-de-son-au-couple-par-des-appoints",
    "href": "methode-pds/le_couple_de_pds.html#compléter-une-prise-de-son-au-couple-par-des-appoints",
    "title": "16  La prise de son au couple",
    "section": "16.4 Compléter une prise de son au couple par des appoints",
    "text": "16.4 Compléter une prise de son au couple par des appoints\nIl est commun, lors d’une prise de son au couple, de chercher à obtenir une entière satisfaction sonore à la seule aide du couple. Cependant, cela n’est parfois pas possible, souvent pour des contraintes physiques et acoustiques (un instrument de l’ensemble jouant moins fort que les autres). Dans ces cas, l’utilisation d’appoint, donc de microphone supplémentaire, placé en proximité de la source, va permettre de venir récupérer une précision supplémentaire de l’instrument.\nLors de l’étape de mixage, le couple servira de base principale et l’on viendra ajouter la quantité nécessaire d’appoints pour préciser le propos. Il sera parfois nécessaire de remettre en phase l’appoint et le couple pour améliorer la sommation de l’ensemble."
  },
  {
    "objectID": "methode-pds/déphasage_et_remise_en_phase.html#les-effets-sonores-de-déphasage",
    "href": "methode-pds/déphasage_et_remise_en_phase.html#les-effets-sonores-de-déphasage",
    "title": "17  Déphasage et remise en phase",
    "section": "17.1 Les effets sonores de déphasage",
    "text": "17.1 Les effets sonores de déphasage\nTous les signaux sont caractérisés par une certaine phase. Celle-ci est moins tangible que celles de niveau sonore ou de fréquence. En effet, lorsqu’un signal est écouté seul, celle-ci ne s’entend pas. C’est au moment où plusieurs signaux corrélés (comprendre, enregistrés au même moment, par plusieurs microphones) sont sommés que les différences de phase peuvent s’entendre."
  },
  {
    "objectID": "methode-pds/déphasage_et_remise_en_phase.html#approche-mathématique",
    "href": "methode-pds/déphasage_et_remise_en_phase.html#approche-mathématique",
    "title": "17  Déphasage et remise en phase",
    "section": "17.2 Approche mathématique",
    "text": "17.2 Approche mathématique\nPrenons l’exemple d’un son pur : \\(sin (\\omega t + \\phi)\\) où \\(\\omega = 2\\pi f\\)\nLa phase de ce signal est décrite par \\(\\omega t +\\phi\\)\nLes deux paramètres responsables de déphasages audibles sont :\n\n\\(t\\), le temps\n\\(\\phi\\), la phase à l’origine\n\nAttention, pour un son pur, l’effet de la modification de \\(t\\) ou de \\(\\phi\\) semble très similaire. Ce n’est pas le cas pour des signaux pseudo-périodiques, atténués dans le temps."
  },
  {
    "objectID": "methode-pds/déphasage_et_remise_en_phase.html#les-sources-de-déphasage",
    "href": "methode-pds/déphasage_et_remise_en_phase.html#les-sources-de-déphasage",
    "title": "17  Déphasage et remise en phase",
    "section": "17.3 Les sources de déphasage",
    "text": "17.3 Les sources de déphasage\nLes causes les plus classiques de déphasages sont :\n\nUn câble XLR avec une inversion sur le point chaud et le point froid\nUne prise de son avec une différence de distance entre deux microphones\nUne prise de son utilisant deux microphones positionnés de part et d’autre d’une membrane\nUn retard de certaines fréquences lié aux objets rencontrés par les signaux"
  },
  {
    "objectID": "methode-pds/en_pratiques.html#le-confort-du-musicien",
    "href": "methode-pds/en_pratiques.html#le-confort-du-musicien",
    "title": "18  En pratiques",
    "section": "18.1 Le confort du musicien",
    "text": "18.1 Le confort du musicien\nMême si elle peut sembler triviale, cette « étape » de la chaîne de prise de son est de loin la plus importante. La qualité de l’interprétation donnée par le musicien dépendra grandement de son état moral et psychologique :\n\nEst-il stressé\nEst-il confiant\nSe sent-il accueilli\netc.\n\nNous pourrions considérer qu’un ou une musicienne arrivant dans un studio d’enregistrement se présente avec un taux de confiance maximal envers l’équipe technique. Dès lors l’objectif des différents techniciens est de conserver cette jauge au maximum.\nLes premières minutes sont particulièrement importantes et va poser un ressenti fort sur la journée de travail. Il y a donc un équilibre à trouver entre un accueil chaleureux et décontracté et un rapport productiviste et sérieux.\nLe système permettant aux musiciens de communiquer entre eux et avec les techniciens est primordial. En pratique, il n’est pas rare de dédier certains microphones du plateau à cette tâche. Du côté régi, le « talkback » est l’outil de communication premier des techniciens présents sur la session. Il convient de l’utiliser avec soin et prudence. Un musicien peut rapidement se sentir isolé, s’il enregistre seul. Il convient de maintenir un contact régulier et précis afin de ne pas l’abandonner dans le seul dans sa cabine. Qui plus est, un quiproquo peut être vite arrivé avec les systèmes de talkback. Prudence quant à l’état d’ouverture ou de fermeture du microphone."
  },
  {
    "objectID": "methode-pds/en_pratiques.html#le-choix-de-linstrument",
    "href": "methode-pds/en_pratiques.html#le-choix-de-linstrument",
    "title": "18  En pratiques",
    "section": "18.2 Le choix de l’instrument",
    "text": "18.2 Le choix de l’instrument\nLa plupart des musiciens se présenteront avec leurs instruments. La marge de manœuvre est donc ici quasi nulle.\nCependant il n’est pas rare que le studio possède du « backline », souvent composé de batteries, d’amplificateur guitare et basse, voire de guitares et de basses. Si l’instrument utilisé par le musicien pose problème pour la prise de son, proposer une alternative peut s’avérer être un bon pari. Il convient évidemment de sonder l’ouverture du musicien par rapport à cette proposition, afin de ne pas le braquer.\nIl peut également être intéressant de « préparer » les instruments. Cette technique est très courante sur les pianos et les batteries, afin de changer les propriétés acoustiques de l’instrument grâce a l’utilisation de draps, coussins, couvertures disposées dans ou sur l’instrument."
  },
  {
    "objectID": "methode-pds/en_pratiques.html#le-choix-de-lacoustique",
    "href": "methode-pds/en_pratiques.html#le-choix-de-lacoustique",
    "title": "18  En pratiques",
    "section": "18.3 Le choix de l’acoustique",
    "text": "18.3 Le choix de l’acoustique\nL’acoustique de la salle d’enregistrement est-elle aussi plus souvent une contrainte qu’une variable d’ajustement.\nOn préférera souvent de grandes salles afin de limiter l’apparition prématurée de premières réflexions. Plus la salle sera petite, plus celle-ci apportera une forte coloration sur le contenu enregistré. Il convient donc d’être attentif aux petites cabines de studio, celles-ci sont souvent très mates, mais leur apport sur le timbre des instruments qui y sont enregistrés est souvent très important.\nLorsque l’on a la possibilité d’enregistrer dans de grandes salles, il est souvent intéressant de disposer des quelques panneaux acoustiques mobiles, afin de modeler la pièce à sa convenance.\nSi l’acoustique imposée est défavorable, on préférera dans ce cas des prises d’hyper proximité, afin de minimiser son effet au maximum."
  },
  {
    "objectID": "methode-pds/en_pratiques.html#placer-et-choisir-son-microphone",
    "href": "methode-pds/en_pratiques.html#placer-et-choisir-son-microphone",
    "title": "18  En pratiques",
    "section": "18.4 Placer et choisir son microphone",
    "text": "18.4 Placer et choisir son microphone\nEn pratique, il est bien difficile de dissocier le choix du microphone de son placement, les deux étant très interdépendants. Cependant, il convient de garder à l’esprit que le positionnement du microphone est, parmi les deux, sans doute le plus déterminant.\nLa première étape, avant même de choisir un microphone, consiste à écouter l’instrument dans l’acoustique d’enregistrement. Il s’agit ici d’une écoute active. On se déplace autour de l’instrument, on s’en approche, on s’en éloigne, afin de sentir l’interaction entre la source et l’acoustique du lieu. Aussi, il est important de trouver deux zones d’émission particulière de l’instrument : la zone de projection maximale et la zone au timbre le plus favorable. La première peut nous servir à positionner l’instrumentiste par rapport aux autres instruments afin de minimiser les reprises entre microphones. La deuxième zone nous indique l’axe de prise de son.\nCette zone au timbre le plus favorable est relative. Elle dépend de l’instrument, bien sûr, mais aussi du modèle. Elle dépend également du mode de jeu, de l’articulation du joueur et évidemment, de l’esthétique de la musique.\n\n18.4.1 Le rapport a la distance du microphone\nLa distance de positionnement du microphone est un élément excessivement important sur le rendu esthétique de la prise de son.\nEn règle générale, plus on prend de distance, plus on approche une prise de son naturaliste, cherchant à reproduire un évènement sonore dans son environnement, tel qu’il aurait été entendu dans la pièce. Plus on se rapproche, plus on fragmente l’événement sonore et plus on l’arrache aussi a son contexte de diffusion.\nAfin de déterminer efficacement le placement d’un microphone, il convient d’abord d’en connaître sa distance critique. Celle-ci correspond au point, dans une pièce, où le son provenant directement d’une source est perçu au même niveau sonore que la réponse acoustique à cette source. Cela signifie que si nous plaçons notre microphone au-delà de ce point, nous obtiendrons plus d’acoustique que de son direct de l’instrument.\nIl est important aussi de considérer que la directivité du microphone influe sur la distance critique. En effet, plus la directivité du microphone est large (tends vers l’omnidirectionnalité), plus le microphone paraîtra éloigné de la source. À l’inverse, plus la directivité d’un microphone est étroite (tends vers la bidirectionnalité), plus le microphone paraîtra proche.\nDans le cas de l’utilisation de microphone directif, le placement en proximité et hyperproximité va créer une accentuation du contenu basse-fréquence de la source. Cela devient parfois un élément esthétique, comme sur les voix radiophoniques. Cela aussi peut être un défaut, une exagération qu’il conviendra de corriger en postproduction.\n\n\n18.4.2 Quand choisir une prise de son stéréophonique\nLa prise de son stéréophonique, comme son nom l’indique, regroupe l’ensemble des techniques de prise de son dédié au système de diffusion stéréophonique (deux enceintes séparées de 60° et orientées vers un auditeur placé à équidistance des deux transducteurs).\nL’avantage de tels dispositifs de prises de son est de peupler dès la prise l’espace stéréophonique qui est donné à l’auditeur lors de la diffusion. Ils permettent également de rendre compte de la position de plusieurs évènements sonores ayant lieu dans la même acoustique. Cette dernière est d’ailleurs bien mieux retranscrite par de tels systèmes de prise de son.\nIl s’agit à nouveau d’un choix esthétique. Faire le choix d’une prise de son monophonique permet de renforcer la sensation de frontalité et de densité d’une source. À l’inverse, une prise de son stéréophonique donnera une définition spatiale accrue.\n\n\n18.4.3 Quand choisir la multi-microphonie\nLa multi-microphonie consiste à enregistrer un instrument via l’utilisation de microphones (principalement) directifs, placés à différents endroits jugés pertinents et en hyperproximité.\nCette approche esthétique de la prise de son est devenue indissociable des « musiques actuelles ». Elle offre l’avantage d’une grande flexibilité de traitement lors de la phase de mixage. Voir, elle implique une certaine partie des traitements.\nEn effet, une prise d’hyperproximité va systématiquement relever deux défauts :\n\nun effet de proximité : le grave/bas médium de la source paraît hypertrophié lors de l’emploi de microphones directifs.\nLes dynamiques de jeux sont également hypertrophiées.\n\nL’effet de proximité implique donc bien souvent l’utilisation d’un égaliseur, permettant de corriger cette augmentation artificielle du grave. De même, l’hypertrophie de la dynamique de jeu implique l’usage d’un compresseur afin de corriger ces variations artificielles.\nAfin de recréer une sensation de spatialisation, on utilisera principalement deux outils. En premier lieu, le potentiomètre de panoramique afin de diriger ces sons mono dans l’espace stéréophonique, puis les réverbérations artificielles permettra de reconstituer un champ acoustique et de réintégrer ces sources dans une scène sonore.\nSi l’approche de la prise au couple pouvait être qualifiée de naturaliste, alors la prise de son en multimicrophone sera son pendant spectaculaire. Évidemment, il convient de ne pas aussi franchement opposer ces deux approches et il existe tout un monde de système de prise de son entre ces deux extrêmes."
  },
  {
    "objectID": "methode-pds/en_pratiques.html#le-choix-du-préamplificateur",
    "href": "methode-pds/en_pratiques.html#le-choix-du-préamplificateur",
    "title": "18  En pratiques",
    "section": "18.5 Le choix du préamplificateur",
    "text": "18.5 Le choix du préamplificateur\nLe rôle du préamplificateur est d’amplifier le signal, le tout en ramenant le minimum de bruit. Un premier élément de choix de préampli va se faire sur le niveau de pression acoustique produit par les sources à enregistrer.\nEnregistrer une batterie impose peut de contrainte sur le préampli quand a sa capacité à amplifier sans rajouter beaucoup de bruit sur le signal. À l’inverse, enregistrer des instruments peux sonores, possiblement avec des microphones peux sensibles, implique l’utilisation de préampli avec une excellente réserve de gain et un excellent rapport signal bruit.\n\n18.5.1 L’influence du préampli sur la « couleur » du son\nIl est assez connu que le préampli peut également devenir un choix esthétique pour influencer la couleur d’une prise de son. Cette question semble assez complexe. Voici quelques éléments de réponse :\n\nLe choix du préampli est d’une influence minime par rapport à tous les autres choix précédemment fait.\nLes préamplis sont souvent catégorisés, en termes de couleur, via les composants utilisés pour réaliser l’amplification. Attention, un composant électronique dépend toujours du contexte dans lequel il est placé (ici, du circuit électronique). Il est donc difficile de précisément qualifier le son d’un préampli à lampe ou à transistor de façon générique.\nLes impédances d’entrée des préamplis ne sont souvent pas évoquées dans ces discussions. Hors, pour la plus parts des microphones (hors statiques), leur impédance de sortie peut être suffisamment élevée pour engendrer une déperdition en aigu et en transitoire. Cette déperdition peut être heureuse, ou malheureuse, mais surtout bien réelle. Une manière de s’en prémunir peut-être d’utiliser des « booster » de microphones (parfois également appelés préamplis), permettant d’augmenter le niveau de sortie des microphones et aussi d’adapter leur impédance."
  },
  {
    "objectID": "spatialisation/spat-introduction.html",
    "href": "spatialisation/spat-introduction.html",
    "title": "32  Introduction",
    "section": "",
    "text": "Cette section, et les suivantes ont à cœur de traiter le vaste sujet de la spatialisation sonore. Il est assez rare de trouver des ouvrages génériques sur les techniques du son abordant aussi cette discipline. Mais cette lacune s’explique très facilement par la très faible part de marché représenté pour les consommateurs de format audio immersif, comme le fameux 5.1. Ces dernières années, les choses bougent un peu. Dolby est revenu d’une longue traversée du désert avec le « Dolby Atmos », d’abord dans les salles et les auditoriums de cinéma, et aujourd’hui dans le monde de la musique. On retrouve ainsi deux logiciels de mixage phares, Pro Tools d’Avid et Logic Pro X d’Apple, intégrant nativement ce « Dolby Atmos ». Cet intérêt pour les technologies immersives se retrouve aussi chez les constructeurs d’enceintes et de casques, proposant des systèmes de plus en plus simples pour accéder a cette expérience d’écoute. Alors, à la lumière de ces récentes évolutions, il paraît important de se pencher sur les longues et riches histoires de la spatialisation sonore, ainsi qu’aborder les différentes technologies et techniques associées.\nCette partie théorique commence par un historique de la spatialisation sonore, qui nous permettra de réaliser que cette conversation ne date pas d’hier, ni de l’Atmos. Puis, il conviendra de s’attarder sur les systèmes de diffusions et le positionnement des enceintes dans l’espace. On pourra ainsi aborder les différents systèmes normés, amenés par des constructeurs tels que DTS ou Dolby, mais aussi décrire et catégoriser les systèmes d’écoutes qui échappent à ces approches rigides. On différenciera alors les systèmes frontaux, les systèmes englobants à une dimension, les systèmes englobants à deux dimensions (souvent appelés « immersif ») et les systèmes à trois dimensions.\nUne fois ces deux blocs très généralistes abordés, nous attaquerons alors une à une les grandes techniques permettant de réaliser des mixages à destination de ces systèmes à multiples enceintes, avec dans l’ordre :\n\nLe binaural\nL’approche perceptive, ou « orientée canal »\nL’ambisonique\nLa WFS, ou synthèse de front d’onde\n\nNous discuterons aussi du mixage orienté objet, proposé notamment par le Dolby Atmos, mais aussi par d’autres, comme l’IRCAM et son « Spat », également intégré par FLUX:: Immersive dans son Spat Revolution.\n\n\n\n\n\n\nNote\n\n\n\nIl n’y aura ici que peu de rappels sur les mécanismes de la perception ainsi que sur notre capacité à localiser les sons dans l’espace. Ces informations sont accessibles dans le chapitre 2."
  },
  {
    "objectID": "spatialisation/spat-historique.html#de-la-monophonie-à-la-stéréophonie-un-besoin-despace",
    "href": "spatialisation/spat-historique.html#de-la-monophonie-à-la-stéréophonie-un-besoin-despace",
    "title": "33  Historique de la spatialisation",
    "section": "33.1 De la monophonie à la stéréophonie : un besoin d’espace",
    "text": "33.1 De la monophonie à la stéréophonie : un besoin d’espace\nPendant près de quatre-vingts ans, la majorité des systèmes de diffusion sonores sont monophoniques. Il faut en effet attendre la fin des années soixante pour que l’écrasante majorité de la musique enregistrée soit produite en stéréophonie. Avant cela, la norme est donc à une écoute ne se constituant que d’un seul haut-parleur.\nPourtant, le besoin d’espace dans la restitution sonore se fait sentir très tôt, car plusieurs ingénieurs se penchent sur cette question dès le début du XXe siècle. Clément Ader (1841-1925) semble être le premier à proposer un système de diffusion sur deux canaux. Cet ingénieur français est avant tout connu pour être le premier à avoir fait décoller un engin motorisé plus lourd que l’air en 1890. Il a également participé au déploiement du réseau téléphonique, à Paris, en 1879. C’est à ce moment qu’il a l’idée d’utiliser le réseau de télécommunication pour diffuser l’opéra dans les foyers. Ce dispositif, nommé Théâtrophone, est pour la première fois utilisé en 1881. Les auditeurs peuvent alors écouter la pièce retransmise en direct en plaçant un « écouteurs » sur chacune de leurs oreilles. La diffusion se fait donc sur deux canaux.\nÀ cette époque, les seuls microphones disponibles sont des microphones à charbons, dont on connaît la faible bande passante. La qualité sonore du dispositif est donc médiocre, mais l’engouement du public est réel, et perdurera jusque dans les années 1930.\n\n\n\nExtrait du journal « Le Gaulois » du 2 avril 1924\n\n\nLa stéréophonie va être pensée, presque simultanément par deux ingénieurs, Arthur C. Keller (1901-1983) et Alan D. Blumlein (1903-1942). Le premier travail aux laboratoires Bells, sous la direction d’Harvey Fletcher (le même Fletcher que les courbes Flecther-Munson 2.6). Keller se retrouve à travailler sur la diffusion stéréophonique, à la demande du chef d’orchestre Leopold Stokowski, alors chargé de produire des concerts sur les ondes de la NBC avec l’orchestre de Philadelphie. Stokowski est alors très insatisfait de la qualité sonore de ce type de dispositif. Il contacte donc les laboratoires Bell, leur demandant d’y apporter une solution. De cette collaboration naît une collection d’enregistrements dont le plus vieux semble être une interprétation du Carnaval romain (Ouverture) de Berlioz, le cinq décembre 1931.\nDe l’autre côté de l’océan, Alan Blumlein travaille pour la Colombia Graphophone Company. L’histoire veut qu’il soit frustré de son expérience du son au cinéma, ne comprenant pas pourquoi les voix sont toutes au centre alors que les acteurs se déplacent de part et d’autre de l’écran. Blumlein propose donc, pêle-mêle dans un seul et même brevet, les techniques de prise de son coïncidente XY/Blumlein et MS (pour garantir la rétrocompatibilité sur les systèmes mono), un système de gravure à deux canaux sur disques microsillons et la description du système d’enceinte que l’on utilise encore aujourd’hui sous le nom de stéréophonie. Il est amusant de constater que dans ses écrits, Blumlein ne parle pas de son stéréophonique, mais de son binaural. Ceci est d’autant plus surprenant qu’il est le premier à revendiquer l’idée que les haut-parleurs, dans un système stéréophonique, ne correspondent pas aux oreilles, et qu’il convient donc d’étudier ce système avec une approche psychoacoustique. Blumlein invente également un système, qu’il appelle « shuffling », permettant de convertir une différence de phase en différence d’intensité. Pour lui, ce dispositif fait partie intégrante d’un système de prise de son stéréophonique. Cependant, ce « shuffler » ne sera pas du tout conservé par le reste de l’industrie.\n\n\n\n\n\n\nNote\n\n\n\nEn 1931, La Colombia Graphophone Company et la Gramophone Company fusionnent pour devenir EMI.\n\n\nIl existe encore quelques enregistrements du travail d’Alan Blumlein, par exemple, cette démonstration du mouvement en stéréophonie (dans un style tout à fait anglais), ou encore cette interprétation de la Chevauchée des Valkyries pour trois pianos enregistrée à Abbey Road. Ces deux enregistrements dates de 1931.\n\n\n\n\n\n\nNote\n\n\n\nLes procédés de gravure stéréophonique sur disque microsillon de Keller et Blumlein sont très proches, cependant, il semblerait qu’aucun des deux n’ait eu connaissance des travaux de l’autre. Keller a développé son système avant Blumlein, mais ce dernier a publié son brevet en premier. Les deux approches reposent sur l’idée de graver les deux canaux à 45° de la verticale. Ce procédé semble être tombé dans l’oubli jusqu’à sa « réinvention » dans les années cinquante par la société Westrex.\n\n\nBlumlein décède en plein milieu de la seconde guerre mondiale suite au crash d’un bombardier Halifax, dans lequel il effectuait des tests sur le système de radar H2S.\nMalgré toutes ces avancées, l’adoption de la stéréophonie traîne, et ce pour deux raisons principales. La première est que les systèmes de gravures sur disques proposés par Blumlein ou Keller ne fonctionnent pas sur les gramophones, à cause du poids de la tête de lecture. Il faut donc attendre leurs remplacements par les disques microsillons au milieu des années cinquante. Le second frein est, comme on le rencontrera pour les systèmes dits « surround », le coût pour s’équiper d’un deuxième amplificateur et d’une deuxième enceinte.\n\n\n\nSur la diffusion stéréophonique — 1950\n\n\nLa situation se débloque dans les années soixante. La radio devient stéréophonique, en partie grâce au passage à la modulation FM. Le coût des équipements devient aussi plus raisonnable. À la fin des années soixante, on peut considérer que l’ensemble de la production musicale est passée à la stéréophonie. Il est d’ailleurs intéressant de noter que cette transition vers la stéréophonie n’a pas eu lieu au même rythme selon les genres musicaux. Dans les musiques dites « classiques », la monophonie limite à ce point la représentation de l’espace acoustique que le passage à la stéréophonie permet un réel gain de qualité. Dans les musiques dites « populaires » ou « amplifiées », la nécessité de la stéréophonie est moins évidente, elle d’ailleurs parfois perçu comme superficielle. Par exemple, il faudra attendre le dernier album des Beatles enregistré (Abbey Road - 1969) pour avoir une version stéréo officielle, approuvée par George Martin. Certains ont vu dans la stéréophonie une passade un peu exubérante, mais qui ne durerait pas. L’histoire leur aura prouvé le contraire. Il ne faut pas non plus leur jeter la pierre. En effet, les premiers enregistrements stéréophoniques n’exploitent souvent pas les deux canaux de mixages comme une opportunité de mieux représenter l’espace, mais plutôt pour démasquer les éléments entre eux. On trouve donc beaucoup de mixages avec les instruments placés dans l’enceinte gauche ou droite, et la voix en plein centre.\nDevant la généralisation de la stéréophonie, certains constructeurs, labels et artistes vont alors tenter la courte aventure de la quadriphonie."
  },
  {
    "objectID": "spatialisation/spat-historique.html#la-quadriphonie-une-entreprise-infructueuse",
    "href": "spatialisation/spat-historique.html#la-quadriphonie-une-entreprise-infructueuse",
    "title": "33  Historique de la spatialisation",
    "section": "33.2 La quadriphonie : une entreprise infructueuse",
    "text": "33.2 La quadriphonie : une entreprise infructueuse\nLa quadriphonie (quadraphonic, quadrasonic ou encore quadrophonic en anglais) est le premier format dit « surround » accessible au grand public. Les premières tentatives de productions musicales en quadriphonie commencent en 1969. Le directeur artistique Thomas Mowrey a alors réalisé un grand nombre de disques quadriphoniques, notamment pour le label Deutsche Grammophon.\nIl a existé trois formats de quadriphonie. Ils sont notés de la façon suivante : K-M-N. K indique le nombre de canaux de mixage, M le nombre de canaux du support de diffusion et N le nombre de canaux de restitution (ici, N est donc toujours égal à quatre).\n\nLe 4-4-4, que l’on pourrait qualifier de « vrai » quadriphonie. Cela impose un équipement tout particulier pour le support de diffusion. En effet, l’écrasante majorité des magnétophones à bandes et des disques microsillons sont stéréo. Certains constructeurs ont donc vendu des magnétophones « quadriphoniques », tandis que d’autres ont tenté de stocker ces quatre canaux sur disques microsillons grâce à des techniques de modulation d’amplitude. Cependant, ces procédés imposent tout de même d’équiper son lecteur vinyle d’une pointe en diamant spécifique.\nLe 4-2-4 cherche à simplifier les choses pour les auditeurs. Grâce à un jeu de matriçage, on encode le signal quadriphonique sur deux canaux, que l’on espère récupérer intact par dématriçage. Si ce matriçage fonctionne parfaitement pour des sons purs, il n’en est rien pour des sons complexes, qui peuvent alors induire une corrélation importante entre les différents canaux. Le matriçage le plus convaincant fut proposé par Peter Scheiber et perfectionné par Benjamin Bauer (SQ Quadriphonic), mais même celui-là posait un problème sur la corrélation des canaux de diffusions. En effet le niveau de séparation entre deux enceintes adjacentes n’était que de trois décibels. Ce jeu d’encodage et de décodage distord l’espace sonore et ne produit pas des résultats satisfaisants.\n\n\n\n\n\n\n\nNote\n\n\n\nIl « inspirera » pourtant les laboratoires de Dolby pour élaborer leur fort mal nommé « Dolby Stéréo »\n\n\n\nReste le 2-2-4, que l’on pourrait qualifier du « pire des deux mondes ». Le principe consiste à « inventer » des canaux arrière à un mixage stéréophonique, pour produire un effet englobant. Le résultat est souvent flou et insatisfaisant.\n\nLa plupart des enregistrements multicanaux produits par la Deutsche Grammophon ne sont jamais sortis qu’en stéréo. En effet, le label n’a pas été convaincu par la qualité du rendu sonore quadriphonique, notamment à cause du matriçage 4-2-4.\nLa complexité d’installation, et l’existence de nombreux formats (dont certains n’offrant pas une qualité de restitution satisfaisante) expliquent largement l’échec commercial du format quadriphonique.\nL’exploitation commerciale du son quadriphonique ne se limite pas à la musique dite « classique ». Un grand nombre d’artistes et de groupe « pop » verront leurs mixages stéréo déclinés en quadriphonie. Les plus connus sont sans doute les Pink Floyd. Dès 1967, ils embarquent en concert l’azimuth coordinator, leur permettant de recréer une diffusion quadriphonique. En 1972-1973, leur ingénieur du son de l’époque, Alan Parson propose un mix quadriphonique de leur album alors en production, The Dark Side of the Moon.\nMalgré tout, le format sera un échec commercial cuisant : trop cher pour équiper un grand nombre de personnes, souvent de qualité douteuse à cause de l’étape de matriçage, existant sous de nombreux formats, etc. On trouve tout même aujourd’hui un certain nombre de mixages quadriphoniques des années soixante-dix édités sur SACD ou DVD-Audio. Le label anglais Dutton Vocalion a dans son catalogue plusieurs dizaines de ces rééditions, principalement de musique dite « classique »."
  },
  {
    "objectID": "spatialisation/spat-historique.html#la-grande-aventure-du-son-spatialisé-au-cinéma",
    "href": "spatialisation/spat-historique.html#la-grande-aventure-du-son-spatialisé-au-cinéma",
    "title": "33  Historique de la spatialisation",
    "section": "33.3 La grande aventure du son spatialisé au cinéma",
    "text": "33.3 La grande aventure du son spatialisé au cinéma\nLe cinéma, naissant autour de 1984, est une des industries du divertissement mettant le plus en avant les intérêts d’une diffusion spatialisée. Sa dimension visuelle spectaculaire et immersive (écran géant, salle noire, etc.) a donc motivé un traitement du son similaire. On qualifie parfois le cinéma des premiers temps de « muet », principalement dû au fait de l’absence de son synchrone rattaché au film. Cependant, imaginer le dispositif cinématographique comme un dispositif silencieux est une erreur. Dès ses débuts, il est très souvent accompagné par un musicien et il est alors courant pour le public de parler, de commenter la projection. N’oublions pas qu’à l’origine le cinéma est un spectacle forain. Ce n’est que dans un second temps que le cinéma prend place dans les théâtres puis dans des lieux qui lui sont dédiés.\nLe Vitaphone est, en 1924, le premier système de synchronisation sonore, développé par Westerne Electric Company en collaboration avec les laboratoires Bells. Le premier film partiellement parlant est Le Chanter de Jazz (1927) et le premier film intégralement parlant est Lights of New York (1928).\nLe cinéma sonore s’appuie alors sur une diffusion monophonique, le système de diffusion est d’ailleurs caché derrière l’écran.\n\n33.3.1 Fantasia et le Fantasound\nFantasia est un film de Walter Disney sorti aux États-Unis en 1940. L’ambition de son auteur est de mettre en image des œuvres incontournables de la musique dite « classique » afin de toucher un nouveau public. Pour cela, il s’associe avec le chef d’orchestre Leopold Stokowski (le même personnage que nous avons évoqué plus haut).\nL’ambition technique du film est énorme et, sous les exigences de Walt Disney, pousse l’équipe technique à développer le Fantasound. Ce Fantasound a deux objectifs principaux :\n\nPremièrement, améliorer la dynamique de diffusion sonore (alors limitée par le support)\nDeuxièmement, permettre de déplacer des sons dans l’espace.\n\nEst alors développé le premier système de VCA (Voltage Controlled Amplifier), piloté par la fréquence d’un signal sinusoïdal inscrit sur le film. Le deuxième point est solutionné par l’invention du premier « pan pot ». Le Fantasound utilise cinq canaux de diffusions, trois enceintes frontales (gauche, centre et droit) ainsi que deux enceintes arrières (gauche et droite). Fantasia est ainsi le premier film « surround » de l’histoire.\n\n\n\n\n\n\nNote\n\n\n\nLe Fantasound est l’ancêtre de ce que l’on appelle aujourd’hui le 5.1 (sans le caisson de grave).\n\n\nCependant, le coût global du système Fantasound le rend impraticable, pour le film Fantasia lui-même, mais également pour d’autres productions. Il faut, en effet, un petit régiment d’opérateurs pour pouvoir correctement diffuser le film.\n\n\n33.3.2 Le Dolby Stereo et le son optique matricé\nDurant toute la période du son analogique, le plus grand ennemi de la chaîne de production est le bruit. Ce bruit est particulièrement inhérent aux supports magnétiques et optiques alors utilisés. C’est dans ce contexte que Ray Dolby fonde son entreprise Dolby Laboratories en 1965, et commercialise le Dolby NR en 1966.\nDans les années 70, les laboratoires Dolby élaborent également un système de diffusion cinématographique multicanal. Ce dispositif d’enceinte dit LCRS (gauches, centre, droit, et une enceinte arrière dite « surround »), est associé à un matriçage 4-2-4 grandement « inspiré » par les travaux de Peter Scheiber sur le matriçage quadriphonique. Ils auront simplement la mauvaise idée d’appeler ce système « Dolby Stereo ».\n\n\n\n\n\n\nImportant\n\n\n\nLe mixage est réalisé sur quatre canaux : gauche, centre, droit et arrière. Ces quatre canaux sont alors matricés sur deux canaux que l’on nomme gauche totale et droite totale.\n\n\n\n\nL\nR\nC\nS\n\n\n\n\nLt\n1\n0\n\\(\\sqrt{2}\\over{2}\\)\n\\(j\\times{\\sqrt{2}}\\over{2}\\)\n\n\nRt\n0\n1\n\\(\\sqrt{2}\\over{2}\\)\n\\(— j\\times{\\sqrt{2}}\\over{2}\\)\n\n\n\nPour décoder le signal et retrouver quatre canaux de diffusion, on doit alors dématricer les signaux Lt et Rt.\n\n\n\n\nLt\nRt\n\n\n\n\nL\n1\n0\n\n\nR\n0\n1\n\n\nC\n1\n1\n\n\nS\n1\n-1\n\n\n\nLe canal central est alors formé par les signaux en phase du matriçage bicanal. Le canal arrière est composé des signaux hors phase dans le matriçage bicanal.\n\n\nLes performances de ce système quant à la restitution de l’espace sont assez médiocres, ou en tout cas, déséquilibrées vers la scène frontale. Cela s’explique évidemment par le dispositif cinématographique et par la présence de l’écran géant captant toute l’attention du spectateur. Il reste tout de même difficile de le considérer comme un système englobant, car l’écart entre les enceintes latérales et l’enceinte arrière est tellement important que l’effet de source fantôme ne peut pas être opérant.\nAussi, il est rare que ce canal « surround », ou arrière, ne soit constitué que d’une seule enceinte à la diffusion. Il est courant de rencontrer un dispositif en « U », situé à l’arrière du spectateur. Cela n’arrange malheureusement pas nos histoires de précisions de restitution du champ spatiale.\n\n\n\n\n\n\nImportant\n\n\n\nEn ce qui concerne la gestion du canal LFE (Low Frequency Effect), il n’y a pas de canal dédié. C’est donc un simple système de « bass management », où une partie du grave est filtré des « têtes » (enceintes principales) et redirigé vers le subwoofer.\n\n\nLe tour de force de Dolby est en réalité de contourner la limitation du support optique sans pour autant remettre en cause la chaîne de production cinématographique. Même après l’avènement du son numérique, une version Dolby Stereo était toujours présente sur la pellicule, pour les salles non équipées en système de diffusion numérique, ou en cas de panne de ce dernier.\nUn des métrages emblématiques de cette technologie Dolby Stereo est La Guerre des Etoiles de Georges Lucas.\n\n\n33.3.3 Le passage au son numérique\nLe matriçage du signal à de grandes conséquences sur l’intelligibilité de l’espace et pause des problèmes importants de flou de localisation. Lorsque le son numérique se démocratise, la nécessité du matriçage disparaît. On peut ainsi stocker plusieurs canaux audio numérisés sur pellicule. D’ailleurs, il n’est plus nécessaire d’utiliser des réducteurs de bruit, la dynamique du signal numérisé est bien plus grande que celle offerte par les supports analogiques.\n\n\n\n\n\n\nNote\n\n\n\nLe passage au son numérique fut le début d’une longue traversée du désert pour l’entreprise Dolby.\n\n\nL’arrivée du son numérique impose aussi une rude concurrence chez les acteurs de cette mutation. On trouvera ainsi la société nipponne Sony, les Californiens de DTS et encore et toujours Dolby. On arrive d’ailleurs très bien à relire l’histoire de cette course aux nouveaux formats audio numériques en regardant une pellicule de cette époque.\n\n\n\nBord d’une copie 35 mm. À droite le Dolby Stereo (analogique), à gauche, le Sony SDDS, au centre, le Dolby Digital, et enfin, tout à droite, la bande de timecode du DTS\n\n\nDolby occupe déjà toute une partie de la pellicule avec le stockage optique analogique du Dolby Stereo. Sony arrive alors le premier avec son système de son numérique sur support optique avec le SDDS (Sony Dynamic Digital Sound). Lorsque Dolby termine l’élaboration de son encodage AC-3 pour le Dolby Digital, il ne reste alors que l’espace entre les perforations. Finalement, DTS est obligé de trouver une solution de contournement et n’inscrit qu’une piste de timecode sur le film. Cette piste permet alors de synchroniser un lecteur de CD-ROM contenant la piste audio du film.\nLe Dolby digital et le format DTS sont tous deux pensés pour une diffusion sur un système 5.1. Le SDDS propose une couverture de l’espace frontale plus importante, avec 5 enceintes accompagnées de deux enceintes arrière pour le « surrounds ». En pratique, ces canaux de diffusions supplémentaires ne sont pas exploités lors du mixage, et les mixeurs se content de fournir un mix 5.1 compatible avec l’ensemble des formats. Le premier film mixé en DTS est Jurassic Park (1993), de Steven Spielberg, tandis que le premier film utilisant le système Sony est Last Action Hereos (1993) par John McTiernan.\nLe 5.1 n’a pas franchement eu de succès dans son exploitation grand public. Les raisons sont toujours un peu les mêmes : le coût, la place, l’esthétique (certains et certaines n’aiment pas peupler leur séjour d’enceintes). Ce format s’est donc limité à un public de niche, souvent associé à l’audiophilie.\nIl y a eu ensuite une multitude de formats dérivés et d’évolutions, comme le Dolby Digital Surround EX, rajoutant une enceinte centrale à l’arrière. Les formats domestiques furent également nombreux, avec les Dolby Surround, Dolby Logic Pro, Dolby Logic Pro II, etc.\nL’évolution « majeure » suivante est le passage au 7.1, où l’on considère alors deux enceintes plein gauche et droite et deux enceintes arrière pour compléter le LCR classique. Le premier film mixé en 7.1 est Toy Story 3 (2010), des studios Pixar.\n\n\n33.3.4 Passage au mixage orienté objet et le retour en force de Dolby\nEn 2012, Dolby annonce un tout nouveau format de mixage, le Dolby Atmos. C’est une technologie hybride entre mixage orienté canal et mixage orienté objet. Cette approche de mixage orienté objet considère une source sonore (un canal audio mono par exemple) comme un objet, auquel on associe des informations de mixage (volume et position dans l’espace), qui sont ensuite interprétées par un décodeur. Ce dispositif permettait initialement de travailler sur un système d’enceintes dit 7.1.4, soit une 7.1 augmentée de quatre enceintes en élévation. Il s’agit donc d’un des premiers formats cinéma intégrant la composante d’élévation.\nL’approche orientée objet permet de solutionner la problématique des « mixdown » des films. En effet, un métrage précédemment mixé en 7.1 doit ensuite être downmixé en 5.1 pour les cinémas moins bien équipés, puis aussi en stéréo pour une exploitation télévisuelle. Ici, il suffit d’indiquer au décodeur le système de haut-parleurs auquel il est connecté et les métadonnées de mixage seront interprétées pour retranscrire au mieux le mixage. On peut même réaliser un mixage dédié à une écoute au casque, en utilisant la synthèse binaurale.\nCe format Dolby Atmos est un succès majeur, replaçant Dolby dans une situation de monopole. Très rapidement, le logiciel Pro Tools d’AVID intègre le premier panner Dolby Atmos, et les salles de cinéma s’équipent du décodeur et du système d’enceinte associé.\nCe succès s’infusera aussi dans le monde de la musique. En 2021, Apple annonce le support natif du Dolby Atmos (avec pour la première fois, le moteur de rendu Dolby Atmos intégré) dans son logiciel de musique assistée par ordinateur Logic Pro. De plus, son service de streaming, Apple Music, intègre également le support de la lecture de mixages réalisés avec le Dolby Atmos."
  },
  {
    "objectID": "spatialisation/spat-historique.html#le-binaural-la-spatialisation-sonore-pour-tous",
    "href": "spatialisation/spat-historique.html#le-binaural-la-spatialisation-sonore-pour-tous",
    "title": "33  Historique de la spatialisation",
    "section": "33.4 Le binaural : la spatialisation sonore pour tous",
    "text": "33.4 Le binaural : la spatialisation sonore pour tous\nIl est bon que nous donnions ici une définition précise du binaural. De nos jours, le binaural rassemble l’ensemble des techniques et des moyens permettant de reproduire l’effet du corps humain (tête, oreilles, torse) sur un signal audio. Un signal binaural comporte donc deux canaux, filtrés par la réponse en fréquence du corps humain. Cependant, dans l’histoire du son, le terme binaural a souvent été utilisé comme synonyme du son stéréophonique.\n\n\n\n\n\n\nNote\n\n\n\nPar ailleurs, le terme « stéréophonie » se substitue souvent pour désigner tous flux audio comprenant deux canaux. Ce n’est rigoureusement pas juste, car la stéréophonie sous-entendant la diffusion sur un système de haut-parleurs spécifique.\n\n\nL’histoire du son binaural s’anime également dans les années 1930, dans les laboratoires Bell. H. Flecther dirige alors une équipe travaillant sur la mise au point d’un mannequin de cire équipé de microphones sur les joues, nommé Oscar.\n\n\n\nOscar : premier dispositif d’enregistrement binaural\n\n\nCe premier dispositif permet de réaliser un certain nombre de tests sur notre perception. Il est alors rapidement montré que l’absence d’image lors de l’écoute d’un signal binaural entraîne l’augmentation d’erreurs de localisation (confusion entre l’avant et l’arrière).\nEn Europe, deux chercheurs de la société néerlandaise Philips, De Boer et Vermeulen, développe le premier mannequin avec une simulation de l’effet du pavillon, grâce à l’intégration des microphones directement dans les oreilles.\nDans les années 40, De Boer dépose un brevet pour un système de prise de son utilisant une simple sphère d’au moins quatorze centimètres de diamètre en lieu et place d’une tête de mannequin. Les microphones sont alors placés de part et d’autre de la sphère.\nToujours aux Pays-Bas, il est alors fait l’expérience de la diffusion de contenus binauraux sur les ondes radio, utilisant sans doute le mannequin de De Boer et Vermeulen.\nDurant les années cinquante, plusieurs compagnies développent leurs propres systèmes de prise de son binaural, notamment Schoeps et AKG.\n\n\n\n\n\n\nPremière tête factice d’AKG\n\n\n\n\n\n\n\nSphère de prise de son Schoeps\n\n\n\n\n\nLes années soixante et soixante-dix sont marquées par de nombreuses études sur l’écoute binaurale et l’effet de notre corps sur les signaux. Le système KEMAR devient le premier mannequin de référence. Les mannequins précédents étant souvent récupérés de grands magasins, leurs propriétés acoustiques ne correspondent pas à celle du corps humain. Ce paramètre est alors corrigé par le modèle KEMAR et est ainsi le premier utilisé pour des mesures de prothèses auditives.\nEn 1973, la société Neumann dévoile sa tête artificielle KU-80, équipée de microphones omnidirectionnels KM83. L’année suivante, AKG propose elle aussi son système de prise de son binaural D99c.\n\n\n\n\n\n\nKEMAR\n\n\n\n\n\n\n\nNeumann KU80\n\n\n\n\n\n\n\nAKG d99c\n\n\n\n\n\nEn 1997, un groupe de chercheur publie une étude comparative des différentes têtes artificielles de l’époque. Il en ressort un problème persistant de trouble de la localisation, particulièrement sur la discrimination de l’avant et de l’arrière. Est alors mise en cause la forme des pavillons des différents mannequins. En partant des résultats de cette étude, plusieurs initiatives similaires apparaissent, consistant à mesurer la réponse en fréquence de la tête d’un grand nombre d’individus, afin de trouver la courbe de réponse correspondant au plus grand nombre. Ce type d’approche donnera, entre autres, naissance à la tête KU-100 de Neumann.\n\n\n\n\n\n\nNote\n\n\n\nLe résultat de ce type de mesures se nomme HRTF, pour Head-Related Transfer Function, ou, fonction de transfert de la tête.\n\n\nGrâce au passage à l’audionumérique, ainsi qu’à l’augmentation significative de la puissance de calcul des ordinateurs, on voit apparaître dans les années quatre-vingt-dix la technique de la synthèse binaurale (voir section — 35.2). Il devient alors possible à partir de simples prises de son monophonique de recréer l’effet de la tête sur le signal grâce à une batterie de filtres (toujours ces fameuses HRTF).\n\n\n\n\n\n\nAstuce\n\n\n\nEn France, Lucie Hardoin est une spécialiste de prise de son et de la postproduction binaurale. Elle met à disposition sur son site web un grand nombre d’extraits pour en découvrir les qualités sonores."
  },
  {
    "objectID": "spatialisation/spat-historique.html#les-pionniers-de-la-musique-spatialisée",
    "href": "spatialisation/spat-historique.html#les-pionniers-de-la-musique-spatialisée",
    "title": "33  Historique de la spatialisation",
    "section": "33.5 Les pionniers de la musique spatialisée",
    "text": "33.5 Les pionniers de la musique spatialisée\nS’il est important de couvrir l’histoire technologique, il l’est tout autant de raconter celle de ceux qui ont pensé la spatialisation sonore dans leurs créations. Les mouvements de la musique concrète et de la musique électroacoustique ont été particulièrement féconds sur ce sujet.\nPierre Schaeffer (1910-1995), ingénieur français diplômé de l’école polytechnique, propose les fondations esthétiques et théoriques de la musique concrète dans les années quarante. Au cœur de ces idées est celle de l’acousmatique. Une musique acousmatique est une musique qui est complètement arrachée à son contexte visuel et n’est plus que son. Elle est grandement permise par les avancées technologiques qui lui sont contemporaines en matière d’enregistrement sonore. L’objectif de l’écoute acousmatique est de permettre à l’auditeur une plus grande imagination à la réponse du stimulus sonore seul. Ce concept est aussi étroitement lié à l’art radiophonique et ses moyens de diffusion. Suite à sa rencontre avec Pierre Henri (1927-2017), les deux hommes créeront « Symphonie pour un homme seul » en 1950. Henri collaborera aussi avec Edgar Varèse en 1954 pour la création de Déserts au Théâtre des Champs-Élysées, en tant que spatialisateur. Le concert est aussi diffusé à la radio, et pour la première fois en stéréophonie.\nEn 1951, Schaeffer créer le « Groupe de Recherche en Musique Concrète », avec Pierre Henri à sa tête, qui devient dès 1958 le « Groupe de Recherche Musique » (GRM). On y retrouve alors, entre autres, Iannis Xenakis, Michel Chion et François Bayle (Pierre Henri quitte le projet en 1958, avant la création du GRM).\n\n\n\n\n\n\nNote\n\n\n\nPierre Schaeffer fonde également le service de recherche de la RTF en 1964, dédié à la recherche autour de radio et de la télévision.\n\n\nCette branche de la musique a à cœur d’étudier les effets de la captation sonore et de sa diffusion. Si l’on y réfléchit beaucoup sur les façons de transformer le son par la manipulation de son support de stockage, on se pose également la question du moyen de diffusion. En 1974, François Bayle invente l’orchestre de haut-parleurs qu’il nomme Acousmonium. Ce système de diffusion est constitué de multiples haut-parleurs, de taille et qualités sonores différentes, placés sur scène, comme des musiciens. Ces haut-parleurs sont alors « orchestré » par le compositeur qui opère depuis une table de mixage, parfois aussi appelée table de spatialisation. Ce dispositif existe toujours et est régulièrement utilisé en concert.\n\n\n\nUne des premières formes de l’Acousmonium\n\n\n\n\n\n\n\n\nAstuce\n\n\n\nÀ découvrir, une archive de 1983 sur l’Acousmonium et la musique qu’on lui fait jouer.\n\n\nÉgalement, est créé en 1970 le Groupe de musique expérimentale de Bourges (GMEB) par Françoise Barrière et Christian Clozier. Il est à l’origine de l’invention d’une table de spatialisation nommée Gmebaphone. Si l’Acousmonium propose un « routage » dynamique des signaux vers les différents haut-parleurs, le Gmebaphone adopte une approche de filtrage, découpant le signal entrant en bandes pour les répartir sur différents haut-parleurs spécialisés.\n\n\n\nLe Gmebaphone\n\n\nLe GMEB devient l’IMEB en 1997 (Institut de Musique Expérimentale de Bourges) avant d’être fermé définitivement en juillet 2011.\nÀ partir des années 50, un grand nombre de compositeurs (principalement de musique électroacoustique et électronique) se poseront cette question de l’espace et la spatialisation. On peut alors citer : Iannis Xenakis, Olivier Messaien, Karlheinz Stockhausen et un certain Pierre Boulez qui fondra l’IRCAM en 1970.\n\n\n\n\n\n\nAstuce\n\n\n\nÀ noter l’excellent site Electrodoc, centre de documentation en musiques électroacoustiques. On y trouve de multiples documentations sur les personnes et les œuvres rattachées à la musique électroacoustique."
  },
  {
    "objectID": "spatialisation/spat-historique.html#lambisonie-décorréler-lespace-de-production-et-lespace-découte",
    "href": "spatialisation/spat-historique.html#lambisonie-décorréler-lespace-de-production-et-lespace-découte",
    "title": "33  Historique de la spatialisation",
    "section": "33.6 L’ambisonie : décorréler l’espace de production et l’espace d’écoute",
    "text": "33.6 L’ambisonie : décorréler l’espace de production et l’espace d’écoute\nDepuis le milieu des années soixante, une équipe de chercheurs anglais de la National Research Development Corporation mènent de nombreuses recherches sur la spatialisation sonore et sur le son « surround ». Nous sommes donc en plein cœur du (bref) boom de la quadriphonie. Parmi eux, on retrouve particulièrement le mathématicien Micheal Gerzon (1945-1996)\nEnsemble, ils élaborent une proposition alternative au son matricé quadriphonique qu’ils nomment « ambisonie ». Le principe fondamental de cette technique est de décomposer l’espace sonore en plusieurs « directions », ou plus justement, en plusieurs harmoniques sphériques. Alors, nous ne prenons plus le son ni ne le mixons, en tenant compte d’un système de diffusion précis (stéréophonie ou quadriphonie par exemple), mais plutôt en représentant un espace échantillonné.\nLe premier apport concret des recherches de Michael Gerzon et de ses collègues est le développement d’une technique de prise de son utilisant un tétraèdre de microphones.\n\n\n\n\n\n\nLe tétraèdre, monté avec des microphones Calrecs, ancêtre des microphones ambisoniques\n\n\n\n\n\n\n\nMichael Gerzon installant le système tétraédrique\n\n\n\n\n\nDe tels dispositifs étant limités par l’encombrement spatial de chaque microphone, il fut ensuite inventé les microphones ambisoniques, regroupant les quatre capsules de façons aussi coïncidentes que possible. Le premier de ce type est le Soundfield, qui fit son baptême en février 1975.\n\n\n\nMicrophone Soundfield\n\n\nLa même année, Michael Gerzon rédige deux articles sur le principe général de l’ambisonie et son utilisation en studio. On y trouve notamment la définition des différents formats d’ambisonie, principalement:\n\nLe A-Format, correspondant aux canaux du microphone Soundfield\nLe B-Format, format matricé WXYZ (W est le canal omni, ou commun, les autres canaux sont les harmoniques sphériques permettant d’apposer une direction au son)\n\nL’ambisonie, comme technique de production et de prise de son, ne rencontra malheureusement qu’un intérêt très limité. On notera tout de même les enregistrements du label Nimbus records, réalisés pour la plupart en ambisonie. De plus, lorsque le nombre de haut-parleurs devient trop important, de fortes colorations apparaissent sur le signal. Elle reste alors longtemps une technique restreinte au monde de la recherche.\n\n\n\n\n\n\nNote\n\n\n\nMichael Gerzon est très influencé par le brevet de 1931 d’Alan Blumlein sur le son stéréophonique. On y retrouve la même idée de matriçage (Stéréo - MS, A-Format — B-Format) et une emphase sur le prise de son coïncidente. Gerzon a d’ailleurs écrit un article sur le circuit de shuffler d’Alan Blumlein.\n\n\nL’intérêt autour de l’ambisonie revient dans les années quatre-vingt-dix, notamment grâce aux recherches sur l’Ambisonie d’Ordre Plus Elevé (HOA : Higher Order Ambisonic). Cette stratégie vise à augmenter la finesse d’échantillonnage de l’espace en augmentant le nombre d’harmoniques sphériques utilisés. Cette approche est décrite dans la thèse de Jérome Daniels. L’ambisonie de Michael Gerzon devient alors l’ambisonie de premier ordre, et chaque nouvel ordre rajoute un certain nombre de nouvelles « directions spatiales ». Cette précision d’échantillonnage permet de meilleurs rendus sur un grand nombre de haut-parleurs, ainsi qu’un décodage ambisonique vers binaural plus qualitatif. En guise d’exemple, les outils de création de vidéos en réalité virtuelle (VR) de Meta supportent l’ambisonie jusqu’au troisième ordre.\n\n\n\nReprésentation des harmoniques sphériques des ordres 0 à 3\n\n\nCe regain d’intérêt et la multiplication des canaux ambisoniques sont à corréler avec l’augmentation de la puissance des ordinateurs. En effet, les outils de traitement numériques sont bien plus adéquats que leurs homologues analogiques pour le traitement ambisonique. De plus, l’industrie de la réalité virtuelle, qu’elle soit sous la forme de simples vidéos, ou mieux vidéoludique, a montré un réel intérêt pour les formats audio ambisoniques, notamment pour leurs capacités à être interactifs."
  },
  {
    "objectID": "spatialisation/espaces-hautparlants.html#catégorisation-des-espaces-haut-parlants",
    "href": "spatialisation/espaces-hautparlants.html#catégorisation-des-espaces-haut-parlants",
    "title": "34  Les espaces haut-parlants",
    "section": "34.1 Catégorisation des espaces haut-parlants",
    "text": "34.1 Catégorisation des espaces haut-parlants\nOn peut généralement différencier deux types de spectacle sonores : ceux où la position de l’auditeur est connue et fixe, et ceux où l’on ne peut pas présupposer du positionnement de l’auditeur. Par exemple, lors de l’écoute d’un disque, on suppose que l’auditeur se positionnera au point d’écoute idéal (sweetspot), qui était alors la même place que l’ingénieur du son pendant sa production. À l’inverse, en muséographie, l’auditeur se déplace en permanence et, même si souvent guidés par un sens de visite, ses déplacements sont aléatoires.\nCes deux grandes catégories ont une forte influence sur le choix des technologies de spatialisation.\n\n\n\n\n\n\nNote\n\n\n\nLe cas du concert amplifié est un peu plus ambigu. Les zones dédiées au public sont connues à l’avance, et l’on suppose que celui-ci ne se déplace pas pendant le spectacle. Cependant, on cherche à tout prix à éviter la création d’un sweetspot. Cela impliquerait que le rendu sonore serait idéal pour quelques personnes et médiocre pour le reste du public. On parle alors de zones de diffusions, que l’on tente de rendre aussi homogènes que possible. Ce travail est celui du caleur système.\nOn considère d’ailleurs souvent que les mixages live sont monophoniques, et que le distribue le même mixage sur plusieurs points de diffusion. Cela se nomme la « multi-mono ». Ici, la nécessité de multiples points de diffusions relève de l’homogénéité de la diffusion, et rarement du souci de la spatialisation. Cependant, les techniques de spatialisation permettent aussi d’accroître cette homogénéité, tout en ajoutant une dimension spatiale au spectacle.\n\n\nLe second cas, où l’auditeur est libre de ses déplacements, donne également la plus grande liberté sur le choix du système à déployer. Les haut-parleurs pourront alors peupler l’espace en n’importe quel point et deviennent alors un élément à part entière de la création sonore. On pourrait même envisager le choix d’un haut-parleur pour une réponse en fréquence particulière, ou pour tout autre défaut qui pourrait prendre sens dans la narratologie d’une œuvre. Cette approche a été un des points de réflexion centraux de la musique concrète et acousmatique. Les espaces haut-parlants supposant un emplacement d’écoute idéal sont les plus courants. Ils concernent la musique enregistrée, le cinéma et une grande majorité du spectacle vivant."
  },
  {
    "objectID": "spatialisation/espaces-hautparlants.html#les-systèmes-de-diffusions",
    "href": "spatialisation/espaces-hautparlants.html#les-systèmes-de-diffusions",
    "title": "34  Les espaces haut-parlants",
    "section": "34.2 Les systèmes de diffusions",
    "text": "34.2 Les systèmes de diffusions\nOn peut sous-catégoriser les espaces haut-parlants en caractérisant le système de haut-parleurs utilisé. On distingue ainsi quatre grandes familles :\n\nLes systèmes frontaux\nLes systèmes englobants à une dimension\nLes systèmes englobants à deux dimensions\nLes systèmes à trois dimensions\n\n\n\n\n\n\n\nImportant\n\n\n\nOn considère ici un système de coordonnées sphérique, comprenant donc la distance ( \\(\\rho\\) , sur le schéma ci-dessous), l’azimut ( \\(\\Theta\\) ) et l’élévation ( \\(\\delta\\) ). Ce repère offre l’avantage d’être au plus proche de notre perception de l’espace.\n\n\nRepère de coordonnées sphériques\n\n\n\nDans la première catégorie, on retrouve notamment deux systèmes historiques : la monophonie et la stéréophonie. Le premier se résume par un espace haut-parlant comprenant un unique haut-parleur. Le second est décrit comme un espace haut-parlant comprenant deux haut-parleurs, formant un triangle équilatéral avec l’auditeur. On pourra également évoquer le système LCR, principalement associé au cinéma. En spectacle, on trouve parfois des systèmes en lignes d’enceintes, comme le L-ISA d’L-acoustique, ou encore certains systèmes utilisant la WFS (voir le chapitre 39)\nLes systèmes englobants à une dimension comprennent l’ensemble des systèmes de haut-parleurs encerclant l’auditeur, offrant ainsi un degré complet de liberté de spatialisation. Le mixeur peut ainsi placer un son tout autour de l’auditeur en changeant son angle d’incidence. On pense alors au système quadriphonique, et à tous les systèmes de cinéma dit « surround » : LCRS, 5.1, 7.1, etc.\nLes systèmes englobants à deux dimensions rajoutent un nouvel axe de liberté, permettant d’élever un son. Le Dolby Atmos est sans doute le représentant le plus connu de cette famille de système. Ces systèmes sont le plus souvent, pour des raisons physiques évidentes, demi-sphériques, et seule l’élévation positive y est possible. Ces dômes peuvent prendre des formes diverses et variées, avec plusieurs couronnes d’enceintes en hauteur.\nLes derniers systèmes, et également les plus rares, sont les systèmes proposant trois axes de liberté. On a donc un maillage d’enceinte ponctuant l’espace et permettant ainsi de créer des effets de profondeurs, non pas par des moyens perceptifs (rapport de volume, réverbération, etc.), mais pas des moyens physiques (enceinte réellement présente au point de diffusion).\n\n\n\n\n\n\nAstuce\n\n\n\nAfin de simplifier le vocabulaire, on admettra que « surround » est synonyme de systèmes d’enceintes englobant à une dimension, et, que « immersive » et « 3D » son synonyme de système englobant à deux dimensions."
  },
  {
    "objectID": "spatialisation/espaces-hautparlants.html#une-simple-analogie-image-son",
    "href": "spatialisation/espaces-hautparlants.html#une-simple-analogie-image-son",
    "title": "34  Les espaces haut-parlants",
    "section": "34.3 Une simple analogie image-son",
    "text": "34.3 Une simple analogie image-son\nLa notion d’échantillonnage d’un signal se retrouve dans de multiples disciplines, notamment celles de l’image et de la vidéo. Par exemple, une image diffusée sur un écran est spatialement échantillonnée. Un peu comme les peintres impressionnistes qui créaient l’illusion d’une image unie à partie de touche de pinceau distincte, on retrouve ici un principe parfaitement analogue. Une image numérique est donc cadre d’un certain nombre de points, et chacun de ces points s’appelle un pixel. C’est le plus petit grain d’une image. Si l’on utilise trop peu de pixels pour décrire une image, ceux-ci deviennent plus gros, et l’on distingue alors ces pixels, et l’illusion d’une image unie est perdue.\nLa notion intéressante rendue évidente par le traitement de l’image est la notion d’échantillonnage de l’espace. Dans le champ d’application du son, cette « résolution d’espace » existe aussi, et est directement liée au nombre de haut-parleurs. Plus le nombre de haut-parleurs est grand, plus la cohérence de notre espace haut-parlant sera grande. On pourrait alors qualifier les systèmes mono, stéréo, voir 5.1 et 7.1 de systèmes basse résolution.\n\n\n\n\n\n\nNote\n\n\n\nGardons bien en tête cette idée d’échantillonnage d’espace, car c’est exactement le sujet de l’ambisonique. La WFS reprend aussi cette d’échantillonnage, mais l’applique au front d’onde de l’onde sonore.\n\n\nSi l’on prend en exemple le système stéréophonique, nous avons à notre disposition deux enceintes, une à gauche de l’auditeur et une à sa droite. De manière assez évidente, si nous envoyons un signal seulement sur l’enceinte gauche, le son semblera venir de la gauche et vice-versa. Alors, que se passe-t-il lorsque nous envoyons le même signal (amplitude et phase identique) sur les deux enceintes ? Il se créer alors un effet psychoacoustique qui va nous donner l’illusion que le son provient du « centre de gravité » du système (barycentre serait plus juste). Ce phénomène se nomme centre fantôme. Si l’on introduit un écart de gain entre les deux enceintes, cette source fantôme se déplace vers l’enceinte la plus forte. Il s’agit alors d’une forme d’interpolation. On déduit ainsi des points de diffusions physiquement inexistants grâce aux points de diffusions réels environnants.\nPar rapport à notre question de résolution, plus celle d’un système est faible, plus on peut supposer que l’espacement entre les enceintes sera grand. Le poids mis sur cette stratégie d’interpolation est alors plus important, jusqu’au point où l’illusion du centre fantôme cesse d’opérer."
  },
  {
    "objectID": "spatialisation/espaces-hautparlants.html#études-des-systèmes-normés",
    "href": "spatialisation/espaces-hautparlants.html#études-des-systèmes-normés",
    "title": "34  Les espaces haut-parlants",
    "section": "34.4 Études des systèmes normés",
    "text": "34.4 Études des systèmes normés\n\n34.4.1 Les systèmes frontaux\n\n\n\n\n\n\nMonophonie\n\n\n\n\n\n\n\nStereophonie\n\n\n\n\n\nLa monophonie n’offre évidemment aucun axe de liberté pour à la spatialisation. On pourra éventuellement « tricher » un effet de profondeur en jouant sur le volume des sources sonores et sur le mixage de la réverbération. Depuis la fin des années soixante, la norme de production et d’écoute est la stéréophonie. La stéréophonie repose sur deux enceintes, formant un triangle équilatéral avec l’auditeur. Elle offre alors une scène sonore de 60° face à l’auditeur, et donc un premier axe de liberté dans la spatialisation. Il semble évident que l’on ne peut pas faire jouer un son plus à gauche que l’enceinte gauche, et plus à droite que l’enceinte droite. Dès lors, les axes formés par chacune des enceintes avec le point d’écoute idéal sont des frontières infranchissables par les sons.\n\n\n\n\n\n\nAstuce\n\n\n\nLe transaural est une technique de spatialisation qui permet, dans une certaine mesure, d’effacer ces frontières. Mais cela vient avec un certain nombre d’inconvénients détaillé dans la chapitre 35.3.\n\n\nChacune des enceintes est alimentée par un signal audio dédié. On associe alors au système de diffusion stéréophonique un mixage stéréophonique, comptant deux canaux.\n\n\n\n\n\n\nAvertissement\n\n\n\nIl ne faut pas confondre « stéréophonie » et « deux canaux ». Si la stéréophonie implique un mixage sur deux canaux, un mixage sur deux canaux n’implique pas de la stéréophonie. Par exemple, une écoute au casque n’est pas une écoute stéréophonique. Un mixage binaural repose aussi sur deux canaux, mais n’est pas du tout indiqué pour une écoute sur un système stéréophonique.\n\n\nComme nous l’avons vu précédemment ( chapitre 33 ), au cinéma, le besoin d’un canal central, caché derrière l’écran, c’est très rapidement fait sentir, principalement avec l’essor des formats larges-écrans type Cinerama et Cinemascope. On trouve donc des systèmes LCR, soit « left », « center », « right ». Cependant, le support optique de la pellicule ne permet pas d’y stocker plus de deux canaux. On réalise alors un matriçage, permettant de réduire le mixage LCR sur deux canaux (appelés Lt, pour « Left total » et Rt, pour « Right total »), puis, à la diffusion du film, on opère le dématriçage vers le système LCR. Le canal central est alors alimenté par la sommation des canaux L et R.\n\n\n\n\n\n\nNote\n\n\n\nLe canal central est alors formé par les signaux en phase du matriçage bicanal.\n\n\nUne telle réduction n’est pas indolore sur le signal, et nuit largement à la cohérence d’espace de la diffusion.\n\n\n34.4.2 Les systèmes englobants à une dimension\n\n\n\n\n\n\nQuadriphonie\n\n\n\n\n \n\n\n\nLa quadriphonie est le premier système de ce type marquant, apparu dans les années soixante-dix. On place alors les quatre enceintes en carré et l’auditeur au centre. On notera ainsi que les angles enceintes-auditeur sont de 90°, soit 30° de plus que la stéréophonie. Nous pouvons déduire que l’on gagne globalement en couverture spatiale, mais l’on perd en précision sur les sources fantômes.\nSon homologue dans le monde du cinéma est le LCRS, composé de trois enceintes frontales (gauche, centre, droit) et d’une enceinte arrière. La position exact des enceintes en cinéma est toujours un sujet délicat. On considère généralement que les enceintes gauche et droite forme une stéréophonie (plus ou moins 30°) auxquelles ont rajoute une enceinte centrale et une enceinte arrière (respectivement 0° et 180°). Cependant, la diffusion cinématographique n’est pas vraiment tournée vers un point précis. En effet, il convient de convenablement couvrir l’ensemble des spectateurs de la salle. Alors, dans les auditoriums de mixage ainsi que dans les salles, cette preservation d’angles est somme toute assez relative.\n\n\n\n\n\n\nAstuce\n\n\n\nLe système LCRS est un système historique et n’est plus du tout utilisé aujourd’hui. La quadriphonie quand à elle continue d’avoir une certaine popularité. On peut y avoir deux phénomènes : une nostalgie toujours plus présente pour le “vintage”, mais aussi une certaine facilité de prise en main pour les artistes. Il est en effet assez facile d’expliquer la quadriphonie comme quatre espace stéréophoniques (même si ce n’est pas rigoureusement exact).\n\n\nC’est grâce à la transition du son analogique vers le son numérique que l’on voit apparaître de nouveaux systèmes de diffusions englobants. Toujours dans le cinéma, le premier et le plus connu est le système 5.1, défendu par Dolby et par DTS.\n\n\n\n\n\n\n5.1\n\n\n\n\n\n\n\n7.1\n\n\n\n\n\nIl s’agit au final d’une simple extension du système LCRS, que l’on va augmenter d’un canal de LFE distinct et d’un canal arrière supplémentaire. Alors, le terme arrière est un peu exagéré, car ces enceintes sont placées à plus ou moins 110 degrés de l’enceinte centrale. On a donc plutôt un placement latéral, légèrement décalé vers l’arrière. On notera aussi l’écart important entre les canaux latéraux avant et ces canaux arrière, grand de 80°. Pire, les enceintes arrières sont séparées d’un angle de 140°. Il est donc délicat d’envisager un placement de sources fantôme à l’arrière de l’auditeur. Aussi, il est plus sage de considérer, lors d’un mixage, qu’un système 5.1 n’offre qu’une scène sonore que de 220°.\n\n\n\n\n\n\nNote\n\n\n\nLe système quadriphonique est quelque part plus homogène et couvre un espace plus grand que le système 5.1. Par contre, sa définition et sa précision sont moindres sur l’espace frontal.\n\n\nLe système 5.1 est défini dans la recommandation de l’ITU (International Union Broadcast) n° BS.775-1. On note d’ailleurs que le placement des enceintes possède une marge de tolérance, celles-ci pouvant être placées entre plus ou moins 100° jusqu’à plus ou moins 120° du centre.\nCette recommandation de l’ITU propose également un ordonnancement des canaux. En effet, ce sujet n’est pas normé, et donne lieu parfois à certaines erreurs de rendus. L’ITU propose alors de ranger les canaux dans l’ordre suivant : gauche, droit, centre, LFE, arrière gauche, arrière droit. Cependant, Pro Tools, logiciel de mixage et montage son le plus répandu dans le monde professionnel, adopte l’agencement suivant : gauche, centre, droit, arrière gauche, arrière droit, LFE. Il convient alors d’être excessivement vigilant quant à la diffusion d’un contenu 5.1, et de bien vérifier le bon routage de chaque canal.\nLe format marquant suivant est le 7.1 Dolby Surround. On conserve alors le 5.1 que nous avons déjà décrit et y rajoutons deux enceintes, véritable arrière gauche et arrière droit. Il s’est principalement imposé en salle de cinéma.\n\n\n\nOctophonie\n\n\nNous pouvons également évoquer un système supplémentaire, l’octophonie, mais qui n’a pas eu de réelle exploitation commerciale. En d’autres termes, son utilisation s’est bornée à l’exploitation lors de spectacles vivants, sans intégrer la problématique du stockage, du transport et de la restitution antérieure du programme. Il s’agit cependant de systèmes relativement classiques.\nL’octophonie est un nom que l’on rencontre assez fréquemment. Cependant, son nom est souvent rattaché à deux systèmes différents. Dans ce livre, nous appellerons « octophonie » un système de haut-parleurs régulièrement positionnés sur un cercle. Les enceintes sont alors toutes espacées de 45°. On peut voir ce système comme une augmentation de la résolution de la quadriphonie. Cette octophonie offre deux avantages : sa couverture est homogène et sa densité satisfaisante.\nL’autre système associé à l’octophonie est le cube, que nous aborderons dans la section suivante.\n\n\n34.4.3 Les systèmes englobants à deux dimensions\nAvant de nous attarder sur un autre (encore) format de spatialisation de Dolby, nous pouvons faire un détour par le Japon. La société NHK (Nippon Hōsō Kyōkai, ou compagnie de télédiffusion japonaise) fait partie du réseau de télédiffusion publique japonaise. Celle-ci possède une branche dédiée à la recherche nommée « NHK Science & Technology Research Laboratories ». Ils ont alors proposé un nouveau format de son « surround » : le 22.2.\nCe système propose trois niveaux d’élévation : un au sol, un à hauteur d’oreille, et un dernier positionné en hauteur. On y ajoute également une enceinte dite de « voice of god », placée au-dessus de la tête de l’auditeur.\n\nLe niveau inférieur est constitué de trois haut-parleurs frontaux\nLe niveau moyen est composé de dix haut-parleurs. On pourra alors le décomposer comme une octophonie (une enceinte tous les 45°), plus deux enceintes supplémentaires sur la scène frontale.\nLe niveau supérieur comporte huit haut-parleurs, disposés comme une octophonie, plus une enceinte juste au-dessus de l’auditeur.\n\nCe dispositif offre une couverture relativement homogène de l’espace et surtout plus dense que la plupart de ces concurrents. Malheureusement, hors du Japon, ce système n’est pas du tout répandu.\n\n\n\n\n\n\nNote\n\n\n\nA consulter, le document de NHK sur son 22.2.\n\n\nRevenons aussi sur nos questions d’octophonie. Cette appelation est parfois utilisée pour parler d’un système de haut-parleur cube composé de deux plans :\n\nune quadriphonie au sol\nune quadriphonie en élévation\n\nOn couvre ainsi une sphère complète, mais avec une résolution plutôt faible. On met ici beaucoup de pression sur l’effet de source fantôme.\nNous pouvons aussi aborder les systèmes définis algorithmiquement. On indique alors un nombre de haut-parleurs souhaités, et l’on obtient un arrangement de points dans l’espace positionnés homogènement.\n\n\n\n\n\n\nMise en garde\n\n\n\nSi le placement de point de façon homogène sur un cercle paraît être une tâche triviale, ce n’est pas du tout le cas pour une sphère.\n\n\nRetenons deux lois de répartitions importantes :\n\nLe maillage t-design\nLe maillage de Lebedev\n\nNous reviendrons plus particulièrement sur ces maillages lors de leurs utilisations dans le cadre de l’ambisonique. En dehors de ce cas, ces lois peuvent être utilisées dans la construction de dômes où de sphères de haut-parleurs.\nEnfin, nous avons donc les arrangements de haut-parleurs associés au Dolby Atmos. Le plus commun est le 7.1.4, reprenant un 7.1 Dolby et y rajoutant une quadriphonie en élévation. Une première réduction consiste à réduire la couche d’élévation à deux canaux (7.1.2). On trouve également les 5.1.4 et 5.1.2, reprenant la même logique.\nPlus intéressant, le Dolby Atmos propose aussi un format 9.1.6, qui apporte enfin une homogénéité correcte, au moins sur le plan principal.\n\n\n\n\n\n\nAvertissement\n\n\n\nLes techniques de mixage entourant le Dolby Atmos sont très différentes des techniques classiques, reposant sur des lois de panoramiques. Le Dolby Atmos est avant tout un format de mixage orienté objet. Nous abordons cet aspect dans le chapitre 45.\n\n\nSi Dolby possède aujourd’hui la force de frappe la plus importante, on trouve tout de même quelques acteurs pour tenter de lui faire concurrence. DTS propose son propre format de mixage orienté objet, nommé DTS:X. Les systèmes de diffusion sont cependant calqués sur ceux de Dolby. On retrouve aussi Auro 3D, également avec son propre format de mixage orienté objet. Compatible avec les systèmes Dolby Atmos, on retrouve aussi le 13.1 (7.1.4, plus une enceinte centrale sur le plan d’élévation et une “voice of god”) et 11.1 (5.1.4, plus une enceinte centrale sur le plan d’élévation et une “voice of god”)."
  },
  {
    "objectID": "spatialisation/binaural.html#la-prise-de-son-binaurale",
    "href": "spatialisation/binaural.html#la-prise-de-son-binaurale",
    "title": "35  Le binaural",
    "section": "35.1 La prise de son binaurale",
    "text": "35.1 La prise de son binaurale"
  },
  {
    "objectID": "spatialisation/binaural.html#sec-binaural-synthese",
    "href": "spatialisation/binaural.html#sec-binaural-synthese",
    "title": "35  Le binaural",
    "section": "35.2 La synthèse binaurale",
    "text": "35.2 La synthèse binaurale"
  },
  {
    "objectID": "spatialisation/binaural.html#sec-transaural",
    "href": "spatialisation/binaural.html#sec-transaural",
    "title": "35  Le binaural",
    "section": "35.3 Le transaural",
    "text": "35.3 Le transaural"
  },
  {
    "objectID": "spatialisation/spat-perceptif.html#lapproche-perceptive",
    "href": "spatialisation/spat-perceptif.html#lapproche-perceptive",
    "title": "36  Orienté canal",
    "section": "36.1 L’approche perceptive",
    "text": "36.1 L’approche perceptive\n\n36.1.1 le VBAP\n\n\n36.1.2 Le VBIP\n\n\n36.1.3 Le LBAP\n\n\n36.1.4 Autres variantes"
  },
  {
    "objectID": "spatialisation/spat-perceptif.html#lapproche-matricielle",
    "href": "spatialisation/spat-perceptif.html#lapproche-matricielle",
    "title": "36  Orienté canal",
    "section": "36.2 L’approche matricielle",
    "text": "36.2 L’approche matricielle\n\n36.2.1 Le DBAP\n\n\n36.2.2 Le KNN"
  },
  {
    "objectID": "spatialisation/ambisonique.html#lambisonie-du-premier-ordre-foa",
    "href": "spatialisation/ambisonique.html#lambisonie-du-premier-ordre-foa",
    "title": "37  L’ambisonique",
    "section": "37.1 L’ambisonie du premier ordre (FOA)",
    "text": "37.1 L’ambisonie du premier ordre (FOA)\nL’ambisonie du premier ordre (ou FOA pour First Order Ambisonic) voit le jour sous la forme d’une technique de prise de son. Celle-ci permet l’enregistrement d’un scène sonore sur quatres canaux, que l’on peut ensuite décoder sur n’importe quel système de haut-parleurs.\nPour capturer le champ acoustique en un point, il faut donc s’intéresser au champ acoustique lui-même. Nous l’avons vu à la Section 2.1, sous sa forme acoustique, une onde sonore se caractérise par la variation locale de la pression. Pour mesurer la pression en un point, nous pouvons utiliser un microphone omnidirectionnel (également appelé microphone à pression). Ce microphone omnidirectionnel va donc rendre compte a chaque instant du temps de la valeur de la pression. A ce stade, il n’est pas question de parler de spatialisation, la capatation d’une tel capsule étant monophonique.\nIl conviendrait donc de mesurer la “direction” du déplacement local des particules d’air. Quelque part, on se demande dans quel sens varie la pression. Pour cela, on cherche à mesurer la vitesse de ce déplacement, et donc la variation de la pression en un point. Pour se faire, on utilise un microphone bidirectionnel (aussi appelé à gradient de pression). Ce microphone va donc mesurer, à chaque un instant du temps, la différence entre la pression qui à lieu tout de suite et celle qui a eu lieu. On caractérise donc une variation dans le temps.\n\n\n\n\n\n\nAstuce\n\n\n\nPour clarifier le sujet, faisons le parallèle avec une voiture sur une route. On peut facilement mesure sa position sur la route (à quelle borne kilométrique se situe t-elle ?). Sa vitesse correspond à la variation de son déplacement dans le temps (combien de temps à telle mise pour passer d’une borne kilométrique A à une borne kilométrique B). Son accélération est alors la mesure de la variation de la vitesse. Quand j’accélère, la vitesse augmente, quand je décélère, la vitesse diminue, quand l’accélération est nulle, la vitesse est constante.\nNous pourrions pousser ce raisonnement dans l’absurde : si la voiture oscille entre marche avant et marche arrière (donc sa position moyenne est toujours la même), nous pouvons faire une parallèle direct avec ce que vie une particule d’air lorsqu’elle est traversée par une onde sonore.\n\n\n\n\n\n\n\nFigure 37.1: Pression et variation de pression sur l’axe X\n\n\n\n\nUne capsule bidirectionnelle nous permet de mesurer dans quel axe se déplace les molécules d’air (si les les molécules oscillent dans l’axe du microphone, on obtient une tension mesurable, si les molécules oscillent sur l’axe normal (à 90°) de l’axe du microphone, on obtient une tension négligeable). Cependant cela ne nous renseigne pas si la source sonore responsable de la perturbation se trouve plutôt devant ou derrière le microphone. Cependant, si nous associons au même point de l’espace une capsule omnidirectionnelle et une capsule bidirectionnelle, nous allons pouvoir lever cette indétermination.\n\n\n\n\n\nFigure 37.2: Résultante de la somme et de la différence des composantes W et X\n\n\n\n\nSi nous additionnons le signal obtenue par la cellule omnidirectionnelle et la cellule bidirectionnelle, nous crééons une directivité cardioïdes qui écoute vers l’avant du microphone bidirectionnelle, et donc, rejette ce qui se trouve dans son dos. Si l’on soustrait au microphone omnidirectionnel la tension obtenue par le microphone bidirectionnel, on créer alors une directivité cardioïdes qui écoute à l’arrière du microphone omnidirectionnel et rejette ce qui se trouve devant lui. Nous venons ici de fabriquer un couple MS.\nNotre dispositif permet ici de capter des ondes sonores selon un axe (ici, X). Si l’on souhaite étendre ce dispositif pour capter l’espace en trois dimensions, nous serions naturellement tenté d’ajouter deux autres capsules bidirectionnelles."
  },
  {
    "objectID": "spatialisation/ambisonique.html#captation-du-champs-sonore",
    "href": "spatialisation/ambisonique.html#captation-du-champs-sonore",
    "title": "37  L’ambisonique",
    "section": "37.2 Captation du champs sonore",
    "text": "37.2 Captation du champs sonore"
  },
  {
    "objectID": "spatialisation/ambisonique.html#synthèse-du-champs-sonore",
    "href": "spatialisation/ambisonique.html#synthèse-du-champs-sonore",
    "title": "37  L’ambisonique",
    "section": "37.3 Synthèse du champs sonore",
    "text": "37.3 Synthèse du champs sonore"
  },
  {
    "objectID": "spatialisation/ambisonique.html#HOA-decode-allrad",
    "href": "spatialisation/ambisonique.html#HOA-decode-allrad",
    "title": "37  L’ambisonique",
    "section": "37.4 Décodeurs",
    "text": "37.4 Décodeurs"
  },
  {
    "objectID": "spatialisation/mix-obj.html#laudio-definition-model",
    "href": "spatialisation/mix-obj.html#laudio-definition-model",
    "title": "38  Mixage orienté objet",
    "section": "38.1 L’Audio Definition Model",
    "text": "38.1 L’Audio Definition Model\n\n38.1.1 Présentation\nL’Audio Definition Model est une autre recommandation de l’ITU, dont la dernière édition date de 2019. Ce papier concentre beaucoup d’informations importantes. Principalement, il s’agit de décrire une base commune, espérant devenir une base commune entre les différents acteurs de l’audio orienté objet. Le premier objectif est donc d’assurer l’interopérabilité entre les différentes solutions de mixage orienté objet.\nTechniquement, l’ADM est donc un catalogue de descripteurs d’audio. Pour chaque canal audio, un certain nombre d’informations textuelles y sont associées. Elles sont donc lisibles par un être humain, et éditables.\n\n\n38.1.2 Description des métadonnées\nL’ADM permet la représentation de quatre “types” d’audio :\n\nLe channel-based ou orienté-canal\nL’HOA ou Ambisonie d’ordre plus élevé\nLes Matrix ou formats matricés\nLes objets\n\n\n\n\n\n\n\nAvertissement\n\n\n\nLe mot “objet” a deux sens dans la documentation de l’ADM. Un objet est un format, permettant le rendu dynamique d’un élément sonore (changement de position, de volume, etc.). Cependant, un objet est également un contenu, qui permet de référencer un bloc d’échantillons d’un canal audio.\nEn ce sens un objet audio en ADM peut représenter le canal d’un flux orienté objet, le canal d’un flux HOA, un élément d’une matrice ou un objet sonore !\n\n\n\n\n\nFigure 38.1: Organigramme de l’ADM\n\n\nL’ADM permet de définir plusieurs programmes, appelés “audioProgramme”. Prenons comme exemple le cas d’une série télévisée diffusée en France et en Grande-Bretagne. Nous aurions, dans ce cas, un premier audioProgramme contenant les voix françaises et le reste de la bande-son, puis ,un second contenant les voix anglaises et le reste de la bande-son. Nous aurions alors trois contenus audio (appelés audioContent) : les voix anglaises, les voix françaises et le reste de la bande-son (bruitages, montage son, musique, etc.). L’audioProgramme “version anglaise” contiendrait le audioContent voix anglaise et le reste de la bande son, celui “version française” contiendrait les audioContent voix française et le reste de la bande son. Ainsi, le spectateur peut choisir d’écouter l’audioProgramme qu’il préfère, de façon interactive.\nChacun de ces audioContent contiennent eux-même des objets, appelés audioObject. Dans notre exemple, nous aurions différents objets dans l’audioContent voix française correspondant aux différents comédiennes et comédiens. Ces audioObject permettent d’indiquer quels échantillons audio du fichier BW64 sont concernés. Il convient alors d’indiquer quel est le format audio de ces échantillons. Les audioObject pointent vers un audioPack qui lui même pointe vers un certain nombre de sous modules permettant de caractériser intégralement le format audio et où il doit être positionné dans l’espace.\nSi l’audioObject est de type “directSpeaker”, on l’affecte à un haut-parleur, dont on renseigne une position fixe. Si ce haut-parleur existe dans l’arrangement de haut-parleurs connecté au décodeur ADM, le signal y est routé, sinon on interpole avec les haut-parleurs les plus proches.\nSi l’audioObject est de type ambisonique, on cherche les autres objets appartenant au même flux, puis on décode l’ensemble, sur le système connecté au décodeur ADM.\nSi l’audioObject est la composante d’une matrice, on cherche les autres objets appartenant au même flux, puis on dématrice l’ensemble vers nouvel audioPack de type “directSpeaker”.\nSi l’audioObject est de type “object”, on regarde ses métadonnées de spatialisation puis on interpole cette position en utilisant les haut-parleurs les plus proches du point indiqué.\n\n\n\n\n\n\nAstuce\n\n\n\nGardons en tête que les audioObject “directSpeaker” ou ambisonique sont des mixages. Les scènes sonores qu’ils représentent sont statiques et immuables. Un audioObject de format objet contient un élément sonore (une voix, un instrument, un bruitage), que le décodeur ADM va ensuite mixer. Un objet peut donc être interactif. L’utilisateur pourrait changer son volume, sa position, etc.\n\n\n\n\n38.1.3 Les métadonnées de spatialisation\nL’ADM n’intègre qu’un jeu de paramètre très sommaire :\n\nLe niveau\nLa position (en coordonnées cartésiennes et sphériques)\nLa taille et diffusion de la source\n\nLe niveau est un simple réglage de gain. La position permet de placer l’objet dans un espace virtuel en trois dimensions et selon deux systèmes de coordonnées. On préfère généralement le système sphérique, plus proche de notre perception sonore, cependant, la description de certains mouvements est plus simple dans un repère plutôt qu’un autre. Le paramètre de distance permet de placer une source entre les haut-parleurs et le point d’écoute idéal.\nLa largeur permet de jouer sur l’étalement de l’objet dans l’espace. En pratique, accroître ce paramètre augmente le nombre de haut-parleurs contribuant à la diffusion de l’objet sonore. Associé à la taille, le paramètre de diffusion permet d’ajouter de la décorrélation sur les haut-parleurs contribuant à la largeur de restitution de la source.\nIl n’y a donc pas de descripteurs perceptifs ou d’espaces. L’approche de la spatialisation y est donc très basique, voire simpliste. Comme nous le verrons dans les sections plus pratiques, cela impose des techniques de mixage relativement complexes à fin de contourner ses limitations.\n\n\n\n\n\n\nNote\n\n\n\nMalheureusement la plupart des moteurs de mixage orienté objet souffrent de ces mêmes lacunes, y compris le Dolby Atmos. L’exception principale est l’IRCAM Spat ainsi que le Spat Revolution de FLUX:: Immersive.\n\n\n\n\n38.1.4 Inscription des métadonnées dans un fichier audio\nL’ADM décrit la possibilité d’inscrire les métadonnées de mixage directement dans un fichier audio, contenant lui-même l’ensemble des données audio nécessaires. Dans le cadre d’un mixage orienté objet, ce fichier audio peut posséder autant de canaux que d’objets déclarés. Le format retenu pour les stockages de ces informations est le BW64 (Broadcast Wavefile 64 bit).\n\n\n\n\n\n\nNote\n\n\n\nLe BW64 est une extension du format BWF. Tous deux sont également définis dans des recommandations de l’ITU. Ces fichiers prennent la forme de simples fichiers “wave” (monfichier.wav).\nÉgalement, on parle d’un en-tête de fichier, pour qualifier l’ensemble des métadonnées (nom, type de fichier, conteneur, etc.) précèdent la donnée utile (ici, les échantillons audio). Attention, les métadonnées ne sont pas toutes relatives à l’ADM. La plupart les fichiers possèdent des métadonnées pour indiquer aux programmes comment exploiter les données qu’ils stockent.\n\n\n\n\n\n\n\n\nFigure 38.2: Structure d’un fichier wave\n\n\n\n\n\n\n\nFigure 38.3: Structure d’un fichier bw64\n\n\n\n\n\n\nIllustations extraites du document BS.2088-2 de l’ITU\n\n\n\nLa figure Figure 38.3 fait mention de plusieurs “chunk” (section, bloc) nommé “axml”, “bxml”, “sxml”, “chna”. La globalité des informations est stockée sous la forme d’un XML dans les chunks “axml”, “bxml” ou “sxml”. Le chunk “chna” stocke les informations permettant de faire les liens entre les données audio et leur description dans l’ADM. Il est en général recommandé d’utiliser le chunk “axml” (le “bxml” est alors plutôt utilisé pour les surcouches à l’ADM, aussi appelés profiles)."
  },
  {
    "objectID": "spatialisation/mix-obj.html#les-profils-adm",
    "href": "spatialisation/mix-obj.html#les-profils-adm",
    "title": "38  Mixage orienté objet",
    "section": "38.2 Les profils ADM",
    "text": "38.2 Les profils ADM\nCertains constructeurs ont choisis de proposer des sur-spécification à l’ADM, aussi appelés “profils”. Ces profils ont généralement deux buts :\n\nRajouter des métadonnées spécifiques à l’utilisation de codec audio propriétaire.\nLimite les spécifications de l’ADM pour correspondre aux usages de certains domaines\n\nLes deux principaux profils connus sont ceux du MPEG-H de Fraunhofer et de Dolby\n\n\n\n\n\n\nImportant\n\n\n\nAucun des deux constructeurs n’ont choisit de maintenir le support de l’ambisonique dans leur format.\nDe plus, il n’existe à ce jour, aucun outil pour convertir un ADM d’un profil quelquonque vers un ADM Dolby. La compatibilité des formats n’est donc pas assurée.\n\n\n## L’ADM Sériel (S-ADM)\nPour certaines applications, comme le live ou le streaming, la description des métadonnées sous forme d’un dictionnaire XML n’est pas adéquate. Ici, les flux audio sont en temps réel et ne dépendent pas nécessairement d’un fichier audio. Il existe donc une norme sérielle de l’ADM, permettant de transmettre les métadonnées dans une connexion numérique, type AES, MADI ou encore à travers de l’audio sur IP.\nLa description du S-ADM est disponible dans l’ITU-R BS.2125."
  },
  {
    "objectID": "spatialisation/mix-obj.html#ladm-osc",
    "href": "spatialisation/mix-obj.html#ladm-osc",
    "title": "38  Mixage orienté objet",
    "section": "38.3 L’ADM OSC",
    "text": "38.3 L’ADM OSC\nL’ADM OSC est une initiative menée par FLUX:: Immersive, L-Acoustic et Radio France pour proposer une base de communication commune entre les différents moteurs de mixage orienté objet dans le monde du Live. Cela assure donc une interopérabilité minimale entre les principaux acteurs du milieu. À ce jour, les acteurs suivants se sont joints à la conversation : d&b Audiotechnik, DiGiCo, Dolby, Lawo, Magix, Merging Technologies, Meyer Sound, Steinberg.\nLes logiciels implémentant l’ADM OSC comptent SPAT Revolution (FLUX::SE), L-ISA Controller (L-Acoustics), Ovation (Merging Technologies), Nuendo (Steinberg), SpaceMap Go (Meyer Sound), QLAB 5 (Figure 53), Space Controller (Sound Particles), Modulo Kinetic (Modulo Pi), Iosono (Barco).\nLe code source de ce projet, développé sous une licence open source, est disponible dans ce répository github"
  },
  {
    "objectID": "spatialisation/mix-obj.html#létape-de-rendue",
    "href": "spatialisation/mix-obj.html#létape-de-rendue",
    "title": "38  Mixage orienté objet",
    "section": "38.4 L’étape de rendue",
    "text": "38.4 L’étape de rendue\nTout ce que nous avons vu pour l’instant, au sujet de l’ADM, ne concerne au final que la description d’un mixage, et nous n’avons pour l’instant pas envisagé le “comment”. En d’autres termes, nous avons une liste d’ingrédients, mais ne savons pas comment les mélanger et les cuisiner entre eux.\nIl existe une recommandation de l’ITU (BS-2127) consacrée au sujet du “renderer”, ou processeur de rendu. On fournit donc audio et métadonnées à ce moteur de rendu, qui, grâce à l’analyse de ces métadonnées et au système de haut-parleurs qui lui est connecté, peut calculer le gain de reproduction de chaque source et chaque haut-parleur.\n\n38.4.1 Description de la loi de panoramique\nLa loi de panoramique recommandée par l’ITU BS-2127 empreinte beaucoup au LBAP (voir la section 36.1.3). Nous sommes donc sur une loi de panoramique favorisant un point d’écoute idéal, situé à égale distance des haut-parleurs.\n\n\n\n\n\n\nNote\n\n\n\nIci, la distance ne s’exprime qu’entre les haut-parleurs et le point d’écoute idéal. En d’autres termes, la distance maximale correspond au plan des haut-parleurs, la distance minimale correspond au point d’écoute idéale. La distance ne contribue pas au gain de la source, mais plutôt à une sorte de spreading.\n\n\nL’algorithme se décrit de la façon suivante :\n\nOn recherche les couches (en élévation) de haut-parleurs au-dessus et en dessous de la source. On calcule alors le gain \\(z\\) en interpolant entre les deux couches.\nSur chaque couche trouvée, on recherche les deux lignes de haut-parleurs en avant et en arrière de la source. On calcule ainsi le gain \\(y\\) en interpolant entre les deux lignes.\nEnfin, sur chaque ligne trouvée, on recherche la paire de haut-parleurs à gauche et à droit de la source, permettant finalement de calculer le gain \\(x\\) entre les deux haut-parleurs.\n\n\n\n\n\n\n\nNote\n\n\n\nSi une source est positionnée sur un haut-parleur, alors seul celui-ci contribue à la diffusion sonore.\n\n\n\n\n38.4.2 Largeur et décorrélation\nLe paramètre de “size” ou taille permet de faire paraître une source plus grande en augmentant le nombre de haut-parleurs contribuant à la diffusion de la source. Ce paramètre “size” est enfaîte un macro-paramètre qui manipule trois autres paramètres:\n\nLa largeur (étalement horizontal)\nLa hauteur (étalement vertical)\nLa profondeur (étalement dans la profondeur)\n\nPar principe de source fantôme, augmenter le nombre de haut-parleurs contribuant à la diffusion d’une source n’augmente pas véritablement sa taille apparente dans notre perception de la scène sonore. La source vient plutôt se positionner au barycentre des haut-parleurs, pondéré par leur énergie de diffusion. Pour réellement créer un effet de “taille”, il est nécessaire d’introduire une différence suffisamment importante pour que notre oreille ait l’illusion d’un étalement dans l’espace. Cette différence doit également être suffisamment faible pour ne pas déformer le signal d’origine. On appelle cela la décorrélation. L’ITU recommande ici l’utilisation de filtres passe-tout sur chaque canal de diffusions.\n\n\n38.4.3 Décodage ambisonique\nL’ITU décrit l’utilisation d’un décodage AllRAD. Voir la [section -@HOA-decode-allrad]"
  },
  {
    "objectID": "outils-spat/mix-obj.html#mpeg-h-et-adm",
    "href": "outils-spat/mix-obj.html#mpeg-h-et-adm",
    "title": "45  Le mixage orienté objet",
    "section": "45.1 MPEG-H et ADM",
    "text": "45.1 MPEG-H et ADM"
  },
  {
    "objectID": "outils-spat/mix-obj.html#dolby-atmos",
    "href": "outils-spat/mix-obj.html#dolby-atmos",
    "title": "45  Le mixage orienté objet",
    "section": "45.2 Dolby Atmos",
    "text": "45.2 Dolby Atmos"
  },
  {
    "objectID": "outils-spat/mix-obj.html#ircam-spat",
    "href": "outils-spat/mix-obj.html#ircam-spat",
    "title": "45  Le mixage orienté objet",
    "section": "45.3 IRCAM Spat",
    "text": "45.3 IRCAM Spat"
  },
  {
    "objectID": "outils-spat/mix-obj.html#flux-spat-revolution",
    "href": "outils-spat/mix-obj.html#flux-spat-revolution",
    "title": "45  Le mixage orienté objet",
    "section": "45.4 FLUX:: Spat Revolution",
    "text": "45.4 FLUX:: Spat Revolution"
  }
]